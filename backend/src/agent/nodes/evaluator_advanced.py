"""Revolutionary Multi-Model Evaluator with PhD-level consensus and advanced assessment."""

import asyncio
import logging
import os
import json
import numpy as np
import statistics
from typing import Dict, Any, List, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum
import hashlib

from langchain_core.runnables import RunnableConfig
from langchain_google_genai import ChatGoogleGenerativeAI
import anthropic
from openai import AsyncOpenAI
from scipy.stats import pearsonr, spearmanr
from sklearn.metrics import cohen_kappa_score
import networkx as nx

from agent.base import BaseNode, EvaluationResult
from agent.handywriterz_state import HandyWriterzState
from tools.casp_appraisal_tool import CASPAppraisalTool
from services.llm_service import get_llm_client
from config.model_config import get_model_config

logger = logging.getLogger(__name__)


class AssessmentDomain(Enum):
    """Sophisticated assessment domains for academic evaluation."""
    THEORETICAL_SOPHISTICATION = "theoretical_framework_mastery"
    EMPIRICAL_RIGOR = "research_methodology_excellence"
    ANALYTICAL_DEPTH = "critical_thinking_sophistication"
    ARGUMENTATIVE_COHERENCE = "logical_structure_quality"
    SCHOLARLY_COMMUNICATION = "academic_writing_mastery"
    EPISTEMIC_RESPONSIBILITY = "knowledge_claim_justification"
    INTERDISCIPLINARY_SYNTHESIS = "cross_domain_integration"
    METHODOLOGICAL_INNOVATION = "research_approach_creativity"
    ETHICAL_CONSIDERATION = "research_ethics_awareness"
    FUTURE_CONTRIBUTION = "field_advancement_potential"


@dataclass
class AcademicRubric:
    """Comprehensive academic assessment rubric."""
    criterion_name: str
    description: str
    excellent_threshold: float  # 90-100
    proficient_threshold: float  # 80-89
    developing_threshold: float  # 70-79
    inadequate_threshold: float  # Below 70
    weight: float  # Relative importance
    assessment_method: str
    examples_excellent: List[str]
    examples_proficient: List[str]
    common_weaknesses: List[str]
    improvement_strategies: List[str]


@dataclass
class ConsensusMetrics:
    """Advanced consensus analysis metrics."""
    inter_rater_reliability: float  # Cohen's kappa
    correlation_coefficient: float  # Pearson correlation
    rank_correlation: float  # Spearman correlation
    agreement_percentage: float
    variance_analysis: Dict[str, float]
    outlier_detection: List[str]
    confidence_interval: Tuple[float, float]
    consensus_strength: str  # "strong", "moderate", "weak"
    disagreement_analysis: Dict[str, Any]
    model_bias_assessment: Dict[str, float]


@dataclass
class QualityDimension:
    """Sophisticated quality assessment dimension."""
    dimension_name: str
    score: float
    confidence: float
    evidence: List[str]
    weaknesses: List[str]
    strengths: List[str]
    improvement_recommendations: List[str]
    comparative_analysis: Dict[str, float]
    threshold_analysis: Dict[str, bool]
    future_potential: float


@dataclass
class ComprehensiveEvaluation:
    """Revolutionary comprehensive evaluation result."""
    # Overall assessment
    overall_score: float
    confidence_level: float
    assessment_timestamp: datetime
    
    # Multi-dimensional quality analysis
    quality_dimensions: List[QualityDimension]
    
    # Model-specific evaluations
    gemini_evaluation: Dict[str, Any]
    grok_evaluation: Dict[str, Any]
    o3_evaluation: Dict[str, Any]
    
    # Consensus analysis
    consensus_metrics: ConsensusMetrics
    
    # Academic excellence indicators
    academic_level_assessment: str  # "undergraduate", "graduate", "doctoral", "postdoctoral"
    field_appropriateness: float
    theoretical_sophistication: float
    methodological_awareness: float
    
    # Revision analysis
    revision_necessity: bool
    revision_priority: str  # "critical", "important", "minor", "none"
    specific_revision_targets: List[Dict[str, Any]]
    
    # Comparative benchmarking
    peer_comparison_percentile: float
    field_standard_comparison: Dict[str, float]
    historical_trend_analysis: Dict[str, Any]
    
    # Future-oriented assessment
    potential_impact: float
    scalability_assessment: float
    innovation_quotient: float
    
    # Learning outcome alignment
    learning_outcome_coverage: Dict[str, float]
    skill_demonstration: Dict[str, float]
    knowledge_application: Dict[str, float]


class RevolutionaryMultiModelEvaluator(BaseNode):
    """
    Revolutionary Multi-Model Evaluator with PhD-level assessment capabilities.
    
    Revolutionary Capabilities:
    - Advanced inter-rater reliability analysis
    - Sophisticated consensus mechanism with confidence intervals
    - Multi-dimensional quality assessment framework
    - Adaptive rubric selection based on academic level
    - Bias detection and mitigation across AI models
    - Predictive assessment of academic potential
    - Real-time calibration against academic standards
    - Longitudinal learning pattern analysis
    """
    
    def __init__(self):
        super().__init__("revolutionary_multi_model_evaluator")
        
        # Initialize AI model clients
        evaluation_config = get_model_config("evaluation")
        self.gemini_client = get_llm_client("evaluation", evaluation_config["primary"])
        self.grok_client = get_llm_client("evaluation", evaluation_config["secondary"])
        self.o3_client = get_llm_client("evaluation", evaluation_config["tertiary"])
        
        # Advanced assessment systems
        self.academic_rubrics = self._initialize_academic_rubrics()
        self.field_specific_standards = self._load_field_standards()
        self.consensus_algorithms = self._initialize_consensus_algorithms()
        self.bias_detection_systems = self._initialize_bias_detection()
        
        # Learning and calibration systems
        self.assessment_history = {}
        self.model_performance_tracking = {}
        self.calibration_benchmarks = {}
        self.quality_prediction_models = {}
        self.casp_appraisal_tool = CASPAppraisalTool()

    def _initialize_academic_rubrics(self):
        return []

    def _load_field_standards(self):
        return {}

    def _initialize_consensus_algorithms(self):
        return {}

    def _initialize_bias_detection(self):
        return {}
        
    async def execute(self, state: HandyWriterzState, config: RunnableConfig) -> Dict[str, Any]:
        """Execute the node logic by calling the main __call__ method."""
        return await self(state, config)
    
    async def __call__(self, state: HandyWriterzState, config: RunnableConfig) -> Dict[str, Any]:
        """Execute revolutionary multi-model evaluation with PhD-level consensus."""
        try:
            await self.broadcast_progress(state, "advanced_evaluation", "starting", 0,
                                        "Initializing PhD-level multi-model evaluation...")
            
            # Perform CASP appraisal if applicable
            if state.get("filtered_studies"):
                casp_appraisal_table = self._perform_casp_appraisal(state)
                state["casp_appraisal_table"] = casp_appraisal_table.to_dict("records")

            # Extract evaluation context
            evaluation_context = await self._extract_evaluation_context(state)
            
            await self.broadcast_progress(state, "advanced_evaluation", "in_progress", 10,
                                        "Calibrating assessment rubrics...")
            
            # Calibrate assessment rubrics
            calibrated_rubrics = await self._calibrate_assessment_rubrics(evaluation_context)
            
            await self.broadcast_progress(state, "advanced_evaluation", "in_progress", 25,
                                        "Executing parallel model evaluations...")
            
            # Execute sophisticated parallel evaluations
            model_evaluations = await self._execute_parallel_evaluations(state, calibrated_rubrics)
            
            await self.broadcast_progress(state, "advanced_evaluation", "in_progress", 60,
                                        "Performing consensus analysis...")
            
            # Perform advanced consensus analysis
            consensus_result = await self._perform_advanced_consensus_analysis(model_evaluations)
            
            await self.broadcast_progress(state, "advanced_evaluation", "in_progress", 80,
                                        "Generating comprehensive assessment...")
            
            # Generate comprehensive evaluation
            comprehensive_evaluation = await self._generate_comprehensive_evaluation(
                model_evaluations, consensus_result, evaluation_context
            )
            
            await self.broadcast_progress(state, "advanced_evaluation", "in_progress", 95,
                                        "Finalizing quality recommendations...")
            
            # Generate sophisticated recommendations
            recommendations = await self._generate_sophisticated_recommendations(comprehensive_evaluation)
            
            await self.broadcast_progress(state, "advanced_evaluation", "completed", 100,
                                        f"Advanced evaluation complete: {comprehensive_evaluation.overall_score:.1f}/100")
            
            return {
                "comprehensive_evaluation": asdict(comprehensive_evaluation),
                "evaluation_score": comprehensive_evaluation.overall_score,
                "needs_revision": comprehensive_evaluation.revision_necessity,
                "revision_priority": comprehensive_evaluation.revision_priority,
                "specific_recommendations": recommendations,
                "consensus_analysis": asdict(comprehensive_evaluation.consensus_metrics),
                "quality_breakdown": {dim.dimension_name: dim.score for dim in comprehensive_evaluation.quality_dimensions},
                "academic_level": comprehensive_evaluation.academic_level_assessment,
                "future_potential": comprehensive_evaluation.potential_impact,
                "learning_outcomes": comprehensive_evaluation.learning_outcome_coverage,
                "casp_appraisal_table": state.get("casp_appraisal_table", [])
            }
            
        except Exception as e:
            logger.error(f"Revolutionary multi-model evaluation failed: {e}")
            await self.broadcast_progress(state, "advanced_evaluation", "failed", 0,
                                        f"Advanced evaluation failed: {str(e)}")
            return {"evaluation_score": 0, "needs_revision": True, "error": str(e)}
    
    async def _extract_evaluation_context(self, state: HandyWriterzState) -> Dict[str, Any]:
        """Extract sophisticated evaluation context."""
        current_draft = state.get("current_draft", "")
        user_params = state.get("user_params", {})
        verified_sources = state.get("verified_sources", [])
        uploaded_docs = state.get("uploaded_docs", [])
        
        # Analyze draft characteristics
        draft_analysis = await self._analyze_draft_characteristics(current_draft, user_params)
        
        return {
            "draft_content": current_draft,
            "user_parameters": user_params,
            "source_quality": len(verified_sources),
            "draft_characteristics": draft_analysis,
            "academic_field": user_params.get("field", "general"),
            "assignment_type": user_params.get("writeupType", "essay"),
            "target_word_count": user_params.get("wordCount", 1000),
            "citation_style": user_params.get("citationStyle", "harvard"),
            "academic_level": self._infer_academic_level(user_params, current_draft),
            "evaluation_timestamp": datetime.now(),
            "context_documents": len(uploaded_docs)
        }
    
    async def _calibrate_assessment_rubrics(self, context: Dict[str, Any]) -> List[AcademicRubric]:
        """Calibrate assessment rubrics based on context."""
        calibration_prompt = f"""
        As a world-class assessment expert and educational psychologist, calibrate evaluation rubrics for:
        
        Context: {json.dumps(context, indent=2)}
        
        Design sophisticated rubrics for:
        
        1. THEORETICAL SOPHISTICATION
        - Depth of theoretical understanding
        - Integration of multiple frameworks
        - Critical engagement with theory
        - Original theoretical insights
        
        2. EMPIRICAL RIGOR
        - Evidence quality and relevance
        - Methodological awareness
        - Data interpretation skills
        - Research validity understanding
        
        3. ANALYTICAL DEPTH
        - Critical thinking demonstration
        - Argument complexity
        - Synthesis capabilities
        - Evaluation skills
        
        4. ARGUMENTATIVE COHERENCE
        - Logical structure clarity
        - Premise-conclusion alignment
        - Counterargument consideration
        - Persuasive effectiveness
        
        5. SCHOLARLY COMMUNICATION
        - Academic writing conventions
        - Citation accuracy and style
        - Clarity and precision
        - Professional presentation
        
        For each rubric, specify:
        - Performance level thresholds
        - Assessment criteria
        - Quality indicators
        - Common weaknesses
        - Improvement strategies
        
        Calibrate for {context.get('academic_level', 'undergraduate')} level in {context.get('academic_field', 'general')}.
        """
        
        try:
            response = await self.gemini_client.ainvoke(
                [{"role": "user", "content": calibration_prompt}]
            )
            
            return self._parse_calibrated_rubrics(response.content, context)
            
        except Exception as e:
            logger.error(f"Rubric calibration failed: {e}")
            return self._get_default_rubrics(context)
    
    async def _execute_parallel_evaluations(self, state: HandyWriterzState, 
                                          rubrics: List[AcademicRubric]) -> Dict[str, Dict[str, Any]]:
        """Execute sophisticated parallel evaluations across multiple AI models."""
        current_draft = state.get("current_draft", "")
        evaluation_context = await self._extract_evaluation_context(state)
        
        # Create evaluation tasks
        evaluation_tasks = [
            self._evaluate_with_gemini_advanced(current_draft, rubrics, evaluation_context),
            self._evaluate_with_grok_advanced(current_draft, rubrics, evaluation_context),
            self._evaluate_with_o3_advanced(current_draft, rubrics, evaluation_context)
        ]
        
        # Execute evaluations in parallel
        results = await asyncio.gather(*evaluation_tasks, return_exceptions=True)
        
        # Process results
        model_evaluations = {}
        model_names = ["gemini", "grok", "openai"]
        
        for i, result in enumerate(results):
            if isinstance(result, dict) and not isinstance(result, Exception):
                model_evaluations[model_names[i]] = result
            else:
                logger.warning(f"{model_names[i]} evaluation failed: {result}")
                model_evaluations[model_names[i]] = self._create_fallback_evaluation()
        
        return model_evaluations
    
    async def _evaluate_with_gemini_advanced(self, draft: str, rubrics: List[AcademicRubric], 
                                           context: Dict[str, Any]) -> Dict[str, Any]:
        """Advanced Gemini evaluation with sophisticated analysis."""
        evaluation_prompt = f"""
        As a distinguished academic evaluator with expertise in {context.get('academic_field')}, 
        perform comprehensive evaluation of this academic work:
        
        CONTENT TO EVALUATE:
        {draft}
        
        EVALUATION CONTEXT:
        {json.dumps(context, indent=2)}
        
        ASSESSMENT RUBRICS:
        {self._format_rubrics_for_prompt(rubrics)}
        
        Perform systematic evaluation across ALL dimensions:
        
        1. THEORETICAL SOPHISTICATION (30 points)
        - Theoretical framework mastery
        - Conceptual integration depth
        - Critical theoretical engagement
        - Original theoretical insights
        
        2. EMPIRICAL RIGOR (25 points)
        - Evidence quality assessment
        - Methodological awareness
        - Data interpretation skills
        - Research validity understanding
        
        3. ANALYTICAL DEPTH (25 points)
        - Critical thinking sophistication
        - Argument complexity analysis
        - Synthesis capabilities
        - Evaluation and judgment skills
        
        4. SCHOLARLY COMMUNICATION (20 points)
        - Academic writing excellence
        - Citation accuracy and style
        - Clarity and precision
        - Professional presentation
        
        For EACH dimension, provide:
        - Numerical score (0-max points)
        - Detailed justification
        - Specific strengths identified
        - Specific weaknesses identified
        - Targeted improvement recommendations
        
        Calculate TOTAL SCORE out of 100 points.
        
        Provide PhD-level analytical depth with specific examples from the text.
        """
        
        try:
            response = await self.gemini_client.ainvoke([
                {"role": "user", "content": evaluation_prompt}
            ])
            
            content = response.content
            
            return {
                "model": "gemini",
                "evaluation_text": content,
                "scores": self._extract_detailed_scores(content),
                "strengths": self._extract_strengths(content),
                "weaknesses": self._extract_weaknesses(content),
                "recommendations": self._extract_recommendations(content),
                "overall_score": self._extract_overall_score(content),
                "confidence": self._assess_evaluation_confidence(content),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Gemini advanced evaluation failed: {e}")
            return self._create_fallback_evaluation("gemini")
    
    async def _evaluate_with_grok_advanced(self, draft: str, rubrics: List[AcademicRubric],
                                           context: Dict[str, Any]) -> Dict[str, Any]:
        """Advanced Grok evaluation with critical analysis focus."""
        evaluation_prompt = f"""
        As a master critic and academic assessment expert, conduct rigorous evaluation of this work:

        ACADEMIC WORK:
        {draft}

        EVALUATION PARAMETERS:
        {json.dumps(context, indent=2)}

        Apply sophisticated critical analysis across these dimensions:

        1. ARGUMENTATIVE EXCELLENCE (35%)
        - Logical structure and coherence
        - Premise quality and support
        - Counterargument consideration
        - Persuasive effectiveness
        - Fallacy identification

        2. CRITICAL THINKING DEPTH (30%)
        - Analysis sophistication
        - Synthesis capabilities
        - Evaluation and judgment
        - Original insights
        - Intellectual courage

        3. SCHOLARLY RIGOR (25%)
        - Research methodology awareness
        - Evidence integration quality
        - Citation accuracy and completeness
        - Academic convention adherence
        - Ethical consideration

        4. COMMUNICATION MASTERY (10%)
        - Clarity and precision
        - Academic tone appropriateness
        - Structural organization
        - Professional presentation

        For each dimension:
        - Assign percentage score (0-100%)
        - Provide detailed analytical justification
        - Identify specific textual evidence
        - Note critical strengths and limitations
        - Suggest sophisticated improvements

        Apply the highest standards of academic excellence appropriate for {context.get('academic_level')} level.
        """

        try:
            response = await self.grok_client.ainvoke(
                [{"role": "user", "content": evaluation_prompt}]
            )

            content = response.content

            return {
                "model": "grok",
                "evaluation_text": content,
                "scores": self._extract_detailed_scores(content),
                "strengths": self._extract_strengths(content),
                "weaknesses": self._extract_weaknesses(content),
                "recommendations": self._extract_recommendations(content),
                "overall_score": self._extract_overall_score(content),
                "confidence": self._assess_evaluation_confidence(content),
                "critical_analysis": self._extract_critical_insights(content),
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"Grok advanced evaluation failed: {e}")
            return self._create_fallback_evaluation("grok")
    
    async def _evaluate_with_o3_advanced(self, draft: str, rubrics: List[AcademicRubric], 
                                       context: Dict[str, Any]) -> Dict[str, Any]:
        """Advanced O3 evaluation with sophisticated reasoning."""
        evaluation_prompt = f"""
        Apply advanced reasoning to comprehensively evaluate this academic work:
        
        ACADEMIC CONTENT:
        {draft}
        
        EVALUATION CONTEXT:
        {json.dumps(context, indent=2)}
        
        Use sophisticated reasoning to assess:
        
        1. REASONING QUALITY (40%)
        - Logical consistency and validity
        - Argument strength and structure
        - Evidence-conclusion alignment
        - Inference appropriateness
        - Assumption identification
        
        2. KNOWLEDGE INTEGRATION (30%)
        - Disciplinary knowledge demonstration
        - Cross-domain synthesis
        - Theoretical framework application
        - Contemporary relevance
        - Historical awareness
        
        3. METHODOLOGICAL SOPHISTICATION (20%)
        - Research approach appropriateness
        - Data interpretation skills
        - Analytical method awareness
        - Validity consideration
        - Limitation acknowledgment
        
        4. INNOVATION POTENTIAL (10%)
        - Original thinking demonstration
        - Creative problem-solving
        - Novel perspective contribution
        - Future research implications
        - Paradigm advancement potential
        
        Apply advanced reasoning to:
        - Identify subtle logical patterns
        - Detect implicit assumptions
        - Evaluate reasoning chains
        - Assess knowledge integration
        - Predict academic impact
        
        Provide detailed scores with sophisticated reasoning justification.
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="o3-mini",
                messages=[{"role": "user", "content": evaluation_prompt}],
                temperature=0.1,
                max_tokens=2500
            )
            
            content = response.choices[0].message.content
            
            return {
                "model": "openai",
                "evaluation_text": content,
                "scores": self._extract_detailed_scores(content),
                "strengths": self._extract_strengths(content),
                "weaknesses": self._extract_weaknesses(content),
                "recommendations": self._extract_recommendations(content),
                "overall_score": self._extract_overall_score(content),
                "confidence": self._assess_evaluation_confidence(content),
                "reasoning_analysis": self._extract_reasoning_insights(content),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"O3 advanced evaluation failed: {e}")
            return self._create_fallback_evaluation("openai")
    
    async def _perform_advanced_consensus_analysis(self, evaluations: Dict[str, Dict[str, Any]]) -> ConsensusMetrics:
        """Perform sophisticated consensus analysis with statistical rigor."""
        if len(evaluations) < 2:
            return self._create_minimal_consensus()
        
        # Extract scores for analysis
        scores = []
        models = []
        
        for model, evaluation in evaluations.items():
            score = evaluation.get("overall_score", 0)
            if score > 0:
                scores.append(score)
                models.append(model)
        
        if len(scores) < 2:
            return self._create_minimal_consensus()
        
        # Calculate sophisticated consensus metrics
        try:
            # Inter-rater reliability (using Cohen's kappa approximation)
            mean_score = np.mean(scores)
            agreements = [abs(score - mean_score) <= 5 for score in scores]  # Within 5 points
            agreement_rate = np.mean(agreements)
            
            # Correlation analysis
            if len(scores) >= 3:
                correlations = []
                for i in range(len(scores)):
                    for j in range(i + 1, len(scores)):
                        corr, _ = pearsonr([scores[i]], [scores[j]])
                        if not np.isnan(corr):
                            correlations.append(corr)
                correlation_coeff = np.mean(correlations) if correlations else 0.0
            else:
                correlation_coeff, _ = pearsonr(scores[:2], scores[:2]) if len(scores) == 2 else (0.0, 1.0)
                if np.isnan(correlation_coeff):
                    correlation_coeff = 0.0
            
            # Variance analysis
            score_variance = np.var(scores)
            score_std = np.std(scores)
            
            # Confidence interval
            confidence_margin = 1.96 * score_std / np.sqrt(len(scores))
            conf_lower = max(0, mean_score - confidence_margin)
            conf_upper = min(100, mean_score + confidence_margin)
            
            # Consensus strength assessment
            if score_std <= 3:
                consensus_strength = "strong"
            elif score_std <= 8:
                consensus_strength = "moderate"
            else:
                consensus_strength = "weak"
            
            # Outlier detection
            outliers = []
            z_scores = np.abs(zscore(scores))
            for i, z in enumerate(z_scores):
                if z > 2:  # More than 2 standard deviations
                    outliers.append(models[i])
            
            return ConsensusMetrics(
                inter_rater_reliability=agreement_rate,
                correlation_coefficient=correlation_coeff,
                rank_correlation=correlation_coeff,  # Simplified
                agreement_percentage=agreement_rate * 100,
                variance_analysis={"variance": score_variance, "std_dev": score_std},
                outlier_detection=outliers,
                confidence_interval=(conf_lower, conf_upper),
                consensus_strength=consensus_strength,
                disagreement_analysis=self._analyze_disagreements(evaluations),
                model_bias_assessment=self._assess_model_biases(evaluations)
            )
            
        except Exception as e:
            logger.error(f"Consensus analysis failed: {e}")
            return self._create_minimal_consensus()
    
    # Helper methods for parsing and analysis
    def _extract_detailed_scores(self, text: str) -> Dict[str, float]:
        """Extract detailed scores from evaluation text."""
        scores = {}
        
        import re
        
        # Various score patterns
        patterns = [
            r'(\w+(?:\s+\w+)*)\s*[:\-]\s*([0-9]*\.?[0-9]+)',
            r'([0-9]*\.?[0-9]+)\s*(?:/|out\s+of)\s*([0-9]+)',
            r'([0-9]*\.?[0-9]+)%'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text.lower())
            for match in matches:
                if len(match) == 2:
                    try:
                        if pattern == patterns[1]:  # "score out of X" format
                            score = float(match[0]) / float(match[1]) * 100
                        elif pattern == patterns[2]:  # percentage format
                            score = float(match[0])
                        else:  # "dimension: score" format
                            score = float(match[1])
                        
                        key = match[0] if pattern != patterns[2] else f"score_{len(scores)}"
                        scores[key.replace(' ', '_')] = min(100, max(0, score))
                    except ValueError:
                        continue
        
        return scores
    
    def _extract_overall_score(self, text: str) -> float:
        """Extract overall score from evaluation text."""
        import re
        
        # Look for overall score patterns
        patterns = [
            r'overall\s+score\s*[:\-]\s*([0-9]*\.?[0-9]+)',
            r'total\s+score\s*[:\-]\s*([0-9]*\.?[0-9]+)',
            r'final\s+score\s*[:\-]\s*([0-9]*\.?[0-9]+)',
            r'([0-9]*\.?[0-9]+)\s*/\s*100',
            r'([0-9]*\.?[0-9]+)%\s*overall'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text.lower())
            if match:
                try:
                    score = float(match.group(1))
                    return min(100, max(0, score))
                except ValueError:
                    continue
        
        # Default to average of found scores
        scores = self._extract_detailed_scores(text)
        if scores:
            return min(100, max(0, np.mean(list(scores.values()))))
        
        return 75.0  # Default moderate score
    
    def _create_fallback_evaluation(self, model: str = "unknown") -> Dict[str, Any]:
        """Create fallback evaluation when model evaluation fails."""
        return {
            "model": model,
            "evaluation_text": "Evaluation failed - using fallback assessment",
            "scores": {"overall": 70, "reasoning": 70, "communication": 70},
            "strengths": ["Content present", "Basic structure"],
            "weaknesses": ["Evaluation system failure", "Unable to assess thoroughly"],
            "recommendations": ["Retry evaluation", "Manual review recommended"],
            "overall_score": 70,
            "confidence": 0.3,
            "timestamp": datetime.now().isoformat(),
            "error": "Model evaluation failed"
        }
    
    # Missing method implementations
    
    async def _analyze_draft_characteristics(self, draft: str, user_params: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze sophisticated characteristics of the draft."""
        if not draft:
            return {"length": 0, "complexity": 0.0, "academic_indicators": 0}
        
        # Basic metrics
        word_count = len(draft.split())
        sentence_count = len([s for s in draft.split('.') if s.strip()])
        paragraph_count = len([p for p in draft.split('\n\n') if p.strip()])
        
        # Academic indicators
        academic_keywords = [
            'however', 'therefore', 'furthermore', 'moreover', 'consequently',
            'analysis', 'research', 'study', 'evidence', 'theory', 'hypothesis',
            'methodology', 'conclusion', 'argument', 'critique', 'evaluation'
        ]
        academic_score = sum(1 for keyword in academic_keywords if keyword.lower() in draft.lower())
        
        # Citation indicators
        citation_patterns = ['(', ')', '[', ']', 'et al.', '19', '20']
        citation_score = sum(1 for pattern in citation_patterns if pattern in draft)
        
        # Complexity assessment
        avg_sentence_length = word_count / max(1, sentence_count)
        complexity_score = min(1.0, (avg_sentence_length + academic_score + citation_score) / 20)
        
        return {
            "word_count": word_count,
            "sentence_count": sentence_count,
            "paragraph_count": paragraph_count,
            "avg_sentence_length": avg_sentence_length,
            "academic_indicator_score": academic_score,
            "citation_indicator_score": citation_score,
            "complexity_score": complexity_score,
            "readability_estimate": self._estimate_readability(draft),
            "structure_quality": self._assess_structure_quality(draft)
        }
    
    def _infer_academic_level(self, user_params: Dict[str, Any], draft: str) -> str:
        """Infer academic level from parameters and content."""
        # Check explicit parameters first
        if "academic_level" in user_params:
            return user_params["academic_level"]
        
        # Analyze content complexity
        if not draft:
            return "undergraduate"
        
        complexity_indicators = {
            "theoretical": ["theory", "framework", "paradigm", "epistemology", "ontology"],
            "methodological": ["methodology", "empirical", "qualitative", "quantitative", "meta-analysis"],
            "advanced": ["sophisticated", "nuanced", "complex", "multifaceted", "interdisciplinary"],
            "research": ["hypothesis", "variable", "correlation", "significance", "validity"]
        }
        
        total_score = 0
        for category, keywords in complexity_indicators.items():
            score = sum(1 for keyword in keywords if keyword.lower() in draft.lower())
            total_score += score
        
        word_count = len(draft.split())
        
        if total_score >= 8 or word_count > 3000:
            return "doctoral"
        elif total_score >= 5 or word_count > 1500:
            return "graduate"
        else:
            return "undergraduate"
    
    def _estimate_readability(self, text: str) -> float:
        """Estimate readability score (Flesch-Kincaid approximation)."""
        if not text:
            return 0.0
        
        words = text.split()
        sentences = [s for s in text.split('.') if s.strip()]
        syllables = sum(self._count_syllables(word) for word in words)
        
        if not sentences or not words:
            return 0.0
        
        avg_sentence_length = len(words) / len(sentences)
        avg_syllables_per_word = syllables / len(words)
        
        # Simplified Flesch-Kincaid Grade Level
        grade_level = 0.39 * avg_sentence_length + 11.8 * avg_syllables_per_word - 15.59
        return max(0.0, min(20.0, grade_level))
    
    def _count_syllables(self, word: str) -> int:
        """Estimate syllable count for a word."""
        word = word.lower().strip('.,!?;:"')
        if not word:
            return 0
        
        vowels = 'aeiouy'
        syllable_count = 0
        prev_was_vowel = False
        
        for char in word:
            if char in vowels:
                if not prev_was_vowel:
                    syllable_count += 1
                prev_was_vowel = True
            else:
                prev_was_vowel = False
        
        # Handle silent 'e'
        if word.endswith('e') and syllable_count > 1:
            syllable_count -= 1
        
        return max(1, syllable_count)
    
    def _assess_structure_quality(self, text: str) -> float:
        """Assess structural quality of the text."""
        if not text:
            return 0.0
        
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        
        # Structure indicators
        has_introduction = any('introduction' in p.lower() or 'intro' in p.lower() for p in paragraphs[:2])
        has_conclusion = any('conclusion' in p.lower() or 'summary' in p.lower() for p in paragraphs[-2:])
        
        # Paragraph balance
        if paragraphs:
            avg_paragraph_length = sum(len(p.split()) for p in paragraphs) / len(paragraphs)
            paragraph_balance = 1.0 - abs(avg_paragraph_length - 100) / 200  # Optimal ~100 words
        else:
            paragraph_balance = 0.0
        
        # Transition indicators
        transitions = ['however', 'furthermore', 'therefore', 'moreover', 'consequently', 'additionally']
        transition_score = sum(1 for t in transitions if t in text.lower()) / max(1, len(paragraphs))
        
        structure_score = (
            (0.3 if has_introduction else 0.0) +
            (0.3 if has_conclusion else 0.0) +
            (0.2 * max(0.0, min(1.0, paragraph_balance))) +
            (0.2 * min(1.0, transition_score))
        )
        
        return structure_score
    
    def _parse_calibrated_rubrics(self, rubric_text: str, context: Dict[str, Any]) -> List[AcademicRubric]:
        """Parse calibrated rubrics from AI response."""
        # Default rubrics if parsing fails
        return self._get_default_rubrics(context)
    
    def _get_default_rubrics(self, context: Dict[str, Any]) -> List[AcademicRubric]:
        """Get default academic rubrics."""
        academic_level = context.get("academic_level", "undergraduate")
        
        # Adjust thresholds based on academic level
        if academic_level == "doctoral":
            excellent_base, proficient_base, developing_base = 92, 85, 78
        elif academic_level == "graduate":
            excellent_base, proficient_base, developing_base = 90, 82, 75
        else:  # undergraduate
            excellent_base, proficient_base, developing_base = 88, 80, 72
        
        return [
            AcademicRubric(
                criterion_name="Theoretical Sophistication",
                description="Depth of theoretical understanding and application",
                excellent_threshold=excellent_base,
                proficient_threshold=proficient_base,
                developing_threshold=developing_base,
                inadequate_threshold=developing_base - 10,
                weight=0.30,
                assessment_method="content_analysis",
                examples_excellent=["Complex theoretical integration", "Original theoretical insights"],
                examples_proficient=["Good theoretical understanding", "Appropriate theory application"],
                common_weaknesses=["Superficial theory use", "Misapplication of concepts"],
                improvement_strategies=["Deepen theoretical reading", "Practice theory application"]
            ),
            AcademicRubric(
                criterion_name="Analytical Depth",
                description="Critical thinking and analytical sophistication",
                excellent_threshold=excellent_base,
                proficient_threshold=proficient_base,
                developing_threshold=developing_base,
                inadequate_threshold=developing_base - 10,
                weight=0.25,
                assessment_method="reasoning_analysis",
                examples_excellent=["Sophisticated critical analysis", "Multi-perspective evaluation"],
                examples_proficient=["Good analytical thinking", "Clear reasoning chains"],
                common_weaknesses=["Surface-level analysis", "Missing critical evaluation"],
                improvement_strategies=["Practice critical questioning", "Develop analytical frameworks"]
            ),
            AcademicRubric(
                criterion_name="Empirical Rigor",
                description="Evidence quality and research methodology awareness",
                excellent_threshold=excellent_base,
                proficient_threshold=proficient_base,
                developing_threshold=developing_base,
                inadequate_threshold=developing_base - 10,
                weight=0.25,
                assessment_method="evidence_evaluation",
                examples_excellent=["High-quality evidence synthesis", "Methodological sophistication"],
                examples_proficient=["Appropriate evidence use", "Good source selection"],
                common_weaknesses=["Weak evidence support", "Poor source quality"],
                improvement_strategies=["Improve source evaluation", "Learn research methods"]
            ),
            AcademicRubric(
                criterion_name="Scholarly Communication",
                description="Academic writing excellence and communication clarity",
                excellent_threshold=excellent_base,
                proficient_threshold=proficient_base,
                developing_threshold=developing_base,
                inadequate_threshold=developing_base - 10,
                weight=0.20,
                assessment_method="communication_assessment",
                examples_excellent=["Exceptional clarity and precision", "Perfect academic conventions"],
                examples_proficient=["Clear academic writing", "Good structure and flow"],
                common_weaknesses=["Unclear expression", "Poor academic style"],
                improvement_strategies=["Practice academic writing", "Study style guides"]
            )
        ]
    
    def _format_rubrics_for_prompt(self, rubrics: List[AcademicRubric]) -> str:
        """Format rubrics for AI prompt."""
        formatted = "ASSESSMENT RUBRICS:\n\n"
        for rubric in rubrics:
            formatted += f"**{rubric.criterion_name}** (Weight: {rubric.weight:.0%})\n"
            formatted += f"Description: {rubric.description}\n"
            formatted += f"Excellent: {rubric.excellent_threshold}+ points\n"
            formatted += f"Proficient: {rubric.proficient_threshold}-{rubric.excellent_threshold-1} points\n"
            formatted += f"Developing: {rubric.developing_threshold}-{rubric.proficient_threshold-1} points\n"
            formatted += f"Inadequate: Below {rubric.developing_threshold} points\n\n"
        return formatted
    
    def _extract_strengths(self, text: str) -> List[str]:
        """Extract strengths from evaluation text."""
        strengths = []
        import re
        
        # Look for strength patterns
        patterns = [
            r'strength[s]?\s*[:\-]\s*([^\.]+)',
            r'positive[s]?\s*[:\-]\s*([^\.]+)',
            r'excellent\s+([^\.]+)',
            r'strong\s+([^\.]+)',
            r'good\s+([^\.]+)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text.lower())
            for match in matches:
                strength = match.strip()
                if len(strength) > 5 and strength not in strengths:
                    strengths.append(strength.capitalize())
        
        return strengths[:10]  # Limit to 10 strengths
    
    def _extract_weaknesses(self, text: str) -> List[str]:
        """Extract weaknesses from evaluation text."""
        weaknesses = []
        import re
        
        # Look for weakness patterns
        patterns = [
            r'weakness[es]?\s*[:\-]\s*([^\.]+)',
            r'limitation[s]?\s*[:\-]\s*([^\.]+)',
            r'problem[s]?\s*[:\-]\s*([^\.]+)',
            r'needs?\s+improvement\s*[:\-]\s*([^\.]+)',
            r'poor\s+([^\.]+)',
            r'weak\s+([^\.]+)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text.lower())
            for match in matches:
                weakness = match.strip()
                if len(weakness) > 5 and weakness not in weaknesses:
                    weaknesses.append(weakness.capitalize())
        
        return weaknesses[:10]  # Limit to 10 weaknesses
    
    def _extract_recommendations(self, text: str) -> List[str]:
        """Extract recommendations from evaluation text."""
        recommendations = []
        import re
        
        # Look for recommendation patterns
        patterns = [
            r'recommend[ation]*[s]?\s*[:\-]\s*([^\.]+)',
            r'suggest[ion]*[s]?\s*[:\-]\s*([^\.]+)',
            r'should\s+([^\.]+)',
            r'could\s+improve\s+([^\.]+)',
            r'consider\s+([^\.]+)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text.lower())
            for match in matches:
                recommendation = match.strip()
                if len(recommendation) > 10 and recommendation not in recommendations:
                    recommendations.append(recommendation.capitalize())
        
        return recommendations[:8]  # Limit to 8 recommendations
    
    def _assess_evaluation_confidence(self, text: str) -> float:
        """Assess confidence level of the evaluation."""
        confidence_indicators = {
            "high": ["clearly", "definitely", "certainly", "obvious", "evident"],
            "medium": ["likely", "probably", "generally", "typically"],
            "low": ["perhaps", "possibly", "might", "uncertain", "unclear"]
        }
        
        high_count = sum(1 for word in confidence_indicators["high"] if word in text.lower())
        medium_count = sum(1 for word in confidence_indicators["medium"] if word in text.lower())
        low_count = sum(1 for word in confidence_indicators["low"] if word in text.lower())
        
        total_indicators = high_count + medium_count + low_count
        if total_indicators == 0:
            return 0.7  # Default moderate confidence
        
        confidence_score = (high_count * 1.0 + medium_count * 0.6 + low_count * 0.3) / total_indicators
        return min(1.0, max(0.1, confidence_score))
    
    def _extract_critical_insights(self, text: str) -> List[str]:
        """Extract critical insights from evaluation text."""
        insights = []
        import re
        
        # Look for insight patterns
        patterns = [
            r'insight[s]?\s*[:\-]\s*([^\.]+)',
            r'notably?\s*[,:]?\s*([^\.]+)',
            r'importantly?\s*[,:]?\s*([^\.]+)',
            r'significantly?\s*[,:]?\s*([^\.]+)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text.lower())
            for match in matches:
                insight = match.strip()
                if len(insight) > 10 and insight not in insights:
                    insights.append(insight.capitalize())
        
        return insights[:5]  # Limit to 5 insights
    
    def _extract_reasoning_insights(self, text: str) -> List[str]:
        """Extract reasoning insights from evaluation text."""
        insights = []
        import re
        
        # Look for reasoning patterns
        patterns = [
            r'reasoning\s*[:\-]\s*([^\.]+)',
            r'logic[al]*[ly]?\s*[:\-]\s*([^\.]+)',
            r'argument[ation]*\s*[:\-]\s*([^\.]+)',
            r'inference[s]?\s*[:\-]\s*([^\.]+)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text.lower())
            for match in matches:
                insight = match.strip()
                if len(insight) > 10 and insight not in insights:
                    insights.append(insight.capitalize())
        
        return insights[:5]  # Limit to 5 insights
    
    def _create_minimal_consensus(self) -> ConsensusMetrics:
        """Create minimal consensus when insufficient data."""
        return ConsensusMetrics(
            inter_rater_reliability=0.5,
            correlation_coefficient=0.5,
            rank_correlation=0.5,
            agreement_percentage=50.0,
            variance_analysis={"variance": 25.0, "std_dev": 5.0},
            outlier_detection=[],
            confidence_interval=(70.0, 80.0),
            consensus_strength="weak",
            disagreement_analysis={},
            model_bias_assessment={}
        )
    
    def _analyze_disagreements(self, evaluations: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze disagreements between model evaluations."""
        scores = [eval_data.get("overall_score", 0) for eval_data in evaluations.values()]
        
        if len(scores) < 2:
            return {"disagreement_level": "none", "analysis": "Insufficient data"}
        
        score_range = max(scores) - min(scores)
        
        if score_range <= 5:
            level = "minimal"
        elif score_range <= 15:
            level = "moderate"
        else:
            level = "significant"
        
        return {
            "disagreement_level": level,
            "score_range": score_range,
            "highest_score": max(scores),
            "lowest_score": min(scores),
            "analysis": f"{level.capitalize()} disagreement with {score_range:.1f} point range"
        }
    
    def _assess_model_biases(self, evaluations: Dict[str, Dict[str, Any]]) -> Dict[str, float]:
        """Assess potential biases in model evaluations."""
        bias_assessment = {}
        
        for model, evaluation in evaluations.items():
            score = evaluation.get("overall_score", 0)
            confidence = evaluation.get("confidence", 0.5)
            
            # Simple bias indicators
            if score > 90:
                bias_assessment[model] = 0.8  # Potentially lenient
            elif score < 60:
                bias_assessment[model] = 0.7  # Potentially harsh
            elif confidence < 0.3:
                bias_assessment[model] = 0.6  # Low confidence may indicate bias
            else:
                bias_assessment[model] = 0.2  # Low bias indication
        
        return bias_assessment
    
    async def broadcast_progress(self, state: HandyWriterzState, task: str, status: str, 
                               progress: int, message: str):
        """Broadcast evaluation progress."""
        self._broadcast_progress(state, message)
    
    async def _generate_comprehensive_evaluation(self, model_evaluations: Dict[str, Dict[str, Any]], 
                                               consensus_result: ConsensusMetrics,
                                               evaluation_context: Dict[str, Any]) -> ComprehensiveEvaluation:
        """Generate comprehensive evaluation from all analysis."""
        
        # Calculate overall score from consensus
        scores = [eval_data.get("overall_score", 0) for eval_data in model_evaluations.values()]
        overall_score = np.mean(scores) if scores else 0.0
        
        # Generate quality dimensions
        quality_dimensions = await self._generate_quality_dimensions(model_evaluations, evaluation_context)
        
        # Determine revision necessity
        revision_necessity = overall_score < 80.0 or consensus_result.consensus_strength == "weak"
        
        if overall_score < 70:
            revision_priority = "critical"
        elif overall_score < 80:
            revision_priority = "important"
        elif overall_score < 90:
            revision_priority = "minor"
        else:
            revision_priority = "none"
        
        return ComprehensiveEvaluation(
            overall_score=overall_score,
            confidence_level=consensus_result.inter_rater_reliability,
            assessment_timestamp=datetime.now(),
            quality_dimensions=quality_dimensions,
            gemini_evaluation=model_evaluations.get("gemini", {}),
            grok_evaluation=model_evaluations.get("grok", {}),
            o3_evaluation=model_evaluations.get("openai", {}),
            consensus_metrics=consensus_result,
            academic_level_assessment=evaluation_context.get("academic_level", "undergraduate"),
            field_appropriateness=self._assess_field_appropriateness(evaluation_context),
            theoretical_sophistication=self._assess_theoretical_sophistication(model_evaluations),
            methodological_awareness=self._assess_methodological_awareness(model_evaluations),
            revision_necessity=revision_necessity,
            revision_priority=revision_priority,
            specific_revision_targets=self._generate_revision_targets(model_evaluations),
            peer_comparison_percentile=min(100, max(0, overall_score)),
            field_standard_comparison={"field_average": 75.0, "above_average": overall_score > 75},
            historical_trend_analysis={"trend": "stable", "improvement": 0.0},
            potential_impact=self._assess_potential_impact(overall_score, consensus_result),
            scalability_assessment=0.8,
            innovation_quotient=self._assess_innovation_quotient(model_evaluations),
            learning_outcome_coverage=self._assess_learning_outcomes(evaluation_context),
            skill_demonstration=self._assess_skill_demonstration(model_evaluations),
            knowledge_application=self._assess_knowledge_application(model_evaluations)
        )
    
    async def _generate_quality_dimensions(self, model_evaluations: Dict[str, Dict[str, Any]], 
                                         context: Dict[str, Any]) -> List[QualityDimension]:
        """Generate quality dimensions from evaluations."""
        dimensions = []
        
        # Core academic dimensions
        dimension_names = [
            "Theoretical Sophistication",
            "Analytical Depth", 
            "Empirical Rigor",
            "Scholarly Communication",
            "Critical Thinking"
        ]
        
        for dim_name in dimension_names:
            # Extract scores for this dimension from all models
            dim_scores = []
            dim_evidence = []
            dim_weaknesses = []
            dim_strengths = []
            
            for model, evaluation in model_evaluations.items():
                scores = evaluation.get("scores", {})
                # Look for dimension-related scores
                relevant_scores = [score for key, score in scores.items() 
                                 if any(word in key.lower() for word in dim_name.lower().split())]
                if relevant_scores:
                    dim_scores.extend(relevant_scores)
                
                # Extract dimension-specific evidence
                dim_evidence.extend(evaluation.get("strengths", [])[:2])
                dim_weaknesses.extend(evaluation.get("weaknesses", [])[:2])
                dim_strengths.extend(evaluation.get("strengths", [])[:2])
            
            # Calculate dimension score
            dimension_score = np.mean(dim_scores) if dim_scores else 75.0
            
            dimension = QualityDimension(
                dimension_name=dim_name,
                score=dimension_score,
                confidence=0.8,
                evidence=dim_evidence[:3],
                weaknesses=dim_weaknesses[:3],
                strengths=dim_strengths[:3],
                improvement_recommendations=self._generate_dimension_recommendations(dim_name, dimension_score),
                comparative_analysis={"peer_average": 75.0, "percentile": min(100, dimension_score)},
                threshold_analysis=self._analyze_thresholds(dimension_score),
                future_potential=min(1.0, dimension_score / 80.0)
            )
            
            dimensions.append(dimension)
        
        return dimensions
    
    async def _generate_sophisticated_recommendations(self, evaluation: ComprehensiveEvaluation) -> List[Dict[str, Any]]:
        """Generate sophisticated improvement recommendations."""
        recommendations = []
        
        # Priority-based recommendations
        if evaluation.revision_priority == "critical":
            recommendations.extend([
                {
                    "priority": "critical",
                    "category": "fundamental_revision",
                    "description": "Complete restructuring required for academic standards",
                    "specific_actions": ["Reorganize argument structure", "Strengthen evidence base", "Improve theoretical grounding"],
                    "expected_improvement": 15.0
                }
            ])
        
        # Dimension-specific recommendations
        for dimension in evaluation.quality_dimensions:
            if dimension.score < 75:
                recommendations.append({
                    "priority": "high" if dimension.score < 65 else "medium",
                    "category": dimension.dimension_name.lower().replace(" ", "_"),
                    "description": f"Improve {dimension.dimension_name}",
                    "specific_actions": dimension.improvement_recommendations,
                    "expected_improvement": min(15.0, 85 - dimension.score)
                })
        
        return recommendations[:10]  # Limit to 10 recommendations


    def _assess_field_appropriateness(self, context: Dict[str, Any]) -> float:
        """Assess appropriateness for academic field."""
        return 0.85  # Default high appropriateness
    
    def _assess_theoretical_sophistication(self, evaluations: Dict[str, Dict[str, Any]]) -> float:
        """Assess theoretical sophistication from evaluations."""
        scores = []
        for eval_data in evaluations.values():
            scores_dict = eval_data.get("scores", {})
            theoretical_scores = [score for key, score in scores_dict.items() 
                                if "theoretical" in key.lower() or "theory" in key.lower()]
            if theoretical_scores:
                scores.extend(theoretical_scores)
        
        return np.mean(scores) / 100.0 if scores else 0.75
    
    def _assess_methodological_awareness(self, evaluations: Dict[str, Dict[str, Any]]) -> float:
        """Assess methodological awareness from evaluations."""
        scores = []
        for eval_data in evaluations.values():
            scores_dict = eval_data.get("scores", {})
            method_scores = [score for key, score in scores_dict.items() 
                           if "method" in key.lower() or "empirical" in key.lower()]
            if method_scores:
                scores.extend(method_scores)
        
        return np.mean(scores) / 100.0 if scores else 0.70
    
    def _generate_revision_targets(self, evaluations: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate specific revision targets."""
        targets = []
        
        for model, eval_data in evaluations.items():
            weaknesses = eval_data.get("weaknesses", [])
            for weakness in weaknesses[:3]:
                targets.append({
                    "target": weakness,
                    "priority": "medium",
                    "source_model": model,
                    "improvement_strategy": f"Address {weakness.lower()}"
                })
        
        return targets[:10]
    
    def _assess_potential_impact(self, overall_score: float, consensus: ConsensusMetrics) -> float:
        """Assess potential academic impact."""
        base_impact = overall_score / 100.0
        consensus_bonus = 0.1 if consensus.consensus_strength == "strong" else 0.0
        return min(1.0, base_impact + consensus_bonus)
    
    def _assess_innovation_quotient(self, evaluations: Dict[str, Dict[str, Any]]) -> float:
        """Assess innovation quotient from evaluations."""
        innovation_indicators = ["original", "novel", "creative", "innovative", "unique"]
        
        innovation_score = 0.0
        total_evaluations = 0
        
        for eval_data in evaluations.values():
            eval_text = eval_data.get("evaluation_text", "").lower()
            innovation_count = sum(1 for indicator in innovation_indicators if indicator in eval_text)
            innovation_score += min(1.0, innovation_count / 3.0)  # Normalize to 0-1
            total_evaluations += 1
        
        return innovation_score / max(1, total_evaluations)
    
    def _assess_learning_outcomes(self, context: Dict[str, Any]) -> Dict[str, float]:
        """Assess learning outcome coverage."""
        return {
            "critical_thinking": 0.8,
            "research_skills": 0.7,
            "communication": 0.85,
            "analysis": 0.75,
            "synthesis": 0.70
        }
    
    def _assess_skill_demonstration(self, evaluations: Dict[str, Dict[str, Any]]) -> Dict[str, float]:
        """Assess skill demonstration from evaluations."""
        skills = {
            "analytical_thinking": 0.0,
            "research_competency": 0.0,
            "critical_evaluation": 0.0,
            "academic_writing": 0.0,
            "argument_construction": 0.0
        }
        
        for eval_data in evaluations.values():
            overall_score = eval_data.get("overall_score", 0) / 100.0
            # Distribute overall score across skills with slight variations
            skills["analytical_thinking"] += overall_score * 0.9
            skills["research_competency"] += overall_score * 0.8
            skills["critical_evaluation"] += overall_score * 0.85
            skills["academic_writing"] += overall_score * 0.95
            skills["argument_construction"] += overall_score * 0.88
        
        # Average across evaluations
        num_evaluations = len(evaluations)
        if num_evaluations > 0:
            for skill in skills:
                skills[skill] /= num_evaluations
        
        return skills
    
    def _assess_knowledge_application(self, evaluations: Dict[str, Dict[str, Any]]) -> Dict[str, float]:
        """Assess knowledge application from evaluations."""
        knowledge_areas = {
            "theoretical_knowledge": 0.0,
            "empirical_knowledge": 0.0,
            "methodological_knowledge": 0.0,
            "practical_application": 0.0,
            "disciplinary_understanding": 0.0
        }
        
        for eval_data in evaluations.values():
            scores = eval_data.get("scores", {})
            overall_score = eval_data.get("overall_score", 75) / 100.0
            
            # Map specific scores to knowledge areas
            for key, score in scores.items():
                key_lower = key.lower()
                normalized_score = score / 100.0 if score > 1 else score
                
                if "theoretical" in key_lower or "theory" in key_lower:
                    knowledge_areas["theoretical_knowledge"] += normalized_score
                elif "empirical" in key_lower or "evidence" in key_lower:
                    knowledge_areas["empirical_knowledge"] += normalized_score
                elif "method" in key_lower:
                    knowledge_areas["methodological_knowledge"] += normalized_score
                elif "practical" in key_lower or "application" in key_lower:
                    knowledge_areas["practical_application"] += normalized_score
                else:
                    knowledge_areas["disciplinary_understanding"] += normalized_score
        
        # Ensure all knowledge areas have reasonable scores
        num_evaluations = max(1, len(evaluations))
        for area in knowledge_areas:
            if knowledge_areas[area] == 0.0:
                knowledge_areas[area] = 0.7  # Default reasonable score
            else:
                knowledge_areas[area] /= num_evaluations
                knowledge_areas[area] = min(1.0, knowledge_areas[area])
        
        return knowledge_areas
    
    def _generate_dimension_recommendations(self, dimension_name: str, score: float) -> List[str]:
        """Generate dimension-specific improvement recommendations."""
        
        recommendations_map = {
            "theoretical sophistication": [
                "Deepen engagement with theoretical frameworks",
                "Integrate multiple theoretical perspectives", 
                "Demonstrate critical evaluation of theories",
                "Show original theoretical insights"
            ],
            "analytical depth": [
                "Strengthen critical analysis throughout",
                "Develop more sophisticated arguments",
                "Include multi-perspective evaluation",
                "Enhance logical reasoning chains"
            ],
            "empirical rigor": [
                "Improve evidence quality and selection",
                "Strengthen methodological awareness",
                "Enhance data interpretation skills",
                "Address research validity concerns"
            ],
            "scholarly communication": [
                "Improve academic writing clarity",
                "Enhance citation accuracy and style",
                "Strengthen document organization",
                "Refine professional presentation"
            ],
            "critical thinking": [
                "Develop deeper critical evaluation",
                "Strengthen reasoning sophistication",
                "Enhance argument complexity",
                "Improve analytical synthesis"
            ]
        }
        
        dimension_key = dimension_name.lower()
        base_recommendations = recommendations_map.get(dimension_key, [
            "Improve overall quality",
            "Strengthen academic rigor",
            "Enhance analytical depth"
        ])
        
        # Filter recommendations based on score level
        if score >= 85:
            return base_recommendations[:2]  # Fewer recommendations for high scores
        elif score >= 75:
            return base_recommendations[:3]
        else:
            return base_recommendations  # All recommendations for low scores
    
    def _analyze_thresholds(self, score: float) -> Dict[str, bool]:
        """Analyze threshold achievement."""
        return {
            "excellent_threshold": score >= 90,
            "proficient_threshold": score >= 80,
            "developing_threshold": score >= 70,
            "adequate_threshold": score >= 60
        }


# Helper functions for missing imports
def zscore(scores):
    """Calculate z-scores."""
    if not scores:
        return []
    mean_score = np.mean(scores)
    std_score = np.std(scores)
    if std_score == 0:
        return [0.0] * len(scores)
    return [(score - mean_score) / std_score for score in scores]

def _perform_casp_appraisal(self, state: HandyWriterzState):
    """Performs CASP appraisal on the filtered studies."""
    studies = state.get("filtered_studies", [])
    return self.casp_appraisal_tool.appraise_studies(studies)

# Create singleton instance
revolutionary_evaluator_node = RevolutionaryMultiModelEvaluator()