# Multi-Agent Powered App - Complete Development Guide

## Project Overview

Building a production-ready multi-agent system with:
- **Backend**: FastAPI + LangGraph + Python agents
- **Frontend**: React 19 + Vite + Shadcn UI
- **Database**: Cloudflare D1/R2 or Supabase
- **Auth**: Dynamic.xyz with MPC wallets (Solana/Base)
- **AI Models**: Claude, GPT-4, Gemini 2.5, Perplexity, DeepSeek R1, Qwen 2.5, Grok 3
- **Mobile**: React Native Expo

## Step 1: Clean and Setup Base Repository

```bash
# Clone the Google Gemini quickstart
git clone https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart multiagent-app
cd multiagent-app

# Clean the repository
rm -rf .git
rm -rf node_modules
rm -rf __pycache__
rm -rf .pytest_cache
rm -rf dist
rm -rf build
find . -name "*.pyc" -delete
find . -name ".DS_Store" -delete

# Initialize new git repository
git init
git add .
git commit -m "Initial commit from Gemini quickstart"
```

## Step 2: Project Structure

```
multiagent-app/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”‚   â”œâ”€â”€ master_orchestrator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ enhanced_user_intent.py
â”‚   â”‚   â”‚   â”œâ”€â”€ intelligent_intent_analyzer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_gemini.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_perplexity.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_o3.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_deepseek.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_qwen.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_grok.py
â”‚   â”‚   â”‚   â”œâ”€â”€ writer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ evaluator_advanced.py
â”‚   â”‚   â”‚   â”œâ”€â”€ formatter_advanced.py
â”‚   â”‚   â”‚   â”œâ”€â”€ swarm_intelligence_coordinator.py
â”‚   â”‚   â”‚   â””â”€â”€ emergent_intelligence_engine.py
â”‚   â”‚   â”œâ”€â”€ handywriterz_state.py
â”‚   â”‚   â””â”€â”€ handywriterz_graph.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚   â”œâ”€â”€ files.py
â”‚   â”‚   â”‚   â””â”€â”€ wallet.py
â”‚   â”‚   â””â”€â”€ middleware/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â””â”€â”€ storage.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ llm_service.py
â”‚   â”‚   â”œâ”€â”€ wallet_service.py
â”‚   â”‚   â””â”€â”€ file_service.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”‚   â”œâ”€â”€ landing/
â”‚   â”‚   â”‚   â””â”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ mobile/
â”‚   â””â”€â”€ expo-app/
â”œâ”€â”€ shared/
â”‚   â””â”€â”€ types/
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

## Step 3: Backend Implementation

### 3.1 Core Dependencies (requirements.txt)

```txt
fastapi==0.109.0
uvicorn[standard]==0.27.0
python-dotenv==1.0.0
langchain==0.1.0
langgraph==0.0.25
langchain-anthropic==0.1.1
langchain-openai==0.0.5
langchain-google-genai==0.0.5
langchain-community==0.0.12
httpx==0.26.0
redis==5.0.1
supabase==2.3.0
cloudflare==3.0.0
pydantic==2.5.3
python-multipart==0.0.6
aiofiles==23.2.1
Pillow==10.2.0
numpy==1.26.3
pandas==2.1.4
scikit-learn==1.4.0
```

### 3.2 Environment Configuration (.env)

```env
# API Keys
ANTHROPIC_API_KEY=your_key
OPENAI_API_KEY=your_key
GEMINI_API_KEY=your_key
PERPLEXITY_API_KEY=your_key
DEEPSEEK_API_KEY=your_key
QWEN_API_KEY=your_key
GROK_API_KEY=your_key

# Database
DATABASE_URL=your_supabase_url
DATABASE_KEY=your_supabase_key
CLOUDFLARE_ACCOUNT_ID=your_account_id
CLOUDFLARE_API_TOKEN=your_token
CLOUDFLARE_D1_DATABASE_ID=your_database_id

# Redis
REDIS_URL=redis://localhost:6379

# Dynamic.xyz
DYNAMIC_ENVIRONMENT_ID=your_env_id
DYNAMIC_API_KEY=your_api_key

# Storage
CLOUDFLARE_R2_ACCESS_KEY=your_key
CLOUDFLARE_R2_SECRET_KEY=your_secret
CLOUDFLARE_R2_BUCKET=your_bucket
```

### 3.3 FastAPI Main Application (backend/api/main.py)

```python
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import uvicorn

from api.routes import chat, files, wallet
from api.auth import auth_router
from core.config import settings
from core.database import init_db

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await init_db()
    yield
    # Shutdown
    pass

app = FastAPI(
    title="MultiAgent AI Platform",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "https://yourdomain.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(auth_router, prefix="/api/auth", tags=["auth"])
app.include_router(chat.router, prefix="/api/chat", tags=["chat"])
app.include_router(files.router, prefix="/api/files", tags=["files"])
app.include_router(wallet.router, prefix="/api/wallet", tags=["wallet"])

# WebSocket endpoint for real-time chat
@app.websocket("/ws/{client_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str):
    await websocket.accept()
    try:
        while True:
            data = await websocket.receive_json()
            # Process with LangGraph
            response = await process_with_agents(data)
            await websocket.send_json(response)
    except WebSocketDisconnect:
        pass

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
```

### 3.4 Enhanced Master Orchestrator (backend/agent/nodes/master_orchestrator.py)

```python
from typing import Dict, Any
from langchain_core.runnables import RunnableConfig
from agent.handywriterz_state import HandyWriterzState
import asyncio

class MasterOrchestratorAgent:
    """Revolutionary master orchestrator for intelligent workflow routing."""
    
    def __init__(self):
        self.complexity_analyzer = ComplexityAnalyzer()
        self.workflow_optimizer = WorkflowOptimizer()
    
    async def __call__(
        self, 
        state: HandyWriterzState, 
        config: RunnableConfig
    ) -> Dict[str, Any]:
        """Execute master orchestration logic."""
        
        # Analyze request complexity
        complexity_analysis = await self.complexity_analyzer.analyze(
            state.get("user_request", ""),
            state.get("context_files", []),
            state.get("requirements", [])
        )
        
        # Determine optimal workflow
        workflow_plan = await self.workflow_optimizer.optimize(
            complexity_analysis,
            state.get("available_resources", {}),
            state.get("time_constraints", None)
        )
        
        # Set orchestration metadata
        orchestration_result = {
            "workflow_intelligence": {
                "academic_complexity": complexity_analysis["score"],
                "recommended_agents": workflow_plan["agents"],
                "estimated_time": workflow_plan["estimated_time"],
                "success_probability": workflow_plan["success_probability"]
            },
            "routing_decision": workflow_plan["primary_route"],
            "use_swarm_intelligence": complexity_analysis["score"] >= 8.0,
            "parallel_processing": workflow_plan["parallel_capable"]
        }
        
        return {
            "orchestration_result": orchestration_result,
            "workflow_status": "orchestrated",
            "next_node": workflow_plan["primary_route"]
        }

class ComplexityAnalyzer:
    """Analyzes request complexity using multi-dimensional scoring."""
    
    async def analyze(
        self, 
        request: str, 
        files: list, 
        requirements: list
    ) -> Dict[str, Any]:
        # Implement sophisticated complexity analysis
        score = 5.0  # Base score
        
        # Length and depth analysis
        if len(request) > 1000:
            score += 2.0
        if len(files) > 5:
            score += 1.5
        if len(requirements) > 10:
            score += 1.5
            
        # Keyword complexity
        complex_keywords = [
            "analyze", "synthesize", "evaluate", "critique",
            "multi-dimensional", "cross-reference", "systematic"
        ]
        for keyword in complex_keywords:
            if keyword in request.lower():
                score += 0.5
                
        return {
            "score": min(score, 10.0),
            "dimensions": {
                "linguistic": score * 0.3,
                "technical": score * 0.4,
                "research": score * 0.3
            }
        }

class WorkflowOptimizer:
    """Optimizes workflow based on complexity and resources."""
    
    async def optimize(
        self,
        complexity: Dict[str, Any],
        resources: Dict[str, Any],
        constraints: Any
    ) -> Dict[str, Any]:
        score = complexity["score"]
        
        if score >= 8.0:
            return {
                "primary_route": "enhanced_user_intent",
                "agents": ["swarm_coordinator", "emergent_intelligence"],
                "estimated_time": 300,
                "success_probability": 0.95,
                "parallel_capable": True
            }
        elif score >= 5.0:
            return {
                "primary_route": "enhanced_user_intent",
                "agents": ["standard_pipeline"],
                "estimated_time": 180,
                "success_probability": 0.85,
                "parallel_capable": True
            }
        else:
            return {
                "primary_route": "user_intent",
                "agents": ["basic_pipeline"],
                "estimated_time": 60,
                "success_probability": 0.90,
                "parallel_capable": False
            }
```

### 3.5 Gemini Search Agent (backend/agent/nodes/search_gemini.py)

```python
import google.generativeai as genai
from typing import Dict, Any
import asyncio

class GeminiSearchAgent:
    """Advanced Gemini-powered search with multimodal capabilities."""
    
    def __init__(self):
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        self.model = genai.GenerativeModel('gemini-2.5-pro-latest')
        self.search_config = genai.types.GenerationConfig(
            temperature=0.7,
            top_p=0.95,
            max_output_tokens=8192,
        )
    
    async def execute(
        self, 
        state: Dict[str, Any], 
        config: Any
    ) -> Dict[str, Any]:
        """Execute Gemini search with Google Search grounding."""
        
        query = state.get("search_query", state.get("user_request", ""))
        files = state.get("context_files", [])
        
        # Build multimodal prompt
        prompt_parts = [
            f"Research Query: {query}",
            "Please provide comprehensive academic insights with sources."
        ]
        
        # Add file contents if available
        for file in files:
            if file["type"] in ["image", "video", "audio"]:
                # Handle multimodal inputs
                prompt_parts.append(file["content"])
        
        # Enable Google Search grounding
        generation_config = genai.types.GenerationConfig(
            temperature=0.7,
            tools='google_search',
            tool_config={'google_search': {'dynamic_retrieval_config': {
                'mode': 'MODE_DYNAMIC',
                'dynamic_threshold': 0.3
            }}}
        )
        
        try:
            response = await asyncio.to_thread(
                self.model.generate_content,
                prompt_parts,
                generation_config=generation_config
            )
            
            # Extract search results and synthesized knowledge
            search_results = self._parse_search_results(response)
            
            return {
                "gemini_results": search_results,
                "gemini_synthesis": response.text,
                "sources_found": len(search_results),
                "multimodal_processed": len(files) > 0
            }
            
        except Exception as e:
            return {
                "error": f"Gemini search failed: {str(e)}",
                "gemini_results": [],
                "sources_found": 0
            }
    
    def _parse_search_results(self, response):
        """Parse grounded search results from Gemini response."""
        results = []
        
        # Extract grounding metadata
        if hasattr(response, 'grounding_metadata'):
            for source in response.grounding_metadata.get('search_queries', []):
                results.append({
                    "title": source.get("title", ""),
                    "url": source.get("url", ""),
                    "snippet": source.get("snippet", ""),
                    "relevance_score": source.get("score", 0.0)
                })
        
        return results
```

### 3.6 Chat Route Handler (backend/api/routes/chat.py)

```python
from fastapi import APIRouter, HTTPException, Depends, UploadFile, File
from typing import List, Optional
import asyncio

from agent.handywriterz_graph import handywriterz_graph
from api.auth import get_current_user
from services.file_service import FileService

router = APIRouter()

@router.post("/message")
async def send_message(
    message: str,
    files: Optional[List[UploadFile]] = File(None),
    current_user=Depends(get_current_user)
):
    """Process user message through multi-agent system."""
    
    # Process uploaded files
    context_files = []
    if files:
        file_service = FileService()
        for file in files:
            processed_file = await file_service.process_file(file)
            context_files.append(processed_file)
    
    # Prepare state for LangGraph
    initial_state = {
        "user_request": message,
        "context_files": context_files,
        "user_id": current_user["id"],
        "timestamp": datetime.utcnow().isoformat()
    }
    
    # Execute the graph
    try:
        config = {"configurable": {"thread_id": current_user["id"]}}
        result = await handywriterz_graph.ainvoke(initial_state, config)
        
        return {
            "success": True,
            "response": result.get("final_response", ""),
            "sources": result.get("sources", []),
            "workflow_status": result.get("workflow_status", ""),
            "processing_time": result.get("processing_time", 0)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

## Step 4: Frontend Implementation

### 4.1 Package.json

```json
{
  "name": "multiagent-frontend",
  "version": "1.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc && vite build",
    "preview": "vite preview"
  },
  "dependencies": {
    "react": "^19.0.0",
    "react-dom": "^19.0.0",
    "@dynamic-labs/sdk-react-core": "^2.0.0",
    "@dynamic-labs/ethereum": "^2.0.0",
    "@dynamic-labs/solana": "^2.0.0",
    "@radix-ui/react-avatar": "^1.0.4",
    "@radix-ui/react-dialog": "^1.0.5",
    "@radix-ui/react-dropdown-menu": "^2.0.6",
    "@radix-ui/react-scroll-area": "^1.0.5",
    "class-variance-authority": "^0.7.0",
    "clsx": "^2.1.0",
    "framer-motion": "^10.16.16",
    "lucide-react": "^0.309.0",
    "react-dropzone": "^14.2.3",
    "react-markdown": "^9.0.1",
    "react-syntax-highlighter": "^15.5.0",
    "recharts": "^2.10.3",
    "tailwind-merge": "^2.2.0",
    "zustand": "^4.4.7"
  },
  "devDependencies": {
    "@types/react": "^18.2.46",
    "@types/react-dom": "^18.2.18",
    "@vitejs/plugin-react": "^4.2.1",
    "autoprefixer": "^10.4.16",
    "postcss": "^8.4.33",
    "tailwindcss": "^3.4.0",
    "typescript": "^5.3.3",
    "vite": "^5.0.10"
  }
}
```

### 4.2 Main App Component (frontend/src/App.tsx)

```tsx
import React from 'react';
import { BrowserRouter as Router, Routes, Route } from 'react-router-dom';
import { DynamicContextProvider } from '@dynamic-labs/sdk-react-core';
import { EthereumWalletConnectors } from '@dynamic-labs/ethereum';
import { SolanaWalletConnectors } from '@dynamic-labs/solana';

import LandingPage from './pages/LandingPage';
import ChatInterface from './pages/ChatInterface';
import { ThemeProvider } from './components/theme-provider';

const App: React.FC = () => {
  return (
    <DynamicContextProvider
      settings={{
        environmentId: import.meta.env.VITE_DYNAMIC_ENVIRONMENT_ID,
        walletConnectors: [EthereumWalletConnectors, SolanaWalletConnectors],
      }}
    >
      <ThemeProvider defaultTheme="dark" storageKey="multiagent-theme">
        <Router>
          <Routes>
            <Route path="/" element={<LandingPage />} />
            <Route path="/chat" element={<ChatInterface />} />
          </Routes>
        </Router>
      </ThemeProvider>
    </DynamicContextProvider>
  );
};

export default App;
```

### 4.3 Modern Chat Interface (frontend/src/pages/ChatInterface.tsx)

```tsx
import React, { useState, useRef, useEffect } from 'react';
import { Send, Paperclip, Sparkles, User, Bot } from 'lucide-react';
import { motion, AnimatePresence } from 'framer-motion';
import { useDynamicContext } from '@dynamic-labs/sdk-react-core';

import { Button } from '@/components/ui/button';
import { Textarea } from '@/components/ui/textarea';
import { ScrollArea } from '@/components/ui/scroll-area';
import { Avatar, AvatarFallback, AvatarImage } from '@/components/ui/avatar';
import FileUploadZone from '@/components/chat/FileUploadZone';
import MessageBubble from '@/components/chat/MessageBubble';
import TypingIndicator from '@/components/chat/TypingIndicator';
import SourceCard from '@/components/chat/SourceCard';

interface Message {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: Date;
  sources?: any[];
  files?: any[];
}

const ChatInterface: React.FC = () => {
  const { user, isAuthenticated } = useDynamicContext();
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState('');
  const [files, setFiles] = useState<File[]>([]);
  const [isLoading, setIsLoading] = useState(false);
  const [isTyping, setIsTyping] = useState(false);
  const scrollRef = useRef<HTMLDivElement>(null);
  const fileInputRef = useRef<HTMLInputElement>(null);

  useEffect(() => {
    scrollRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages]);

  const handleSendMessage = async () => {
    if (!input.trim() && files.length === 0) return;

    const userMessage: Message = {
      id: Date.now().toString(),
      role: 'user',
      content: input,
      timestamp: new Date(),
      files: files.map(f => ({ name: f.name, type: f.type }))
    };

    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsLoading(true);
    setIsTyping(true);

    try {
      const formData = new FormData();
      formData.append('message', input);
      files.forEach(file => formData.append('files', file));

      const response = await fetch('/api/chat/message', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${user?.sessionToken}`
        },
        body: formData
      });

      const data = await response.json();
      
      setIsTyping(false);
      
      const assistantMessage: Message = {
        id: (Date.now() + 1).toString(),
        role: 'assistant',
        content: data.response,
        timestamp: new Date(),
        sources: data.sources
      };

      setMessages(prev => [...prev, assistantMessage]);
      setFiles([]);
    } catch (error) {
      console.error('Error sending message:', error);
      setIsTyping(false);
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <div className="flex flex-col h-screen bg-background">
      {/* Header */}
      <header className="border-b px-4 py-3 flex items-center justify-between">
        <div className="flex items-center gap-3">
          <Sparkles className="w-6 h-6 text-primary" />
          <h1 className="text-xl font-semibold">MultiAgent AI</h1>
        </div>
        <div className="flex items-center gap-4">
          <span className="text-sm text-muted-foreground">
            {isAuthenticated ? `Connected: ${user?.email}` : 'Not connected'}
          </span>
          <Avatar>
            <AvatarImage src={user?.profilePicture} />
            <AvatarFallback>{user?.email?.[0]?.toUpperCase()}</AvatarFallback>
          </Avatar>
        </div>
      </header>

      {/* Messages Area */}
      <ScrollArea className="flex-1 p-4">
        <div className="max-w-4xl mx-auto space-y-4">
          <AnimatePresence>
            {messages.map((message) => (
              <motion.div
                key={message.id}
                initial={{ opacity: 0, y: 20 }}
                animate={{ opacity: 1, y: 0 }}
                exit={{ opacity: 0, y: -20 }}
                transition={{ duration: 0.3 }}
              >
                <MessageBubble message={message} />
                {message.sources && message.sources.length > 0 && (
                  <div className="mt-3 grid grid-cols-1 md:grid-cols-2 gap-3 ml-12">
                    {message.sources.map((source, idx) => (
                      <SourceCard key={idx} source={source} />
                    ))}
                  </div>
                )}
              </motion.div>
            ))}
            {isTyping && <TypingIndicator />}
          </AnimatePresence>
          <div ref={scrollRef} />
        </div>
      </ScrollArea>

      {/* File Upload Zone */}
      {files.length > 0 && (
        <div className="px-4 py-2 border-t">
          <FileUploadZone files={files} onRemove={(idx) => {
            setFiles(prev => prev.filter((_, i) => i !== idx));
          }} />
        </div>
      )}

      {/* Input Area */}
      <div className="border-t p-4">
        <div className="max-w-4xl mx-auto">
          <div className="flex gap-2 items-end">
            <Button
              variant="ghost"
              size="icon"
              onClick={() => fileInputRef.current?.click()}
              className="mb-1"
            >
              <Paperclip className="w-5 h-5" />
            </Button>
            <input
              ref={fileInputRef}
              type="file"
              multiple
              className="hidden"
              onChange={(e) => {
                if (e.target.files) {
                  setFiles(Array.from(e.target.files));
                }
              }}
              accept="image/*,video/*,audio/*,.pdf,.doc,.docx,.txt"
            />
            <Textarea
              value={input}
              onChange={(e) => setInput(e.target.value)}
              onKeyDown={(e) => {
                if (e.key === 'Enter' && !e.shiftKey) {
                  e.preventDefault();
                  handleSendMessage();
                }
              }}
              placeholder="Ask anything... (Shift+Enter for new line)"
              className="min-h-[60px] max-h-[200px] resize-none"
              disabled={isLoading}
            />
            <Button
              onClick={handleSendMessage}
              disabled={isLoading || (!input.trim() && files.length === 0)}
              className="mb-1"
            >
              <Send className="w-5 h-5" />
            </Button>
          </div>
          <p className="text-xs text-muted-foreground mt-2 text-center">
            Powered by Claude, GPT-4, Gemini, and advanced multi-agent orchestration
          </p>
        </div>
      </div>
    </div>
  );
};

export default ChatInterface;
```

## Step 5: Docker Configuration

### 5.1 Backend Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    git \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port
EXPOSE 8000

# Run with hot reload in development
CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```

### 5.2 Docker Compose (docker-compose.yml)

```yaml
version: '3.8'

services:
  backend:
    build: 
      context: ./backend
      dockerfile: Dockerfile
    volumes:
      - ./backend:/app
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
    env_file:
      - ./backend/.env
    depends_on:
      - redis
    networks:
      - app-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    volumes:
      - ./frontend:/app
      - /app/node_modules
    ports:
      - "5173:5173"
    environment:
      - VITE_API_URL=http://localhost:8000
    networks:
      - app-network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - app-network

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - backend
      - frontend
    networks:
      - app-network

volumes:
  redis-data:

networks:
  app-network:
    driver: bridge
```

### 5.3 Makefile

```makefile
.PHONY: help dev build test deploy clean

help: ## Show this help message
	@echo 'Usage: make [target]'
	@echo ''
	@echo 'Targets:'
	@awk 'BEGIN {FS = ":.*?## "} /^[a-zA-Z_-]+:.*?## / {printf "  %-15s %s\n", $$1, $$2}' $(MAKEFILE_LIST)

dev: ## Start development environment with hot reload
	docker-compose up --build

dev-backend: ## Start only backend with hot reload
	cd backend && uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

dev-frontend: ## Start only frontend with hot reload
	cd frontend && npm run dev

build: ## Build production images
	docker-compose build --no-cache

test: ## Run tests
	cd backend && pytest tests/ -v
	cd frontend && npm test

deploy-cf: ## Deploy to Cloudflare
	cd frontend && npm
	
	## Step 6: Mobile App (React Native Expo)

### 6.1 Expo App Structure (mobile/expo-app/App.tsx)

```tsx
import React from 'react';
import { NavigationContainer } from '@react-navigation/native';
import { createStackNavigator } from '@react-navigation/stack';
import { SafeAreaProvider } from 'react-native-safe-area-context';
import { QueryClient, QueryClientProvider } from '@tanstack/react-query';

import HomeScreen from './screens/HomeScreen';
import ChatScreen from './screens/ChatScreen';
import ProfileScreen from './screens/ProfileScreen';

const Stack = createStackNavigator();
const queryClient = new QueryClient();

export default function App() {
  return (
    <QueryClientProvider client={queryClient}>
      <SafeAreaProvider>
        <NavigationContainer>
          <Stack.Navigator initialRouteName="Home">
            <Stack.Screen 
              name="Home" 
              component={HomeScreen}
              options={{ headerShown: false }}
            />
            <Stack.Screen 
              name="Chat" 
              component={ChatScreen}
              options={{ title: 'AI Assistant' }}
            />
            <Stack.Screen 
              name="Profile" 
              component={ProfileScreen}
              options={{ title: 'Profile' }}
            />
          </Stack.Navigator>
        </NavigationContainer>
      </SafeAreaProvider>
    </QueryClientProvider>
  );
}
```

### 6.2 Mobile Chat Screen (mobile/expo-app/screens/ChatScreen.tsx)

```tsx
import React, { useState, useRef } from 'react';
import {
  View,
  Text,
  TextInput,
  TouchableOpacity,
  FlatList,
  KeyboardAvoidingView,
  Platform,
  StyleSheet,
  ActivityIndicator,
} from 'react-native';
import { Ionicons } from '@expo/vector-icons';
import * as DocumentPicker from 'expo-document-picker';
import * as ImagePicker from 'expo-image-picker';

const ChatScreen = () => {
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [attachments, setAttachments] = useState([]);
  const flatListRef = useRef(null);

  const sendMessage = async () => {
    if (!input.trim() && attachments.length === 0) return;

    const userMessage = {
      id: Date.now(),
      role: 'user',
      content: input,
      attachments: [...attachments],
      timestamp: new Date(),
    };

    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setAttachments([]);
    setIsLoading(true);

    try {
      const formData = new FormData();
      formData.append('message', input);
      
      attachments.forEach((file, index) => {
        formData.append('files', {
          uri: file.uri,
          type: file.mimeType || 'application/octet-stream',
          name: file.name || `file_${index}`,
        });
      });

      const response = await fetch(`${API_URL}/api/chat/message`, {
        method: 'POST',
        body: formData,
        headers: {
          'Content-Type': 'multipart/form-data',
        },
      });

      const data = await response.json();

      const assistantMessage = {
        id: Date.now() + 1,
        role: 'assistant',
        content: data.response,
        sources: data.sources,
        timestamp: new Date(),
      };

      setMessages(prev => [...prev, assistantMessage]);
    } catch (error) {
      console.error('Error:', error);
    } finally {
      setIsLoading(false);
    }
  };

  const pickDocument = async () => {
    const result = await DocumentPicker.getDocumentAsync({
      type: '*/*',
      multiple: true,
    });

    if (!result.canceled) {
      setAttachments(prev => [...prev, ...result.assets]);
    }
  };

  const pickImage = async () => {
    const result = await ImagePicker.launchImageLibraryAsync({
      mediaTypes: ImagePicker.MediaTypeOptions.All,
      allowsMultipleSelection: true,
      quality: 1,
    });

    if (!result.canceled) {
      setAttachments(prev => [...prev, ...result.assets]);
    }
  };

  const renderMessage = ({ item }) => (
    <View style={[
      styles.messageBubble,
      item.role === 'user' ? styles.userMessage : styles.assistantMessage
    ]}>
      <Text style={styles.messageText}>{item.content}</Text>
      {item.attachments && item.attachments.length > 0 && (
        <View style={styles.attachmentsList}>
          {item.attachments.map((att, idx) => (
            <Text key={idx} style={styles.attachmentText}>
              ðŸ“Ž {att.name}
            </Text>
          ))}
        </View>
      )}
      {item.sources && item.sources.length > 0 && (
        <View style={styles.sourcesList}>
          {item.sources.map((source, idx) => (
            <TouchableOpacity key={idx} style={styles.sourceCard}>
              <Text style={styles.sourceTitle}>{source.title}</Text>
              <Text style={styles.sourceUrl}>{source.url}</Text>
            </TouchableOpacity>
          ))}
        </View>
      )}
    </View>
  );

  return (
    <KeyboardAvoidingView 
      style={styles.container}
      behavior={Platform.OS === 'ios' ? 'padding' : 'height'}
    >
      <FlatList
        ref={flatListRef}
        data={messages}
        renderItem={renderMessage}
        keyExtractor={item => item.id.toString()}
        contentContainerStyle={styles.messagesList}
        onContentSizeChange={() => flatListRef.current?.scrollToEnd()}
      />

      {attachments.length > 0 && (
        <View style={styles.attachmentsBar}>
          {attachments.map((att, idx) => (
            <View key={idx} style={styles.attachmentChip}>
              <Text style={styles.attachmentChipText}>{att.name}</Text>
              <TouchableOpacity
                onPress={() => setAttachments(prev => prev.filter((_, i) => i !== idx))}
              >
                <Ionicons name="close-circle" size={16} color="#666" />
              </TouchableOpacity>
            </View>
          ))}
        </View>
      )}

      <View style={styles.inputContainer}>
        <TouchableOpacity onPress={pickDocument} style={styles.attachButton}>
          <Ionicons name="attach" size={24} color="#666" />
        </TouchableOpacity>
        <TouchableOpacity onPress={pickImage} style={styles.attachButton}>
          <Ionicons name="image" size={24} color="#666" />
        </TouchableOpacity>
        <TextInput
          style={styles.input}
          value={input}
          onChangeText={setInput}
          placeholder="Ask anything..."
          multiline
          maxHeight={100}
        />
        <TouchableOpacity 
          onPress={sendMessage}
          disabled={isLoading}
          style={[styles.sendButton, isLoading && styles.sendButtonDisabled]}
        >
          {isLoading ? (
            <ActivityIndicator size="small" color="#fff" />
          ) : (
            <Ionicons name="send" size={20} color="#fff" />
          )}
        </TouchableOpacity>
      </View>
    </KeyboardAvoidingView>
  );
};

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#f5f5f5',
  },
  messagesList: {
    padding: 16,
  },
  messageBubble: {
    marginVertical: 4,
    padding: 12,
    borderRadius: 16,
    maxWidth: '80%',
  },
  userMessage: {
    alignSelf: 'flex-end',
    backgroundColor: '#007AFF',
  },
  assistantMessage: {
    alignSelf: 'flex-start',
    backgroundColor: '#fff',
    borderWidth: 1,
    borderColor: '#e0e0e0',
  },
  messageText: {
    fontSize: 16,
    color: '#000',
  },
  attachmentsList: {
    marginTop: 8,
  },
  attachmentText: {
    fontSize: 12,
    color: '#666',
  },
  sourcesList: {
    marginTop: 8,
  },
  sourceCard: {
    padding: 8,
    backgroundColor: '#f0f0f0',
    borderRadius: 8,
    marginTop: 4,
  },
  sourceTitle: {
    fontSize: 12,
    fontWeight: 'bold',
  },
  sourceUrl: {
    fontSize: 10,
    color: '#666',
  },
  attachmentsBar: {
    flexDirection: 'row',
    padding: 8,
    backgroundColor: '#fff',
    borderTopWidth: 1,
    borderTopColor: '#e0e0e0',
  },
  attachmentChip: {
    flexDirection: 'row',
    alignItems: 'center',
    backgroundColor: '#f0f0f0',
    paddingHorizontal: 8,
    paddingVertical: 4,
    borderRadius: 12,
    marginRight: 8,
  },
  attachmentChipText: {
    fontSize: 12,
    marginRight: 4,
  },
  inputContainer: {
    flexDirection: 'row',
    alignItems: 'flex-end',
    padding: 8,
    backgroundColor: '#fff',
    borderTopWidth: 1,
    borderTopColor: '#e0e0e0',
  },
  attachButton: {
    padding: 8,
  },
  input: {
    flex: 1,
    borderWidth: 1,
    borderColor: '#e0e0e0',
    borderRadius: 20,
    paddingHorizontal: 16,
    paddingVertical: 8,
    marginRight: 8,
    fontSize: 16,
  },
  sendButton: {
    backgroundColor: '#007AFF',
    width: 40,
    height: 40,
    borderRadius: 20,
    justifyContent: 'center',
    alignItems: 'center',
  },
  sendButtonDisabled: {
    opacity: 0.5,
  },
});

export default ChatScreen;
```

## Step 7: Advanced Agent Nodes

### 7.1 Swarm Intelligence Coordinator (backend/agent/nodes/swarm_intelligence_coordinator.py)

```python
import asyncio
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor
import numpy as np

class SwarmIntelligenceCoordinator:
    """Coordinates multiple AI agents for collective intelligence."""
    
    def __init__(self):
        self.agent_pool = self._initialize_agent_pool()
        self.consensus_engine = ConsensusEngine()
        self.diversity_optimizer = DiversityOptimizer()
    
    async def __call__(self, state: Dict[str, Any], config: Any) -> Dict[str, Any]:
        """Execute swarm intelligence coordination."""
        
        # Prepare swarm tasks
        swarm_tasks = self._prepare_swarm_tasks(state)
        
        # Deploy agent swarm with diversity optimization
        swarm_results = await self._deploy_swarm(swarm_tasks, state)
        
        # Aggregate insights through consensus
        collective_insights = await self.consensus_engine.aggregate(
            swarm_results,
            state.get("consensus_threshold", 0.7)
        )
        
        # Synthesize emergent patterns
        emergent_patterns = self._identify_emergent_patterns(swarm_results)
        
        return {
            "swarm_results": swarm_results,
            "collective_insights": collective_insights,
            "emergent_patterns": emergent_patterns,
            "consensus_strength": collective_insights.get("consensus_score", 0),
            "diversity_score": self._calculate_diversity_score(swarm_results),
            "swarm_size": len(swarm_results)
        }
    
    def _initialize_agent_pool(self) -> List[Any]:
        """Initialize diverse agent pool for swarm intelligence."""
        return [
            CreativeAgent(),
            AnalyticalAgent(),
            CriticalAgent(),
            SynthesisAgent(),
            PatternRecognitionAgent(),
            OutlierDetectionAgent()
        ]
    
    async def _deploy_swarm(
        self, 
        tasks: List[Dict], 
        state: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Deploy agent swarm in parallel."""
        
        async def execute_agent(agent, task):
            try:
                return await agent.process(task, state)
            except Exception as e:
                return {"error": str(e), "agent": agent.__class__.__name__}
        
        # Execute agents in parallel
        tasks_list = []
        for i, agent in enumerate(self.agent_pool):
            task = tasks[i % len(tasks)] if tasks else {"default": True}
            tasks_list.append(execute_agent(agent, task))
        
        results = await asyncio.gather(*tasks_list)
        return [r for r in results if "error" not in r]
    
    def _prepare_swarm_tasks(self, state: Dict[str, Any]) -> List[Dict]:
        """Prepare diverse tasks for swarm agents."""
        base_request = state.get("user_request", "")
        
        return [
            {"perspective": "creative", "focus": "novel solutions"},
            {"perspective": "analytical", "focus": "data-driven insights"},
            {"perspective": "critical", "focus": "identify weaknesses"},
            {"perspective": "synthesis", "focus": "integrate ideas"},
            {"perspective": "pattern", "focus": "recurring themes"},
            {"perspective": "outlier", "focus": "unconventional approaches"}
        ]
    
    def _identify_emergent_patterns(
        self, 
        swarm_results: List[Dict]
    ) -> Dict[str, Any]:
        """Identify emergent patterns from swarm intelligence."""
        
        # Extract key themes across all results
        all_themes = []
        for result in swarm_results:
            themes = result.get("themes", [])
            all_themes.extend(themes)
        
        # Count theme frequencies
        theme_counts = {}
        for theme in all_themes:
            theme_counts[theme] = theme_counts.get(theme, 0) + 1
        
        # Identify emergent patterns (themes appearing in multiple agents)
        emergent_threshold = len(swarm_results) * 0.4
        emergent_patterns = {
            theme: count 
            for theme, count in theme_counts.items() 
            if count >= emergent_threshold
        }
        
        return {
            "patterns": emergent_patterns,
            "total_themes": len(set(all_themes)),
            "convergence_ratio": len(emergent_patterns) / max(len(set(all_themes)), 1)
        }
    
    def _calculate_diversity_score(self, results: List[Dict]) -> float:
        """Calculate diversity score of swarm outputs."""
        if not results:
            return 0.0
        
        # Simple diversity metric based on unique insights
        all_insights = set()
        for result in results:
            insights = result.get("key_insights", [])
            all_insights.update(insights)
        
        avg_insights_per_agent = sum(
            len(r.get("key_insights", [])) for r in results
        ) / len(results)
        
        diversity_score = len(all_insights) / (avg_insights_per_agent * len(results))
        return min(diversity_score, 1.0)


class ConsensusEngine:
    """Engine for reaching consensus among swarm agents."""
    
    async def aggregate(
        self, 
        results: List[Dict], 
        threshold: float
    ) -> Dict[str, Any]:
        """Aggregate results to form consensus."""
        
        # Extract all recommendations
        all_recommendations = []
        for result in results:
            recs = result.get("recommendations", [])
            all_recommendations.extend(recs)
        
        # Calculate recommendation scores
        rec_scores = {}
        for rec in all_recommendations:
            key = rec.get("key", str(rec))
            rec_scores[key] = rec_scores.get(key, 0) + rec.get("confidence", 0.5)
        
        # Filter by consensus threshold
        consensus_items = {
            k: v for k, v in rec_scores.items() 
            if v / len(results) >= threshold
        }
        
        return {
            "consensus_recommendations": consensus_items,
            "consensus_score": len(consensus_items) / max(len(rec_scores), 1),
            "total_recommendations": len(all_recommendations),
            "unique_recommendations": len(rec_scores)
        }


# Agent implementations for swarm
class CreativeAgent:
    async def process(self, task: Dict, state: Dict) -> Dict[str, Any]:
        # Implement creative reasoning
        return {
            "themes": ["innovation", "disruption", "paradigm_shift"],
            "key_insights": ["Consider unconventional approaches", "Break traditional boundaries"],
            "recommendations": [
                {"key": "explore_novel_methods", "confidence": 0.8},
                {"key": "challenge_assumptions", "confidence": 0.9}
            ]
        }

class AnalyticalAgent:
    async def process(self, task: Dict, state: Dict) -> Dict[str, Any]:
        # Implement analytical reasoning
        return {
            "themes": ["data_driven", "evidence_based", "systematic"],
            "key_insights": ["Statistical significance required", "Empirical validation needed"],
            "recommendations": [
                {"key": "quantitative_analysis", "confidence": 0.95},
                {"key": "systematic_review", "confidence": 0.85}
            ]
        }

# ... (implement other agents similarly)
```

### 7.2 Emergent Intelligence Engine (backend/agent/nodes/emergent_intelligence_engine.py)

```python
import numpy as np
from typing import Dict, Any, List
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA

class EmergentIntelligenceEngine:
    """Synthesizes emergent intelligence from swarm collective."""
    
    def __init__(self):
        self.pattern_synthesizer = PatternSynthesizer()
        self.insight_crystallizer = InsightCrystallizer()
        self.meta_learner = MetaLearner()
    
    async def __call__(self, state: Dict[str, Any], config: Any) -> Dict[str, Any]:
        """Process swarm intelligence to extract emergent insights."""
        
        swarm_results = state.get("swarm_results", [])
        collective_insights = state.get("collective_insights", {})
        
        # Synthesize cross-agent patterns
        synthesized_patterns = await self.pattern_synthesizer.synthesize(
            swarm_results,
            collective_insights
        )
        
        # Crystallize key insights
        crystallized_insights = await self.insight_crystallizer.crystallize(
            synthesized_patterns,
            state.get("user_request", "")
        )
        
        # Meta-learning from collective intelligence
        meta_insights = await self.meta_learner.learn(
            swarm_results,
            crystallized_insights
        )
        
        # Generate emergent recommendations
        emergent_recommendations = self._generate_emergent_recommendations(
            crystallized_insights,
            meta_insights
        )
        
        return {
            "emergent_intelligence": {
                "synthesized_patterns": synthesized_patterns,
                "crystallized_insights": crystallized_insights,
                "meta_insights": meta_insights,
                "recommendations": emergent_recommendations
            },
            "intelligence_confidence": self._calculate_confidence(crystallized_insights),
            "emergence_score": self._calculate_emergence_score(meta_insights)
        }
    
    def _generate_emergent_recommendations(
        self,
        insights: Dict,
        meta_insights: Dict
    ) -> List[Dict[str, Any]]:
        """Generate recommendations from emergent intelligence."""
        
        recommendations = []
        
        # Primary recommendations from crystallized insights
        for key, insight in insights.get("primary_insights", {}).items():
            recommendations.append({
                "type": "primary",
                "recommendation": insight["recommendation"],
                "confidence": insight["confidence"],
                "supporting_agents": insight.get("supporting_agents", [])
            })
        
        # Meta-level recommendations
        for pattern in meta_insights.get("meta_patterns", []):
            recommendations.append({
                "type": "meta",
                "recommendation": pattern["insight"],
                "confidence": pattern["strength"],
                "emergence_factor": pattern.get("emergence_factor", 0)
            })
        
        # Sort by confidence and emergence
        recommendations.sort(
            key=lambda x: x["confidence"] * x.get("emergence_factor", 1),
            reverse=True
        )
        
        return recommendations[:10]  # Top 10 recommendations
    
    def _calculate_confidence(self, insights: Dict) -> float:
        """Calculate overall confidence in emergent intelligence."""
        if not insights.get("primary_insights"):
            return 0.0
        
        confidences = [
            i["confidence"] 
            for i in insights["primary_insights"].values()
        ]
        
        return np.mean(confidences) if confidences else 0.0
    
    def _calculate_emergence_score(self, meta_insights: Dict) -> float:
        """Calculate emergence score indicating novel insights."""
        meta_patterns = meta_insights.get("meta_patterns", [])
        if not meta_patterns:
            return 0.0
        
        emergence_factors = [p.get("emergence_factor", 0) for p in meta_patterns]
        return np.mean(emergence_factors)


class PatternSynthesizer:
    """Synthesizes patterns across swarm agent outputs."""
    
    async def synthesize(
        self,
        swarm_results: List[Dict],
        collective_insights: Dict
    ) -> Dict[str, Any]:
        """Synthesize cross-agent patterns."""
        
        # Extract feature vectors from each agent
        feature_vectors = self._extract_features(swarm_results)
        
        if len(feature_vectors) < 2:
            return {"patterns": [], "clusters": []}
        
        # Perform dimensionality reduction
        pca = PCA(n_components=min(3, len(feature_vectors)))
        reduced_features = pca.fit_transform(feature_vectors)
        
        # Cluster similar patterns
        clustering = DBSCAN(eps=0.5, min_samples=2)
        clusters = clustering.fit_predict(reduced_features)
        
        # Extract pattern descriptions
        patterns = []
        for cluster_id in set(clusters):
            if cluster_id == -1:  # Noise
                continue
            
            cluster_indices = [i for i, c in enumerate(clusters) if c == cluster_id]
            cluster_results = [swarm_results[i] for i in cluster_indices]
            
            pattern = {
                "cluster_id": int(cluster_id),
                "size": len(cluster_indices),
                "common_themes": self._extract_common_themes(cluster_results),
                "variance": float(np.var(reduced_features[cluster_indices])),
                "representative": cluster_results[0]  # Representative result
            }
            patterns.append(pattern)
        
        return {
            "patterns": patterns,
            "clusters": clusters.tolist(),
            "explained_variance": pca.explained_variance_ratio_.tolist()
        }
    
    def _extract_features(self, results: List[Dict]) -> np.ndarray:
        """Extract numerical features from agent results."""
        features = []
        
        for result in results:
            # Simple feature extraction (extend based on your needs)
            feature = [
                len(result.get("themes", [])),
                len(result.get("key_insights", [])),
                len(result.get("recommendations", [])),
                result.get("confidence", 0.5),
                result.get("complexity_score", 0.5)
            ]
            features.append(feature)
        
        return np.array(features)
    
    def _extract_common_themes(self, results: List[Dict]) -> List[str]:
        """Extract common themes from cluster results."""
        theme_counts = {}
        
        for result in results:
            for theme in result.get("themes", []):
                theme_counts[theme] = theme_counts.get(theme, 0) + 1
        
        # Return themes appearing in >50% of results
        threshold = len(results) * 0.5
        return [
            theme for theme, count in theme_counts.items()
            if count >= threshold
        ]


class InsightCrystallizer:
    """Crystallizes key insights from synthesized patterns."""
    
    async def crystallize(
        self,
        patterns: Dict,
        user_request: str
    ) -> Dict[str, Any]:
        """Crystallize insights from patterns."""
        
        primary_insights = {}
        secondary_insights = []
        
        for pattern in patterns.get("patterns", []):
            # Extract primary insight from pattern
            insight = {
                "summary": f"Pattern identified across {pattern['size']} agents",
                "themes": pattern["common_themes"],
                "confidence": 1.0 - pattern["variance"],
                "supporting_agents": pattern["size"],
                "recommendation": self._generate_recommendation(pattern, user_request)
            }
            
            # Classify as primary or secondary based on confidence and size
            if pattern["size"] >= 3 and insight["confidence"] > 0.7:
                key = "_".join(pattern["common_themes"][:2])
                primary_insights[key] = insight
            else:
                secondary_insights.append(insight)
        
        return {
            "primary_insights": primary_insights,
            "secondary_insights": secondary_insights,
            "total_patterns": len(patterns.get("patterns", [])),
            "crystallization_quality": self._assess_quality(primary_insights)
        }
    
    def _generate_recommendation(self, pattern: Dict, request: str) -> str:
        """Generate actionable recommendation from pattern."""
        themes = pattern["common_themes"]
        
        if "innovation" in themes:
            return "Explore innovative approaches leveraging emerging technologies"
        elif "data_driven" in themes:
            return "Implement rigorous data-driven methodology with empirical validation"
        elif "systematic" in themes:
            return "Adopt systematic framework with clear evaluation metrics"
        else:
            return f"Focus on {', '.join(themes[:2])} for optimal results"
    
    def _assess_quality(self, insights: Dict) -> float:
        """Assess crystallization quality."""
        if not insights:
            return 0.0
        
        # Quality based on confidence and diversity
        avg_confidence = np.mean([i["confidence"] for i in insights.values()])
        theme_diversity = len(set(
            theme 
            for i in insights.values() 
            for theme in i["themes"]
        ))
        
        return min((avg_confidence + theme_diversity / 10) / 2, 1.0)


class MetaLearner:
    """Learns meta-patterns from collective intelligence."""
    
    async def learn(
        self,
        swarm_results: List[Dict],
        crystallized_insights: Dict
    ) -> Dict[str, Any]:
        """Extract meta-level insights."""
        
        meta_patterns = []
        
        # Analyze convergence patterns
        convergence_analysis = self._analyze_convergence(swarm_results)
        if convergence_analysis["convergence_detected"]:
            meta_patterns.append({
                "type": "convergence",
                "insight": f"Strong convergence on {convergence_analysis['convergent_theme']}",
                "strength": convergence_analysis["strength"],
                "emergence_factor": 0.8
            })
        
        # Analyze divergence patterns
        divergence_analysis = self._analyze_divergence(swarm_results)
        if divergence_analysis["divergence_detected"]:
            meta_patterns.append({
                "type": "divergence",
                "insight": f"Creative divergence suggests multiple viable approaches",
                "strength": divergence_analysis["strength"],
                "emergence_factor": 0.9
            })
        
        # Analyze emergent complexity
        complexity_emergence = self._analyze_complexity_emergence(
            swarm_results,
            crystallized_insights
        )
        if complexity_emergence["detected"]:
            meta_patterns.append({
                "type": "complexity",
                "insight": complexity_emergence["insight"],
                "strength": complexity_emergence["strength"],
                "emerge# Multi-Agent Powered App - Complete Development Guide

## Project Overview

Building a production-ready multi-agent system with:
- **Backend**: FastAPI + LangGraph + Python agents
- **Frontend**: React 19 + Vite + Shadcn UI
- **Database**: Cloudflare D1/R2 or Supabase
- **Auth**: Dynamic.xyz with MPC wallets (Solana/Base)
- **AI Models**: Claude, GPT-4, Gemini 2.5, Perplexity, DeepSeek R1, Qwen 2.5, Grok 3
- **Mobile**: React Native Expo

## Step 1: Clean and Setup Base Repository

```bash
# Clone the Google Gemini quickstart
git clone https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart multiagent-app
cd multiagent-app

# Clean the repository
rm -rf .git
rm -rf node_modules
rm -rf __pycache__
rm -rf .pytest_cache
rm -rf dist
rm -rf build
find . -name "*.pyc" -delete
find . -name ".DS_Store" -delete

# Initialize new git repository
git init
git add .
git commit -m "Initial commit from Gemini quickstart"
```

## Step 2: Project Structure

```
multiagent-app/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”‚   â”œâ”€â”€ master_orchestrator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ enhanced_user_intent.py
â”‚   â”‚   â”‚   â”œâ”€â”€ intelligent_intent_analyzer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_gemini.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_perplexity.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_o3.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_deepseek.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_qwen.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_grok.py
â”‚   â”‚   â”‚   â”œâ”€â”€ writer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ evaluator_advanced.py
â”‚   â”‚   â”‚   â”œâ”€â”€ formatter_advanced.py
â”‚   â”‚   â”‚   â”œâ”€â”€ swarm_intelligence_coordinator.py
â”‚   â”‚   â”‚   â””â”€â”€ emergent_intelligence_engine.py
â”‚   â”‚   â”œâ”€â”€ handywriterz_state.py
â”‚   â”‚   â””â”€â”€ handywriterz_graph.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚   â”œâ”€â”€ files.py
â”‚   â”‚   â”‚   â””â”€â”€ wallet.py
â”‚   â”‚   â””â”€â”€ middleware/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â””â”€â”€ storage.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ llm_service.py
â”‚   â”‚   â”œâ”€â”€ wallet_service.py
â”‚   â”‚   â””â”€â”€ file_service.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”‚   â”œâ”€â”€ landing/
â”‚   â”‚   â”‚   â””â”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ mobile/
â”‚   â””â”€â”€ expo-app/
â”œâ”€â”€ shared/
â”‚   â””â”€â”€ types/
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

## Step 3: Backend Implementation

### 3.1 Core Dependencies (requirements.txt)

```txt
fastapi==0.109.0
uvicorn[standard]==0.27.0
python-dotenv==1.0.0
langchain==0.1.0
langgraph==0.0.25
langchain-anthropic==0.1.1
langchain-openai==0.0.5
langchain-google-genai==0.0.5
langchain-community==0.0.12
httpx==0.26.0
redis==5.0.1
supabase==2.3.0
cloudflare==3.0.0
pydantic==2.5.3
python-multipart==0.0.6
aiofiles==23.2.1
Pillow==10.2.0
numpy==1.26.3
pandas==2.1.4
scikit-learn==1.4.0
```

### 3.2 Environment Configuration (.env)

```env
# API Keys
ANTHROPIC_API_KEY=your_key
OPENAI_API_KEY=your_key
GEMINI_API_KEY=your_key
PERPLEXITY_API_KEY=your_key
DEEPSEEK_API_KEY=your_key
QWEN_API_KEY=your_key
GROK_API_KEY=your_key

# Database
DATABASE_URL=your_supabase_url
DATABASE_KEY=your_supabase_key
CLOUDFLARE_ACCOUNT_ID=your_account_id
CLOUDFLARE_API_TOKEN=your_token
CLOUDFLARE_D1_DATABASE_ID=your_database_id

# Redis
REDIS_URL=redis://localhost:6379

# Dynamic.xyz
DYNAMIC_ENVIRONMENT_ID=your_env_id
DYNAMIC_API_KEY=your_api_key

# Storage
CLOUDFLARE_R2_ACCESS_KEY=your_key
CLOUDFLARE_R2_SECRET_KEY=your_secret
CLOUDFLARE_R2_BUCKET=your_bucket
```

### 3.3 FastAPI Main Application (backend/api/main.py)

```python
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import uvicorn

from api.routes import chat, files, wallet
from api.auth import auth_router
from core.config import settings
from core.database import init_db

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await init_db()
    yield
    # Shutdown
    pass

app = FastAPI(
    title="MultiAgent AI Platform",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "https://yourdomain.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(auth_router, prefix="/api/auth", tags=["auth"])
app.include_router(chat.router, prefix="/api/chat", tags=["chat"])
app.include_router(files.router, prefix="/api/files", tags=["files"])
app.include_router(wallet.router, prefix="/api/wallet", tags=["wallet"])

# WebSocket endpoint for real-time chat
@app.websocket("/ws/{client_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str):
    await websocket.accept()
    try:
        while True:
            data = await websocket.receive_json()
            # Process with LangGraph
            response = await process_with_agents(data)
            await websocket.send_json(response)
    except WebSocketDisconnect:
        pass

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
```

### 3.4 Enhanced Master Orchestrator (backend/agent/nodes/master_orchestrator.py)

```python
from typing import Dict, Any
from langchain_core.runnables import RunnableConfig
from agent.handywriterz_state import HandyWriterzState
import asyncio

class MasterOrchestratorAgent:
    """Revolutionary master orchestrator for intelligent workflow routing."""
    
    def __init__(self):
        self.complexity_analyzer = ComplexityAnalyzer()
        self.workflow_optimizer = WorkflowOptimizer()
    
    async def __call__(
        self, 
        state: HandyWriterzState, 
        config: RunnableConfig
    ) -> Dict[str, Any]:
        """Execute master orchestration logic."""
        
        # Analyze request complexity
        complexity_analysis = await self.complexity_analyzer.analyze(
            state.get("user_request", ""),
            state.get("context_files", []),
            state.get("requirements", [])
        )
        
        # Determine optimal workflow
        workflow_plan = await self.workflow_optimizer.optimize(
            complexity_analysis,
            state.get("available_resources", {}),
            state.get("time_constraints", None)
        )
        
        # Set orchestration metadata
        orchestration_result = {
            "workflow_intelligence": {
                "academic_complexity": complexity_analysis["score"],
                "recommended_agents": workflow_plan["agents"],
                "estimated_time": workflow_plan["estimated_time"],
                "success_probability": workflow_plan["success_probability"]
            },
            "routing_decision": workflow_plan["primary_route"],
            "use_swarm_intelligence": complexity_analysis["score"] >= 8.0,
            "parallel_processing": workflow_plan["parallel_capable"]
        }
        
        return {
            "orchestration_result": orchestration_result,
            "workflow_status": "orchestrated",
            "next_node": workflow_plan["primary_route"]
        }

class ComplexityAnalyzer:
    """Analyzes request complexity using multi-dimensional scoring."""
    
    async def analyze(
        self, 
        request: str, 
        files: list, 
        requirements: list
    ) -> Dict[str, Any]:
        # Implement sophisticated complexity analysis
        score = 5.0  # Base score
        
        # Length and depth analysis
        if len(request) > 1000:
            score += 2.0
        if len(files) > 5:
            score += 1.5
        if len(requirements) > 10:
            score += 1.5
            
        # Keyword complexity
        complex_keywords = [
            "analyze", "synthesize", "evaluate", "critique",
            "multi-dimensional", "cross-reference", "systematic"
        ]
        for keyword in complex_keywords:
            if keyword in request.lower():
                score += 0.5
                
        return {
            "score": min(score, 10.0),
            "dimensions": {
                "linguistic": score * 0.3,
                "technical": score * 0.4,
                "research": score * 0.3
            }
        }

class WorkflowOptimizer:
    """Optimizes workflow based on complexity and resources."""
    
    async def optimize(
        self,
        complexity: Dict[str, Any],
        resources: Dict[str, Any],
        constraints: Any
    ) -> Dict[str, Any]:
        score = complexity["score"]
        
        if score >= 8.0:
            return {
                "primary_route": "enhanced_user_intent",
                "agents": ["swarm_coordinator", "emergent_intelligence"],
                "estimated_time": 300,
                "success_probability": 0.95,
                "parallel_capable": True
            }
        elif score >= 5.0:
            return {
                "primary_route": "enhanced_user_intent",
                "agents": ["standard_pipeline"],
                "estimated_time": 180,
                "success_probability": 0.85,
                "parallel_capable": True
            }
        else:
            return {
                "primary_route": "user_intent",
                "agents": ["basic_pipeline"],
                "estimated_time": 60,
                "success_probability": 0.90,
                "parallel_capable": False
            }
```

### 3.5 Gemini Search Agent (backend/agent/nodes/search_gemini.py)

```python
import google.generativeai as genai
from typing import Dict, Any
import asyncio

class GeminiSearchAgent:
    """Advanced Gemini-powered search with multimodal capabilities."""
    
    def __init__(self):
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        self.model = genai.GenerativeModel('gemini-2.5-pro-latest')
        self.search_config = genai.types.GenerationConfig(
            temperature=0.7,
            top_p=0.95,
            max_output_tokens=8192,
        )
    
    async def execute(
        self, 
        state: Dict[str, Any], 
        config: Any
    ) -> Dict[str, Any]:
        """Execute Gemini search with Google Search grounding."""
        
        query = state.get("search_query", state.get("user_request", ""))
        files = state.get("context_files", [])
        
        # Build multimodal prompt
        prompt_parts = [
            f"Research Query: {query}",
            "Please provide comprehensive academic insights with sources."
        ]
        
        # Add file contents if available
        for file in files:
            if file["type"] in ["image", "video", "audio"]:
                # Handle multimodal inputs
                prompt_parts.append(file["content"])
        
        # Enable Google Search grounding
        generation_config = genai.types.GenerationConfig(
            temperature=0.7,
            tools='google_search',
            tool_config={'google_search': {'dynamic_retrieval_config': {
                'mode': 'MODE_DYNAMIC',
                'dynamic_threshold': 0.3
            }}}
        )
        
        try:
            response = await asyncio.to_thread(
                self.model.generate_content,
                prompt_parts,
                generation_config=generation_config
            )
            
            # Extract search results and synthesized knowledge
            search_results = self._parse_search_results(response)
            
            return {
                "gemini_results": search_results,
                "gemini_synthesis": response.text,
                "sources_found": len(search_results),
                "multimodal_processed": len(files) > 0
            }
            
        except Exception as e:
            return {
                "error": f"Gemini search failed: {str(e)}",
                "gemini_results": [],
                "sources_found": 0
            }
    
    def _parse_search_results(self, response):
        """Parse grounded search results from Gemini response."""
        results = []
        
        # Extract grounding metadata
        if hasattr(response, 'grounding_metadata'):
            for source in response.grounding_metadata.get('search_queries', []):
                results.append({
                    "title": source.get("title", ""),
                    "url": source.get("url", ""),
                    "snippet": source.get("snippet", ""),
                    "relevance_score": source.get("score", 0.0)
                })
        
        return results
```

### 3.6 Chat Route Handler (backend/api/routes/chat.py)

```python
from fastapi import APIRouter, HTTPException, Depends, UploadFile, File
from typing import List, Optional
import asyncio

from agent.handywriterz_graph import handywriterz_graph
from api.auth import get_current_user
from services.file_service import FileService

router = APIRouter()

@router.post("/message")
async def send_message(
    message: str,
    files: Optional[List[UploadFile]] = File(None),
    current_user=Depends(get_current_user)
):
    """Process user message through multi-agent system."""
    
    # Process uploaded files
    context_files = []
    if files:
        file_service = FileService()
        for file in files:
            processed_file = await file_service.process_file(file)
            context_files.append(processed_file)
    
    # Prepare state for LangGraph
    initial_state = {
        "user_request": message,
        "context_files": context_files,
        "user_id": current_user["id"],
        "timestamp": datetime.utcnow().isoformat()
    }
    
    # Execute the graph
    try:
        config = {"configurable": {"thread_id": current_user["id"]}}
        result = await handywriterz_graph.ainvoke(initial_state, config)
        
        return {
            "success": True,
            "response": result.get("final_response", ""),
            "sources": result.get("sources", []),
            "workflow_status": result.get("workflow_status", ""),
            "processing_time": result.get("processing_time", 0)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

## Step 4: Frontend Implementation

### 4.1 Package.json

```json
{
  "name": "multiagent-frontend",
  "version": "1.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc && vite build",
    "preview": "vite preview"
  },
  "dependencies": {
    "react": "^19.0.0",
    "react-dom": "^19.0.0",
    "@dynamic-labs/sdk-react-core": "^2.0.0",
    "@dynamic-labs/ethereum": "^2.0.0",
    "@dynamic-labs/solana": "^2.0.0",
    "@radix-ui/react-avatar": "^1.0.4",
    "@radix-ui/react-dialog": "^1.0.5",
    "@radix-ui/react-dropdown-menu": "^2.0.6",
    "@radix-ui/react-scroll-area": "^1.0.5",
    "class-variance-authority": "^0.7.0",
    "clsx": "^2.1.0",
    "framer-motion": "^10.16.16",
    "lucide-react": "^0.309.0",
    "react-dropzone": "^14.2.3",
    "react-markdown": "^9.0.1",
    "react-syntax-highlighter": "^15.5.0",
    "recharts": "^2.10.3",
    "tailwind-merge": "^2.2.0",
    "zustand": "^4.4.7"
  },
  "devDependencies": {
    "@types/react": "^18.2.46",
    "@types/react-dom": "^18.2.18",
    "@vitejs/plugin-react": "^4.2.1",
    "autoprefixer": "^10.4.16",
    "postcss": "^8.4.33",
    "tailwindcss": "^3.4.0",
    "typescript": "^5.3.3",
    "vite": "^5.0.10"
  }
}
```

### 4.2 Main App Component (frontend/src/App.tsx)

```tsx
import React from 'react';
import { BrowserRouter as Router, Routes, Route } from 'react-router-dom';
import { DynamicContextProvider } from '@dynamic-labs/sdk-react-core';
import { EthereumWalletConnectors } from '@dynamic-labs/ethereum';
import { SolanaWalletConnectors } from '@dynamic-labs/solana';

import LandingPage from './pages/LandingPage';
import ChatInterface from './pages/ChatInterface';
import { ThemeProvider } from './components/theme-provider';

const App: React.FC = () => {
  return (
    <DynamicContextProvider
      settings={{
        environmentId: import.meta.env.VITE_DYNAMIC_ENVIRONMENT_ID,
        walletConnectors: [EthereumWalletConnectors, SolanaWalletConnectors],
      }}
    >
      <ThemeProvider defaultTheme="dark" storageKey="multiagent-theme">
        <Router>
          <Routes>
            <Route path="/" element={<LandingPage />} />
            <Route path="/chat" element={<ChatInterface />} />
          </Routes>
        </Router>
      </ThemeProvider>
    </DynamicContextProvider>
  );
};

export default App;
```

### 4.3 Modern Chat Interface (frontend/src/pages/ChatInterface.tsx)

```tsx
import React, { useState, useRef, useEffect } from 'react';
import { Send, Paperclip, Sparkles, User, Bot } from 'lucide-react';
import { motion, AnimatePresence } from 'framer-motion';
import { useDynamicContext } from '@dynamic-labs/sdk-react-core';

import { Button } from '@/components/ui/button';
import { Textarea } from '@/components/ui/textarea';
import { ScrollArea } from '@/components/ui/scroll-area';
import { Avatar, AvatarFallback, AvatarImage } from '@/components/ui/avatar';
import FileUploadZone from '@/components/chat/FileUploadZone';
import MessageBubble from '@/components/chat/MessageBubble';
import TypingIndicator from '@/components/chat/TypingIndicator';
import SourceCard from '@/components/chat/SourceCard';

interface Message {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: Date;
  sources?: any[];
  files?: any[];
}

const ChatInterface: React.FC = () => {
  const { user, isAuthenticated } = useDynamicContext();
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState('');
  const [files, setFiles] = useState<File[]>([]);
  const [isLoading, setIsLoading] = useState(false);
  const [isTyping, setIsTyping] = useState(false);
  const scrollRef = useRef<HTMLDivElement>(null);
  const fileInputRef = useRef<HTMLInputElement>(null);

  useEffect(() => {
    scrollRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages]);

  const handleSendMessage = async () => {
    if (!input.trim() && files.length === 0) return;

    const userMessage: Message = {
      id: Date.now().toString(),
      role: 'user',
      content: input,
      timestamp: new Date(),
      files: files.map(f => ({ name: f.name, type: f.type }))
    };

    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsLoading(true);
    setIsTyping(true);

    try {
      const formData = new FormData();
      formData.append('message', input);
      files.forEach(file => formData.append('files', file));

      const response = await fetch('/api/chat/message', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${user?.sessionToken}`
        },
        body: formData
      });

      const data = await response.json();
      
      setIsTyping(false);
      
      const assistantMessage: Message = {
        id: (Date.now() + 1).toString(),
        role: 'assistant',
        content: data.response,
        timestamp: new Date(),
        sources: data.sources
      };

      setMessages(prev => [...prev, assistantMessage]);
      setFiles([]);
    } catch (error) {
      console.error('Error sending message:', error);
      setIsTyping(false);
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <div className="flex flex-col h-screen bg-background">
      {/* Header */}
      <header className="border-b px-4 py-3 flex items-center justify-between">
        <div className="flex items-center gap-3">
          <Sparkles className="w-6 h-6 text-primary" />
          <h1 className="text-xl font-semibold">MultiAgent AI</h1>
        </div>
        <div className="flex items-center gap-4">
          <span className="text-sm text-muted-foreground">
            {isAuthenticated ? `Connected: ${user?.email}` : 'Not connected'}
          </span>
          <Avatar>
            <AvatarImage src={user?.profilePicture} />
            <AvatarFallback>{user?.email?.[0]?.toUpperCase()}</AvatarFallback>
          </Avatar>
        </div>
      </header>

      {/* Messages Area */}
      <ScrollArea className="flex-1 p-4">
        <div className="max-w-4xl mx-auto space-y-4">
          <AnimatePresence>
            {messages.map((message) => (
              <motion.div
                key={message.id}
                initial={{ opacity: 0, y: 20 }}
                animate={{ opacity: 1, y: 0 }}
                exit={{ opacity: 0, y: -20 }}
                transition={{ duration: 0.3 }}
              >
                <MessageBubble message={message} />
                {message.sources && message.sources.length > 0 && (
                  <div className="mt-3 grid grid-cols-1 md:grid-cols-2 gap-3 ml-12">
                    {message.sources.map((source, idx) => (
                      <SourceCard key={idx} source={source} />
                    ))}
                  </div>
                )}
              </motion.div>
            ))}
            {isTyping && <TypingIndicator />}
          </AnimatePresence>
          <div ref={scrollRef} />
        </div>
      </ScrollArea>

      {/* File Upload Zone */}
      {files.length > 0 && (
        <div className="px-4 py-2 border-t">
          <FileUploadZone files={files} onRemove={(idx) => {
            setFiles(prev => prev.filter((_, i) => i !== idx));
          }} />
        </div>
      )}

      {/* Input Area */}
      <div className="border-t p-4">
        <div className="max-w-4xl mx-auto">
          <div className="flex gap-2 items-end">
            <Button
              variant="ghost"
              size="icon"
              onClick={() => fileInputRef.current?.click()}
              className="mb-1"
            >
              <Paperclip className="w-5 h-5" />
            </Button>
            <input
              ref={fileInputRef}
              type="file"
              multiple
              className="hidden"
              onChange={(e) => {
                if (e.target.files) {
                  setFiles(Array.from(e.target.files));
                }
              }}
              accept="image/*,video/*,audio/*,.pdf,.doc,.docx,.txt"
            />
            <Textarea
              value={input}
              onChange={(e) => setInput(e.target.value)}
              onKeyDown={(e) => {
                if (e.key === 'Enter' && !e.shiftKey) {
                  e.preventDefault();
                  handleSendMessage();
                }
              }}
              placeholder="Ask anything... (Shift+Enter for new line)"
              className="min-h-[60px] max-h-[200px] resize-none"
              disabled={isLoading}
            />
            <Button
              onClick={handleSendMessage}
              disabled={isLoading || (!input.trim() && files.length === 0)}
              className="mb-1"
            >
              <Send className="w-5 h-5" />
            </Button>
          </div>
          <p className="text-xs text-muted-foreground mt-2 text-center">
            Powered by Claude, GPT-4, Gemini, and advanced multi-agent orchestration
          </p>
        </div>
      </div>
    </div>
  );
};

export default ChatInterface;
```

## Step 5: Docker Configuration

### 5.1 Backend Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    git \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port
EXPOSE 8000

# Run with hot reload in development
CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```

### 5.2 Docker Compose (docker-compose.yml)

```yaml
version: '3.8'

services:
  backend:
    build: 
      context: ./backend
      dockerfile: Dockerfile
    volumes:
      - ./backend:/app
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
    env_file:
      - ./backend/.env
    depends_on:
      - redis
    networks:
      - app-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    volumes:
      - ./frontend:/app
      - /app/node_modules
    ports:
      - "5173:5173"
    environment:
      - VITE_API_URL=http://localhost:8000
    networks:
      - app-network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - app-network

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - backend
      - frontend
    networks:
      - app-network

volumes:
  redis-data:

networks:
  app-network:
    driver: bridge
```

### 5.3 Makefile

```makefile
.PHONY: help dev build test deploy clean

help: ## Show this help message
	@echo 'Usage: make [target]'
	@echo ''
	@echo 'Targets:'
	@awk 'BEGIN {FS = ":.*?## "} /^[a-zA-Z_-]+:.*?## / {printf "  %-15s %s\n", $$1, $$2}' $(MAKEFILE_LIST)

dev: ## Start development environment with hot reload
	docker-compose up --build

dev-backend: ## Start only backend with hot reload
	cd backend && uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

dev-frontend: ## Start only frontend with hot reload
	cd frontend && npm run dev

build: ## Build production images
	docker-compose build --no-cache

test: ## Run tests
	cd backend && pytest tests/ -v
	cd frontend && npm test

deploy-cf: ## Deploy to Cloudflare
	cd frontend && npm run build
	npx wrangler pages publish dist --project-name multiagent-app

deploy-vercel: ## Deploy to Vercel
	vercel --prod

clean: ## Clean all containers, volumes and build artifacts
	docker-compose down -v
	rm -rf backend/__pycache__
	rm -rf frontend/node_modules
	rm -rf frontend/dist
	rm -rf mobile/node_modules

install: ## Install all dependencies
	cd backend && pip install -r requirements.txt
	cd frontend && npm install
	cd mobile && npm install

migrate: ## Run database migrations
	cd backend && python -m alembic upgrade head

seed: ## Seed database with sample data
	cd backend && python scripts/seed_db.py

logs: ## Show logs
	docker-compose logs -f

shell-backend: ## Open shell in backend container
	docker-compose exec backend bash

shell-frontend: ## Open shell in frontend container
	docker-compose exec frontend sh

## Step 8: Production Deployment

### 8.1 Cloudflare Pages Configuration (frontend/wrangler.toml)

```toml
name = "multiagent-app"
type = "webpack"
compatibility_date = "2024-07-01"

[build]
command = "npm run build"
[build.upload]
format = "service-worker"

[[kv_namespaces]]
binding = "USER_SESSIONS"
id = "your_kv_namespace_id"

[[r2_buckets]]
binding = "FILE_STORAGE"
bucket_name = "multiagent-files"

[[d1_databases]]
binding = "DB"
database_name = "multiagent-db"
database_id = "your_d1_database_id"

[env.production]
vars = { VITE_API_URL = "https://api.multiagent.app" }
```

### 8.2 Cloudflare D1 Schema (backend/db/schema.sql)

```sql
-- Users table
CREATE TABLE users (
    id TEXT PRIMARY KEY,
    email TEXT UNIQUE NOT NULL,
    wallet_address TEXT,
    chain TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Conversations table
CREATE TABLE conversations (
    id TEXT PRIMARY KEY,
    user_id TEXT NOT NULL,
    title TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES users(id)
);

-- Messages table
CREATE TABLE messages (
    id TEXT PRIMARY KEY,
    conversation_id TEXT NOT NULL,
    role TEXT NOT NULL CHECK (role IN ('user', 'assistant')),
    content TEXT NOT NULL,
    metadata JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (conversation_id) REFERENCES conversations(id)
);

-- Files table
CREATE TABLE files (
    id TEXT PRIMARY KEY,
    message_id TEXT NOT NULL,
    filename TEXT NOT NULL,
    content_type TEXT,
    size INTEGER,
    r2_key TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (message_id) REFERENCES messages(id)
);

-- Sources table
CREATE TABLE sources (
    id TEXT PRIMARY KEY,
    message_id TEXT NOT NULL,
    title TEXT,
    url TEXT,
    snippet TEXT,
    relevance_score REAL,
    metadata JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (message_id) REFERENCES messages(id)
);

-- Create indexes
CREATE INDEX idx_conversations_user_id ON conversations(user_id);
CREATE INDEX idx_messages_conversation_id ON messages(conversation_id);
CREATE INDEX idx_files_message_id ON files(message_id);
CREATE INDEX idx_sources_message_id ON sources(message_id);
```

### 8.3 API Gateway with Auth (backend/api/auth.py)

```python
from fastapi import APIRouter, HTTPException, Depends, Request
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import httpx
import jwt
from typing import Optional, Dict, Any

from core.config import settings
from core.database import get_db

router = APIRouter()
security = HTTPBearer()

class DynamicAuth:
    """Dynamic.xyz authentication handler."""
    
    def __init__(self):
        self.dynamic_api_url = "https://api.dynamic.xyz/v1"
        self.environment_id = settings.DYNAMIC_ENVIRONMENT_ID
        self.api_key = settings.DYNAMIC_API_KEY
    
    async def verify_token(self, token: str) -> Dict[str, Any]:
        """Verify Dynamic.xyz JWT token."""
        try:
            # Decode without verification first to get claims
            unverified = jwt.decode(token, options={"verify_signature": False})
            
            # Verify with Dynamic API
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.dynamic_api_url}/verify",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "X-Environment-ID": self.environment_id
                    },
                    json={"jwt": token}
                )
                
                if response.status_code != 200:
                    raise HTTPException(status_code=401, detail="Invalid token")
                
                return response.json()
                
        except Exception as e:
            raise HTTPException(status_code=401, detail=str(e))
    
    async def get_user_wallets(self, user_id: str) -> List[Dict[str, Any]]:
        """Get user's connected wallets from Dynamic."""
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{self.dynamic_api_url}/users/{user_id}/wallets",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "X-Environment-ID": self.environment_id
                }
            )
            
            if response.status_code == 200:
                return response.json()["wallets"]
            return []

dynamic_auth = DynamicAuth()

async def get_current_user(
    credentials: HTTPAuthorizationCredentials = Depends(security),
    db = Depends(get_db)
) -> Dict[str, Any]:
    """Get current authenticated user."""
    token = credentials.credentials
    
    # Verify token with Dynamic
    token_data = await dynamic_auth.verify_token(token)
    user_id = token_data["sub"]
    email = token_data.get("email")
    
    # Get or create user in database
    user = await db.fetch_one(
        "SELECT * FROM users WHERE id = ?",
        (user_id,)
    )
    
    if not user:
        # Get wallets from Dynamic
        wallets = await dynamic_auth.get_user_wallets(user_id)
        primary_wallet = wallets[0] if wallets else None
        
        # Create user
        await db.execute(
            """
            INSERT INTO users (id, email, wallet_address, chain)
            VALUES (?, ?, ?, ?)
            """,
            (
                user_id,
                email,
                primary_wallet.get("address") if primary_wallet else None,
                primary_wallet.get("chain") if primary_wallet else None
            )
        )
        
        user = {
            "id": user_id,
            "email": email,
            "wallet_address": primary_wallet.get("address") if primary_wallet else None,
            "chain": primary_wallet.get("chain") if primary_wallet else None
        }
    
    return dict(user)

@router.post("/verify")
async def verify_auth(request: Request):
    """Verify authentication status."""
    auth_header = request.headers.get("Authorization")
    if not auth_header or not auth_header.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Missing token")
    
    token = auth_header.split(" ")[1]
    user_data = await dynamic_auth.verify_token(token)
    
    return {
        "authenticated": True,
        "user_id": user_data["sub"],
        "email": user_data.get("email")
    }

auth_router = router
```

### 8.4 File Service with R2 Storage (backend/services/file_service.py)

```python
import boto3
from botocore.config import Config
import hashlib
import mimetypes
from typing import Dict, Any, BinaryIO
import aiofiles
from PIL import Image
import io

from core.config import settings

class FileService:
    """Handle file uploads to Cloudflare R2."""
    
    def __init__(self):
        self.s3_client = boto3.client(
            's3',
            endpoint_url=f'https://{settings.CLOUDFLARE_ACCOUNT_ID}.r2.cloudflarestorage.com',
            aws_access_key_id=settings.CLOUDFLARE_R2_ACCESS_KEY,
            aws_secret_access_key=settings.CLOUDFLARE_R2_SECRET_KEY,
            config=Config(signature_version='s3v4'),
            region_name='auto'
        )
        self.bucket_name = settings.CLOUDFLARE_R2_BUCKET
    
    async def process_file(self, file: Any) -> Dict[str, Any]:
        """Process and upload file to R2."""
        content = await file.read()
        file_hash = hashlib.sha256(content).hexdigest()
        
        # Determine content type
        content_type = file.content_type or mimetypes.guess_type(file.filename)[0]
        
        # Process based on file type
        if content_type and content_type.startswith('image/'):
            processed_content = await self._process_image(content)
            metadata = {"type": "image", "processed": True}
        elif content_type and content_type.startswith('video/'):
            metadata = {"type": "video", "processed": False}
            processed_content = content
        elif content_type and content_type.startswith('audio/'):
            metadata = {"type": "audio", "processed": False}
            processed_content = content
        else:
            # Text or document
            metadata = {"type": "document", "processed": False}
            processed_content = content
        
        # Upload to R2
        r2_key = f"uploads/{file_hash}/{file.filename}"
        self.s3_client.put_object(
            Bucket=self.bucket_name,
            Key=r2_key,
            Body=processed_content,
            ContentType=content_type,
            Metadata=metadata
        )
        
        # Generate presigned URL for retrieval
        url = self.s3_client.generate_presigned_url(
            'get_object',
            Params={'Bucket': self.bucket_name, 'Key': r2_key},
            ExpiresIn=3600 * 24  # 24 hours
        )
        
        return {
            "filename": file.filename,
            "content_type": content_type,
            "size": len(content),
            "r2_key": r2_key,
            "url": url,
            "metadata": metadata,
            "hash": file_hash
        }
    
    async def _process_image(self, content: bytes) -> bytes:
        """Process image for optimization."""
        img = Image.open(io.BytesIO(content))
        
        # Resize if too large
        max_size = (1920, 1080)
        if img.size[0] > max_size[0] or img.size[1] > max_size[1]:
            img.thumbnail(max_size, Image.Resampling.LANCZOS)
        
        # Convert to RGB if necessary
        if img.mode not in ('RGB', 'RGBA'):
            img = img.convert('RGB')
        
        # Save optimized image
        output = io.BytesIO()
        img.save(output, format='JPEG', quality=85, optimize=True)
        return output.getvalue()
    
    async def get_file(self, r2_key: str) -> Dict[str, Any]:
        """Retrieve file from R2."""
        try:
            response = self.s3_client.get_object(
                Bucket=self.bucket_name,
                Key=r2_key
            )
            
            return {
                "content": response['Body'].read(),
                "content_type": response['ContentType'],
                "metadata": response.get('Metadata', {})
            }
        except Exception as e:
            raise Exception(f"File not found: {str(e)}")
```

### 8.5 LLM Service Integration (backend/services/llm_service.py)

```python
import asyncio
from typing import Dict, Any, List, Optional
from abc import ABC, abstractmethod

# LLM Clients
from anthropic import AsyncAnthropic
from openai import AsyncOpenAI
import google.generativeai as genai
import httpx

from core.config import settings

class BaseLLMClient(ABC):
    """Base class for LLM clients."""
    
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        pass
    
    @abstractmethod
    async def generate_with_sources(self, prompt: str, **kwargs) -> Dict[str, Any]:
        pass

class ClaudeClient(BaseLLMClient):
    """Anthropic Claude client."""
    
    def __init__(self):
        self.client = AsyncAnthropic(api_key=settings.ANTHROPIC_API_KEY)
    
    async def generate(self, prompt: str, **kwargs) -> str:
        response = await self.client.messages.create(
            model="claude-3-opus-20240229",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=kwargs.get("max_tokens", 4096),
            temperature=kwargs.get("temperature", 0.7)
        )
        return response.content[0].text
    
    async def generate_with_sources(self, prompt: str, **kwargs) -> Dict[str, Any]:
        # Claude doesn't have built-in search, combine with search results
        content = await self.generate(prompt, **kwargs)
        return {"content": content, "sources": []}

class GPT4Client(BaseLLMClient):
    """OpenAI GPT-4 client."""
    
    def __init__(self):
        self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
    
    async def generate(self, prompt: str, **kwargs) -> str:
        response = await self.client.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=kwargs.get("max_tokens", 4096),
            temperature=kwargs.get("temperature", 0.7)
        )
        return response.choices[0].message.content
    
    async def generate_with_sources(self, prompt: str, **kwargs) -> Dict[str, Any]:
        # Use function calling for search if needed
        tools = [{
            "type": "function",
            "function": {
                "name": "search",
                "description": "Search for information",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string"}
                    },
                    "required": ["query"]
                }
            }
        }]
        
        response = await self.client.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[{"role": "user", "content": prompt}],
            tools=tools,
            tool_choice="auto"
        )
        
        # Process tool calls if any
        sources = []
        content = response.choices[0].message.content
        
        if response.choices[0].message.tool_calls:
            for tool_call in response.choices[0].message.tool_calls:
                if tool_call.function.name == "search":
                    # Perform actual search
                    query = json.loads(tool_call.function.arguments)["query"]
                    search_results = await self._perform_search(query)
                    sources.extend(search_results)
        
        return {"content": content, "sources": sources}
    
    async def _perform_search(self, query: str) -> List[Dict]:
        # Implement search logic
        return []

class GeminiClient(BaseLLMClient):
    """Google Gemini client with search grounding."""
    
    def __init__(self):
        genai.configure(api_key=settings.GEMINI_API_KEY)
        self.model = genai.GenerativeModel('gemini-pro')
    
    async def generate(self, prompt: str, **kwargs) -> str:
        response = await asyncio.to_thread(
            self.model.generate_content,
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=kwargs.get("temperature", 0.7),
                max_output_tokens=kwargs.get("max_tokens", 4096)
            )
        )
        return response.text
    
    async def generate_with_sources(self, prompt: str, **kwargs) -> Dict[str, Any]:
        # Enable Google Search grounding
        generation_config = genai.types.GenerationConfig(
            temperature=kwargs.get("temperature", 0.7),
            max_output_tokens=kwargs.get("max_tokens", 4096),
            tools='google_search',
            tool_config={'google_search': {'dynamic_retrieval_config': {
                'mode': 'MODE_DYNAMIC',
                'dynamic_threshold': 0.3
            }}}
        )
        
        response = await asyncio.to_thread(
            self.model.generate_content,
            prompt,
            generation_config=generation_config
        )
        
        # Extract sources from grounding metadata
        sources = []
        if hasattr(response, 'grounding_metadata'):
            for source in response.grounding_metadata.get('search_queries', []):
                sources.append({
                    "title": source.get("title", ""),
                    "url": source.get("url", ""),
                    "snippet": source.get("snippet", "")
                })
        
        return {"content": response.text, "sources": sources}

class PerplexityClient(BaseLLMClient):
    """Perplexity AI client with built-in search."""
    
    def __init__(self):
        self.api_key = settings.PERPLEXITY_API_KEY
        self.base_url = "https://api.perplexity.ai"
    
    async def generate(self, prompt: str, **kwargs) -> str:
        return (await self.generate_with_sources(prompt, **kwargs))["content"]
    
    async def generate_with_sources(self, prompt: str, **kwargs) -> Dict[str, Any]:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "pplx-70b-online",
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": kwargs.get("temperature", 0.7),
                    "max_tokens": kwargs.get("max_tokens", 4096),
                    "return_citations": True
                }
            )
            
            data = response.json()
            content = data["choices"][0]["message"]["content"]
            
            # Extract citations
            sources = []
            if "citations" in data:
                for citation in data["citations"]:
                    sources.append({
                        "title": citation.get("title", ""),
                        "url": citation.get("url", ""),
                        "snippet": citation.get("snippet", "")
                    })
            
            return {"content": content, "sources": sources}

class LLMService:
    """Unified LLM service for multi-model orchestration."""
    
    def __init__(self):
        self.clients = {
            "claude": ClaudeClient(),
            "gpt4": GPT4Client(),
            "gemini": GeminiClient(),
            "perplexity": PerplexityClient()
        }
    
    async def generate(
        self,
        prompt: str,
        model: str = "claude",
        **kwargs
    ) -> str:
        """Generate response using specified model."""
        if model not in self.clients:
            raise ValueError(f"Unknown model: {model}")
        
        return await self.clients[model].generate(prompt, **kwargs)
    
    async def generate_with_sources(
        self,
        prompt: str,
        model: str = "perplexity",
        **kwargs
    ) -> Dict[str, Any]:
        """Generate response with sources using specified model."""
        if model not in self.clients:
            raise ValueError(f"Unknown model: {model}")
        
        return await self.clients[model].generate_with_sources(prompt, **kwargs)
    
    async def multi_model_consensus(
        self,
        prompt: str,
        models: List[str] = ["claude", "gpt4", "gemini"],
        **kwargs
    ) -> Dict[str, Any]:
        """Generate consensus response from multiple models."""
        tasks = [
            self.clients[model].generate(prompt, **kwargs)
            for model in models
            if model in self.clients
        ]
        
        responses = await asyncio.gather(*tasks)
        
        # Simple consensus: Use Claude to synthesize
        synthesis_prompt = f"""
        Synthesize these AI responses into a single coherent answer:
        
        {chr(10).join([f"{models[i]}: {resp}" for i, resp in enumerate(responses)])}
        
        Provide a balanced, comprehensive response that captures the best insights from all models.
        """
        
        consensus = await self.clients["claude"].generate(synthesis_prompt)
        
        return {
            "consensus": consensus,
            "individual_responses": dict(zip(models, responses)),
            "models_used": models
        }
```

## Step 9: Final Integration and Testing

### 9.1 End-to-End Test Script (tests/test_e2e.py)

```python
import pytest
import asyncio
from httpx import AsyncClient
from unittest.mock import patch

from api.main import app
from agent.handywriterz_graph import handywriterz_graph

@pytest.mark.asyncio
async def test_multi_agent_workflow():
    """Test complete multi-agent workflow."""
    
    async with AsyncClient(app=app, base_url="http://test") as client:
        # Mock authentication
        with patch("api.auth.get_current_user") as mock_auth:
            mock_auth.return_value = {
                "id": "test_user",
                "email": "test@example.com"
            }
            
            # Test message with file upload
            files = [
                ("files", ("test.pdf", b"PDF content", "application/pdf"))
            ]
            
            response = await client.post(
                "/api/chat/message",
                data={"message": "Analyze this document and write a summary"},
                files=files,
                headers={"Authorization": "Bearer test_token"}
            )
            
            assert response.status_code == 200
            data = response.json()
            
            assert data["success"] is True
            assert "response" in data
            assert "sources" in data
            assert data["workflow_status"] == "completed"

@pytest.mark.asyncio
async def test_swarm_intelligence_activation():
    """Test that swarm intelligence activates for complex queries."""
    
    complex_query = """
    Conduct a comprehensive analysis of quantum computing's impact on 
    cryptography, including current vulnerabilities, post-quantum 
    cryptography solutions, timeline for quantum threat realization, 
    and recommendations for organizations to prepare.
    """
    
    state = {
        "user_request": complex_query,
        "context_files": [],
        "user_id": "test_user"
    }
    
    result = await handywriterz_graph.ainvoke(state)
    
    # Verify swarm intelligence was activated
    assert result.get("swarm_results") is not None
    assert len(result["swarm_results"]) >= 6  # At least 6 agents
    assert result.get("emergent_intelligence") is not None
    assert result["emergence_score"] > 0.7  # High emergence score

@pytest.mark.asyncio
async def test_multimodal_processing():
    """Test multimodal file processing."""
    
    async with AsyncClient(app=app, base_url="http://test") as client:
        files = [
            ("files", ("image.jpg", b"JPEG content", "image/jpeg")),
            ("files", ("audio.mp3", b"MP3 content", "audio/mpeg")),
            ("files", ("video.mp4", b"MP4 content", "video/mp4"))
        ]
        
        response = await client.post(
            "/api/chat/message",
            data={"message": "Analyze these multimedia files"},
            files=files,
            headers={"Authorization": "Bearer test_token"}
        )
        
        assert response.status_code == 200
        data = response.json()
        assert data["success"] is True
        # Verify Gemini was used for multimodal
        assert "multimodal_processed" in data
```

### 9.2 README.md

```markdown
# MultiAgent AI Platform

A production-ready multi-agent AI system featuring advanced orchestration, swarm intelligence, and multimodal processing capabilities.

## Features

- **Multi-Agent Orchestration**: Intelligent routing between specialized AI agents
- **Swarm Intelligence**: Collective problem-solving for complex queries
- **Multimodal Support**: Process text, images, audio, and video
- **Multi-LLM Integration**: Claude, GPT-4, Gemini 2.5, Perplexity, and more
- **Web3 Authentication**: Dynamic.xyz integration with MPC wallets
- **Real-time Updates**: WebSocket support for streaming responses
- **Mobile Support**: React Native app for iOS and Android

## Tech Stack

- **Backend**: FastAPI + LangGraph + Python 3.11
- **Frontend**: React 19 + Vite + Shadcn UI
- **Database**: Cloudflare D1 / Supabase
- **Storage**: Cloudflare R2
- **Auth**: Dynamic.xyz
- **Deployment**: Cloudflare Pages / Vercel

## Quick Start

### Prerequisites

- Python 3.11+
- Node.js 18+
- Docker & Docker Compose
- Cloudflare account (for D1 & R2)
- API keys for AI services

### Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/multiagent-app.git
cd multiagent-app
```

2. Set up environment variables:
```bash
cp backend/.env.example backend/.env
# Edit .env with your API keys
```

3. Install dependencies:
```bash
make install
```

4. Run development environment:
```bash
make dev
```

5. Access the app:
- Frontend: http://localhost:5173
- Backend: http://localhost:8000
- API Docs: http://localhost:8000/docs

## Architecture

The system uses a sophisticated multi-agent architecture:

1. **Master Orchestrator**: Routes requests based on complexity analysis
2. **Intent Analyzer**: Understands user requirements
3. **Search Agents**: Parallel search across multiple AI services
4. **Swarm Intelligence**: Collective reasoning for complex problems
5. **Evaluator**: Multi-model quality assessment
6. **Formatter**: Professional document generation

## Deployment

### Deploy to Cloudflare Pages

```bash
make deploy-cf
```

### Deploy to Vercel

```bash
make deploy-vercel
```

## API Usage

### Send Message with Files

```python
import httpx

async with httpx.AsyncClient() as client:
    files = [
        ("files", open("document.pdf", "rb")),
        ("files", open("image.jpg", "rb"))
    ]
    
    response = await client.post(
        "https://api.multiagent.app/api/chat/message",
        data={"message": "Analyze these files"},
        files=files,
        headers={"Authorization": f"Bearer {token}"}
    )
    
    result = response.json()
```

### WebSocket Connection

```javascript
const ws = new WebSocket('wss://api.multiagent.app/ws/client123');

ws.onmessage = (event) => {
  const data = JSON.parse(event.data);
  console.log('AI Response:', data);
};

ws.send(JSON.stringify({
  message: "Complex research query",
  context_files: []
}));
```

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Built on top of LangGraph and LangChain
- Inspired by advanced multi-agent systems research
- Uses cutting-edge AI models from Anthropic, OpenAI, Google, and others
```

## Step 10: Advanced Features Implementation

### 10.1 Real-time Monitoring Dashboard (frontend/src/pages/Dashboard.tsx)

```tsx
import React, { useState, useEffect } from 'react';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';
import {
  LineChart, Line, BarChart, Bar, XAxis, YAxis, CartesianGrid,
  Tooltip, Legend, ResponsiveContainer, PieChart, Pie, Cell
} from 'recharts';
import { Activity, Brain, Zap, Users } from 'lucide-react';

const Dashboard: React.FC = () => {
  const [metrics, setMetrics] = useState<any>({});
  const [agentActivity, setAgentActivity] = useState<any[]>([]);
  const [swarmMetrics, setSwarmMetrics] = useState<any>({});

  useEffect(() => {
    // WebSocket connection for real-time metrics
    const ws = new WebSocket('ws://localhost:8000/ws/metrics');
    
    ws.onmessage = (event) => {
      const data = JSON.parse(event.data);
      if (data.type === 'metrics') {
        setMetrics(data.metrics);
      } else if (data.type === 'agent_activity') {
        setAgentActivity(prev => [...prev.slice(-19), data.activity]);
      } else if (data.type === 'swarm_metrics') {
        setSwarmMetrics(data.swarm);
      }
    };

    return () => ws.close();
  }, []);

  const COLORS = ['#0088FE', '#00C49F', '#FFBB28', '#FF8042', '#8884d8'];

  return (
    <div className="p-6 space-y-6">
      <h1 className="text-3xl font-bold">Multi-Agent System Dashboard</h1>
      
      {/* Summary Cards */}
      <div className="grid grid-cols-1 md:grid-cols-4 gap-4">
        <Card>
          <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
            <CardTitle className="text-sm font-medium">Active Agents</CardTitle>
            <Brain className="h-4 w-4 text-muted-foreground" />
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">{metrics.activeAgents || 0}</div>
            <p className="text-xs text-muted-foreground">
              +{metrics.agentGrowth || 0}% from last hour
            </p>
          </CardContent>
        </Card>
        
        <Card>
          <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
            <CardTitle className="text-sm font-medium">Swarm Intelligence Active</CardTitle>
            <Zap className="h-4 w-4 text-muted-foreground" />
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">{swarmMetrics.active ? 'Yes' : 'No'}</div>
            <p className="text-xs text-muted-foreground">
              Consensus: {swarmMetrics.consensusScore || 0}%
            </p>
          </CardContent>
        </Card>
        
        <Card>
          <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
            <CardTitle className="text-sm font-medium">Queries Processed</CardTitle>
            <Activity className="h-4 w-4 text-muted-foreground" />
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">{metrics.totalQueries || 0}</div>
            <p className="text-xs text-muted-foreground">
              Avg response: {metrics.avgResponseTime || 0}s
            </p>
          </CardContent>
        </Card>
        
        <Card>
          <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
            <CardTitle className="text-sm font-medium">Active Users</CardTitle>
            <Users className="h-4 w-4 text-muted-foreground" />
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">{metrics.activeUsers || 0}</div>
            <p className="text-xs text-muted-foreground">
              {metrics.newUsers || 0} new today
            </p>
          </CardContent>
        </Card>
      </div>

      <Tabs defaultValue="agents" className="space-y-4">
        <TabsList>
          <TabsTrigger value="agents">Agent Activity</TabsTrigger>
          <TabsTrigger value="swarm">Swarm Intelligence</TabsTrigger>
          <TabsTrigger value="performance">Performance</TabsTrigger>
        </TabsList>
        
        <TabsContent value="agents" className="space-y-4">
          <Card>
            <CardHeader>
              <CardTitle>Agent Activity Timeline</CardTitle>
            </CardHeader>
            <CardContent>
              <ResponsiveContainer width="100%" height={300}>
                <LineChart data={agentActivity}>
                  <CartesianGrid strokeDasharray="3 3" />
                  <XAxis dataKey="time" />
                  <YAxis />
                  <Tooltip />
                  <Legend />
                  <Line type="monotone" dataKey="orchestrator" stroke="#8884d8" />
                  <Line type="monotone" dataKey="search" stroke="#82ca9d" />
                  <Line type="monotone" dataKey="writer" stroke="#ffc658" />
                  <Line type="monotone" dataKey="evaluator" stroke="#ff7c7c" />
                </LineChart>
              </ResponsiveContainer>
            </CardContent>
          </Card>
          
          <Card>
            <CardHeader>
              <CardTitle>Agent Distribution</CardTitle>
            </CardHeader>
            <CardContent>
              <ResponsiveContainer width="100%" height={300}>
                <PieChart>
                  <Pie
                    data={[
                      { name: 'Search Agents', value: 30 },
                      { name: 'Writer Agents', value: 25 },
                      { name: 'Evaluator Agents', value: 20 },
                      { name: 'Swarm Agents', value: 15 },
                      { name: 'Other', value: 10 }
                    ]}
                    cx="50%"
                    cy="50%"
                    labelLine={false}
                    outerRadius={80}
                    fill="#8884d8"
                    dataKey="value"
                  >
                    {agentActivity.map((entry, index) => (
                      <Cell key={`cell-${index}`} fill={COLORS[index % COLORS.length]} />
                    ))}
                  </Pie>
                  <Tooltip />
                </PieChart>
              </ResponsiveContainer>
            </CardContent>
          </Card>
        </TabsContent>
        
        <TabsContent value="swarm" className="space-y-4">
          <Card>
            <CardHeader>
              <CardTitle>Swarm Intelligence Metrics</CardTitle>
            </CardHeader>
            <CardContent>
              <div className="space-y-4">
                <div>
                  <div className="flex justify-between mb-2">
                    <span className="text-sm font-medium">Consensus Strength</span>
                    <span className="text-sm">{swarmMetrics.consensusScore}%</span>
                  </div>
                  <div className="w-full bg-gray-200 rounded-full h-2">
                    <div
                      className="bg-blue-600 h-2 rounded-full"
                      style={{ width: `${swarmMetrics.consensusScore}%` }}
                    />
                  </div>
                </div>
                
                <div>
                  <div className="flex justify-between mb-2">
                    <span className="text-sm font-medium">Diversity Score</span>
                    <span className="text-sm">{swarmMetrics.diversityScore}%</span>
                  </div>
                  <div className="w-full bg-gray-200 rounded-full h-2">
                    <div
                      className="bg-green-600 h-2 rounded-full"
                      style={{ width: `${swarmMetrics.diversityScore}%` }}
                    />
                  </div>
                </div>
                
                <div>
                  <div className="flex justify-between mb-2">
                    <span className="text-sm font-medium">Emergence Factor</span>
                    <span className="text-sm">{swarmMetrics.emergenceFactor}</span>
                  </div>
                  <div className="w-full bg-gray-200 rounded-full h-2">
                    <div
                      className="bg-purple-600 h-2 rounded-full"
                      style={{ width: `${swarmMetrics.emergenceFactor * 100}%` }}
                    />
                  </div>
                </div>
              </div>
            </CardContent>
          </Card>
        </TabsContent>
        
        <TabsContent value="performance" className="space-y-4">
          <Card>
            <CardHeader>
              <CardTitle>System Performance</CardTitle>
            </CardHeader>
            <CardContent>
              <ResponsiveContainer width="100%" height={300}>
                <BarChart data={[
                  { metric: 'Response Time', value: metrics.avgResponseTime || 0 },
                  { metric: 'Success Rate', value: metrics.successRate || 95 },
                  { metric: 'Cache Hit Rate', value: metrics.cacheHitRate || 60 },
                  { metric: 'API Efficiency', value: metrics.apiEfficiency || 88 }
                ]}>
                  <CartesianGrid strokeDasharray="3 3" />
                  <XAxis dataKey="metric" />
                  <YAxis />
                  <Tooltip />
                  <Bar dataKey="value" fill="#8884d8" />
                </BarChart>
              </ResponsiveContainer>
            </CardContent>
          </Card>
        </TabsContent>
      </Tabs>
    </div>
  );
};

export default Dashboard;
```

### 10.2 Wallet Service Integration (backend/services/wallet_service.py)

```python
from web3 import Web3
from solana.rpc.async_api import AsyncClient
from solana.publickey import PublicKey
from typing import Dict, Any, Optional
import httpx

from core.config import settings

class WalletService:
    """Handle wallet operations for Solana and Base chains."""
    
    def __init__(self):
        # Solana setup
        self.solana_client = AsyncClient(settings.SOLANA_RPC_URL)
        
        # Base (Ethereum L2) setup
        self.w3_base = Web3(Web3.HTTPProvider(settings.BASE_RPC_URL))
        
        # USDC contract addresses
        self.usdc_addresses = {
            "solana": "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v",
            "base": "0x833589fCD6eDb6E08f4c7C32D4f71b54bdA02913"
        }
    
    async def get_balance(
        self,
        address: str,
        chain: str = "solana"
    ) -> Dict[str, Any]:
        """Get wallet balance including USDC."""
        if chain == "solana":
            return await self._get_solana_balance(address)
        elif chain == "base":
            return await self._get_base_balance(address)
        else:
            raise ValueError(f"Unsupported chain: {chain}")
    
    async def _get_solana_balance(self, address: str) -> Dict[str, Any]:
        """Get Solana wallet balance."""
        try:
            pubkey = PublicKey(address)
            
            # Get SOL balance
            response = await self.solana_client.get_balance(pubkey)
            sol_balance = response["result"]["value"] / 1e9  # Convert lamports to SOL
            
            # Get USDC balance
            # This is simplified - in production, use proper SPL token queries
            usdc_balance = await self._get_spl_token_balance(
                address,
                self.usdc_addresses["solana"]
            )
            
            return {
                "chain": "solana",
                "address": address,
                "balances": {
                    "SOL": sol_balance,
                    "USDC": usdc_balance
                },
                "success": True
            }
        except Exception as e:
            return {
                "chain": "solana",
                "address": address,
                "error": str(e),
                "success": False
            }
    
    async def _get_base_balance(self, address: str) -> Dict[str, Any]:
        """Get Base chain wallet balance."""
        try:
            # Validate address
            if not self.w3_base.is_address(address):
                raise ValueError("Invalid Ethereum address")
            
            # Get ETH balance
            eth_balance = self.w3_base.eth.get_balance(address)
            eth_balance_ether = self.w3_base.from_wei(eth_balance, 'ether')
            
            # Get USDC balance (ERC20)
            usdc_contract = self.w3_base.eth.contract(
                address=self.usdc_addresses["base"],
                abi=self._get_erc20_abi()
            )
            usdc_balance = usdc_contract.functions.balanceOf(address).call()
            usdc_balance_human = usdc_balance / 1e6  # USDC has 6 decimals
            
            return {
                "chain": "base",
                "address": address,
                "balances": {
                    "ETH": float(eth_balance_ether),
                    "USDC": usdc_balance_human
                },
                "success": True
            }
        except Exception as e:
            return {
                "chain": "base",
                "address": address,
                "error": str(e),
                "success": False
            }
    
    async def _get_spl_token_balance(
        self,
        wallet_address: str,
        token_mint: str
    ) -> float:
        """Get SPL token balance on Solana."""
        # Simplified implementation
        # In production, use proper SPL token account queries
        return 0.0
    
    def _get_erc20_abi(self) -> list:
        """Get minimal ERC20 ABI for balance queries."""
        return [{
            "constant": True,
            "inputs": [{"name": "_owner", "type": "address"}],
            "name": "balanceOf",
            "outputs": [{"name": "balance", "type": "uint256"}],
            "type": "function"
        }]
    
    async def estimate_transaction_cost(
        self,
        chain: str,
        transaction_type: str = "transfer"
    ) -> Dict[str, Any]:
        """Estimate transaction costs."""
        if chain == "solana":
            # Solana fees are relatively fixed
            return {
                "chain": "solana",
                "estimated_fee": 0.00025,  # SOL
                "currency": "SOL"
            }
        elif chain == "base":
            # Get current gas price
            gas_price = self.w3_base.eth.gas_price
            
            # Estimate gas for different transaction types
            gas_estimates = {
                "transfer": 21000,  # Basic ETH transfer
                "token_transfer": 65000,  # ERC20 transfer
                "contract_interaction": 150000  # Complex contract call
            }
            
            gas_limit = gas_estimates.get(transaction_type, 100000)
            estimated_fee = self.w3_base.from_wei(gas_price * gas_limit, 'ether')
            
            return {
                "chain": "base",
                "estimated_fee": float(estimated_fee),
                "currency": "ETH",
                "gas_price": gas_price,
                "gas_limit": gas_limit
            }
        else:
            raise ValueError(f"Unsupported chain: {chain}")
```

### 10.3 Production Configuration Files

#### nginx.conf
```nginx
events {
    worker_connections 1024;
}

http {
    upstream backend {
        server backend:8000;
    }

    upstream frontend {
        server frontend:5173;
    }

    server {
        listen 80;
        server_name multiagent.app;

        # Redirect to HTTPS
        return 301 https://$server_name$request_uri;
    }

    server {
        listen 443 ssl http2;
        server_name multiagent.app;

        ssl_certificate /etc/ssl/certs/cert.pem;
        ssl_certificate_key /etc/ssl/private/key.pem;

        # API routes
        location /api {
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # WebSocket support
        location /ws {
            proxy_pass http://backend;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }

        # Frontend
        location / {
            proxy_pass http://frontend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }
    }
}
```

#### .github/workflows/deploy.yml
```yaml
name: Deploy to Production

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install backend dependencies
        run: |
          cd backend
          pip install -r requirements.txt
          pip install pytest pytest-asyncio
      
      - name: Run backend tests
        run: |
          cd backend
          pytest tests/ -v
      
      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
      
      - name: Install frontend dependencies
        run: |
          cd frontend
          npm ci
      
      - name: Run frontend tests
        run: |
          cd frontend
          npm test

  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Deploy backend to Cloud Run
        uses: google-github-actions/deploy-cloudrun@v1
        with:
          service: multiagent-backend
          image: gcr.io/${{ secrets.GCP_PROJECT }}/multiagent-backend
          region: us-central1
      
      - name: Deploy frontend to Cloudflare Pages
        uses: cloudflare/pages-action@v1
        with:
          apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          accountId: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          projectName: multiagent-app
          directory: frontend/dist
          gitHubToken: ${{ secrets.GITHUB_TOKEN }}
```

## Conclusion

This complete multi-agent powered application demonstrates:

1. **Advanced Multi-Agent Orchestration**: Intelligent routing and coordination between specialized agents
2. **Swarm Intelligence**: Collective problem-solving for complex queries
3. **Production-Ready Architecture**: Scalable, secure, and maintainable
4. **Modern UI/UX**: Beautiful chat interface with real-time updates
5. **Web3 Integration**: Wallet support with Dynamic.xyz
6. **Multimodal Processing**: Handle text, images, audio, and video
7. **Comprehensive Monitoring**: Real-time dashboard for system insights

The system is ready for deployment and can handle enterprise-level workloads with its robust architecture and comprehensive feature set.## Step 6: Mobile App (React Native Expo)

### 6.1 Expo App Structure (mobile/expo-app/App.tsx)

```tsx
import React from 'react';
import { NavigationContainer } from '@react-navigation/native';
import { createStackNavigator } from '@react-navigation/stack';
import { SafeAreaProvider } from 'react-native-safe-area-context';
import { QueryClient, QueryClientProvider } from '@tanstack/react-query';

import HomeScreen from './screens/HomeScreen';
import ChatScreen from './screens/ChatScreen';
import ProfileScreen from './screens/ProfileScreen';

const Stack = createStackNavigator();
const queryClient = new QueryClient();

export default function App() {
  return (
    <QueryClientProvider client={queryClient}>
      <SafeAreaProvider>
        <NavigationContainer>
          <Stack.Navigator initialRouteName="Home">
            <Stack.Screen 
              name="Home" 
              component={HomeScreen}
              options={{ headerShown: false }}
            />
            <Stack.Screen 
              name="Chat" 
              component={ChatScreen}
              options={{ title: 'AI Assistant' }}
            />
            <Stack.Screen 
              name="Profile" 
              component={ProfileScreen}
              options={{ title: 'Profile' }}
            />
          </Stack.Navigator>
        </NavigationContainer>
      </SafeAreaProvider>
    </QueryClientProvider>
  );
}
```

### 6.2 Mobile Chat Screen (mobile/expo-app/screens/ChatScreen.tsx)

```tsx
import React, { useState, useRef } from 'react';
import {
  View,
  Text,
  TextInput,
  TouchableOpacity,
  FlatList,
  KeyboardAvoidingView,
  Platform,
  StyleSheet,
  ActivityIndicator,
} from 'react-native';
import { Ionicons } from '@expo/vector-icons';
import * as DocumentPicker from 'expo-document-picker';
import * as ImagePicker from 'expo-image-picker';

const ChatScreen = () => {
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [attachments, setAttachments] = useState([]);
  const flatListRef = useRef(null);

  const sendMessage = async () => {
    if (!input.trim() && attachments.length === 0) return;

    const userMessage = {
      id: Date.now(),
      role: 'user',
      content: input,
      attachments: [...attachments],
      timestamp: new Date(),
    };

    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setAttachments([]);
    setIsLoading(true);

    try {
      const formData = new FormData();
      formData.append('message', input);
      
      attachments.forEach((file, index) => {
        formData.append('files', {
          uri: file.uri,
          type: file.mimeType || 'application/octet-stream',
          name: file.name || `file_${index}`,
        });
      });

      const response = await fetch(`${API_URL}/api/chat/message`, {
        method: 'POST',
        body: formData,
        headers: {
          'Content-Type': 'multipart/form-data',
        },
      });

      const data = await response.json();

      const assistantMessage = {
        id: Date.now() + 1,
        role: 'assistant',
        content: data.response,
        sources: data.sources,
        timestamp: new Date(),
      };

      setMessages(prev => [...prev, assistantMessage]);
    } catch (error) {
      console.error('Error:', error);
    } finally {
      setIsLoading(false);
    }
  };

  const pickDocument = async () => {
    const result = await DocumentPicker.getDocumentAsync({
      type: '*/*',
      multiple: true,
    });

    if (!result.canceled) {
      setAttachments(prev => [...prev, ...result.assets]);
    }
  };

  const pickImage = async () => {
    const result = await ImagePicker.launchImageLibraryAsync({
      mediaTypes: ImagePicker.MediaTypeOptions.All,
      allowsMultipleSelection: true,
      quality: 1,
    });

    if (!result.canceled) {
      setAttachments(prev => [...prev, ...result.assets]);
    }
  };

  const renderMessage = ({ item }) => (
    <View style={[
      styles.messageBubble,
      item.role === 'user' ? styles.userMessage : styles.assistantMessage
    ]}>
      <Text style={styles.messageText}>{item.content}</Text>
      {item.attachments && item.attachments.length > 0 && (
        <View style={styles.attachmentsList}>
          {item.attachments.map((att, idx) => (
            <Text key={idx} style={styles.attachmentText}>
              ðŸ“Ž {att.name}
            </Text>
          ))}
        </View>
      )}
      {item.sources && item.sources.length > 0 && (
        <View style={styles.sourcesList}>
          {item.sources.map((source, idx) => (
            <TouchableOpacity key={idx} style={styles.sourceCard}>
              <Text style={styles.sourceTitle}>{source.title}</Text>
              <Text style={styles.sourceUrl}>{source.url}</Text>
            </TouchableOpacity>
          ))}
        </View>
      )}
    </View>
  );

  return (
    <KeyboardAvoidingView 
      style={styles.container}
      behavior={Platform.OS === 'ios' ? 'padding' : 'height'}
    >
      <FlatList
        ref={flatListRef}
        data={messages}
        renderItem={renderMessage}
        keyExtractor={item => item.id.toString()}
        contentContainerStyle={styles.messagesList}
        onContentSizeChange={() => flatListRef.current?.scrollToEnd()}
      />

      {attachments.length > 0 && (
        <View style={styles.attachmentsBar}>
          {attachments.map((att, idx) => (
            <View key={idx} style={styles.attachmentChip}>
              <Text style={styles.attachmentChipText}>{att.name}</Text>
              <TouchableOpacity
                onPress={() => setAttachments(prev => prev.filter((_, i) => i !== idx))}
              >
                <Ionicons name="close-circle" size={16} color="#666" />
              </TouchableOpacity>
            </View>
          ))}
        </View>
      )}

      <View style={styles.inputContainer}>
        <TouchableOpacity onPress={pickDocument} style={styles.attachButton}>
          <Ionicons name="attach" size={24} color="#666" />
        </TouchableOpacity>
        <TouchableOpacity onPress={pickImage} style={styles.attachButton}>
          <Ionicons name="image" size={24} color="#666" />
        </TouchableOpacity>
        <TextInput
          style={styles.input}
          value={input}
          onChangeText={setInput}
          placeholder="Ask anything..."
          multiline
          maxHeight={100}
        />
        <TouchableOpacity 
          onPress={sendMessage}
          disabled={isLoading}
          style={[styles.sendButton, isLoading && styles.sendButtonDisabled]}
        >
          {isLoading ? (
            <ActivityIndicator size="small" color="#fff" />
          ) : (
            <Ionicons name="send" size={20} color="#fff" />
          )}
        </TouchableOpacity>
      </View>
    </KeyboardAvoidingView>
  );
};

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#f5f5f5',
  },
  messagesList: {
    padding: 16,
  },
  messageBubble: {
    marginVertical: 4,
    padding: 12,
    borderRadius: 16,
    maxWidth: '80%',
  },
  userMessage: {
    alignSelf: 'flex-end',
    backgroundColor: '#007AFF',
  },
  assistantMessage: {
    alignSelf: 'flex-start',
    backgroundColor: '#fff',
    borderWidth: 1,
    borderColor: '#e0e0e0',
  },
  messageText: {
    fontSize: 16,
    color: '#000',
  },
  attachmentsList: {
    marginTop: 8,
  },
  attachmentText: {
    fontSize: 12,
    color: '#666',
  },
  sourcesList: {
    marginTop: 8,
  },
  sourceCard: {
    padding: 8,
    backgroundColor: '#f0f0f0',
    borderRadius: 8,
    marginTop: 4,
  },
  sourceTitle: {
    fontSize: 12,
    fontWeight: 'bold',
  },
  sourceUrl: {
    fontSize: 10,
    color: '#666',
  },
  attachmentsBar: {
    flexDirection: 'row',
    padding: 8,
    backgroundColor: '#fff',
    borderTopWidth: 1,
    borderTopColor: '#e0e0e0',
  },
  attachmentChip: {
    flexDirection: 'row',
    alignItems: 'center',
    backgroundColor: '#f0f0f0',
    paddingHorizontal: 8,
    paddingVertical: 4,
    borderRadius: 12,
    marginRight: 8,
  },
  attachmentChipText: {
    fontSize: 12,
    marginRight: 4,
  },
  inputContainer: {
    flexDirection: 'row',
    alignItems: 'flex-end',
    padding: 8,
    backgroundColor: '#fff',
    borderTopWidth: 1,
    borderTopColor: '#e0e0e0',
  },
  attachButton: {
    padding: 8,
  },
  input: {
    flex: 1,
    borderWidth: 1,
    borderColor: '#e0e0e0',
    borderRadius: 20,
    paddingHorizontal: 16,
    paddingVertical: 8,
    marginRight: 8,
    fontSize: 16,
  },
  sendButton: {
    backgroundColor: '#007AFF',
    width: 40,
    height: 40,
    borderRadius: 20,
    justifyContent: 'center',
    alignItems: 'center',
  },
  sendButtonDisabled: {
    opacity: 0.5,
  },
});

export default ChatScreen;
```

## Step 7: Advanced Agent Nodes

### 7.1 Swarm Intelligence Coordinator (backend/agent/nodes/swarm_intelligence_coordinator.py)

```python
import asyncio
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor
import numpy as np

class SwarmIntelligenceCoordinator:
    """Coordinates multiple AI agents for collective intelligence."""
    
    def __init__(self):
        self.agent_pool = self._initialize_agent_pool()
        self.consensus_engine = ConsensusEngine()
        self.diversity_optimizer = DiversityOptimizer()
    
    async def __call__(self, state: Dict[str, Any], config: Any) -> Dict[str, Any]:
        """Execute swarm intelligence coordination."""
        
        # Prepare swarm tasks
        swarm_tasks = self._prepare_swarm_tasks(state)
        
        # Deploy agent swarm with diversity optimization
        swarm_results = await self._deploy_swarm(swarm_tasks, state)
        
        # Aggregate insights through consensus
        collective_insights = await self.consensus_engine.aggregate(
            swarm_results,
            state.get("consensus_threshold", 0.7)
        )
        
        # Synthesize emergent patterns
        emergent_patterns = self._identify_emergent_patterns(swarm_results)
        
        return {
            "swarm_results": swarm_results,
            "collective_insights": collective_insights,
            "emergent_patterns": emergent_patterns,
            "consensus_strength": collective_insights.get("consensus_score", 0),
            "diversity_score": self._calculate_diversity_score(swarm_results),
            "swarm_size": len(swarm_results)
        }
    
    def _initialize_agent_pool(self) -> List[Any]:
        """Initialize diverse agent pool for swarm intelligence."""
        return [
            CreativeAgent(),
            AnalyticalAgent(),
            CriticalAgent(),
            SynthesisAgent(),
            PatternRecognitionAgent(),
            OutlierDetectionAgent()
        ]
    
    async def _deploy_swarm(
        self, 
        tasks: List[Dict], 
        state: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Deploy agent swarm in parallel."""
        
        async def execute_agent(agent, task):
            try:
                return await agent.process(task, state)
            except Exception as e:
                return {"error": str(e), "agent": agent.__class__.__name__}
        
        # Execute agents in parallel
        tasks_list = []
        for i, agent in enumerate(self.agent_pool):
            task = tasks[i % len(tasks)] if tasks else {"default": True}
            tasks_list.append(execute_agent(agent, task))
        
        results = await asyncio.gather(*tasks_list)
        return [r for r in results if "error" not in r]
    
    def _prepare_swarm_tasks(self, state: Dict[str, Any]) -> List[Dict]:
        """Prepare diverse tasks for swarm agents."""
        base_request = state.get("user_request", "")
        
        return [
            {"perspective": "creative", "focus": "novel solutions"},
            {"perspective": "analytical", "focus": "data-driven insights"},
            {"perspective": "critical", "focus": "identify weaknesses"},
            {"perspective": "synthesis", "focus": "integrate ideas"},
            {"perspective": "pattern", "focus": "recurring themes"},
            {"perspective": "outlier", "focus": "unconventional approaches"}
        ]
    
    def _identify_emergent_patterns(
        self, 
        swarm_results: List[Dict]
    ) -> Dict[str, Any]:
        """Identify emergent patterns from swarm intelligence."""
        
        # Extract key themes across all results
        all_themes = []
        for result in swarm_results:
            themes = result.get("themes", [])
            all_themes.extend(themes)
        
        # Count theme frequencies
        theme_counts = {}
        for theme in all_themes:
            theme_counts[theme] = theme_counts.get(theme, 0) + 1
        
        # Identify emergent patterns (themes appearing in multiple agents)
        emergent_threshold = len(swarm_results) * 0.4
        emergent_patterns = {
            theme: count 
            for theme, count in theme_counts.items() 
            if count >= emergent_threshold
        }
        
        return {
            "patterns": emergent_patterns,
            "total_themes": len(set(all_themes)),
            "convergence_ratio": len(emergent_patterns) / max(len(set(all_themes)), 1)
        }
    
    def _calculate_diversity_score(self, results: List[Dict]) -> float:
        """Calculate diversity score of swarm outputs."""
        if not results:
            return 0.0
        
        # Simple diversity metric based on unique insights
        all_insights = set()
        for result in results:
            insights = result.get("key_insights", [])
            all_insights.update(insights)
        
        avg_insights_per_agent = sum(
            len(r.get("key_insights", [])) for r in results
        ) / len(results)
        
        diversity_score = len(all_insights) / (avg_insights_per_agent * len(results))
        return min(diversity_score, 1.0)


class ConsensusEngine:
    """Engine for reaching consensus among swarm agents."""
    
    async def aggregate(
        self, 
        results: List[Dict], 
        threshold: float
    ) -> Dict[str, Any]:
        """Aggregate results to form consensus."""
        
        # Extract all recommendations
        all_recommendations = []
        for result in results:
            recs = result.get("recommendations", [])
            all_recommendations.extend(recs)
        
        # Calculate recommendation scores
        rec_scores = {}
        for rec in all_recommendations:
            key = rec.get("key", str(rec))
            rec_scores[key] = rec_scores.get(key, 0) + rec.get("confidence", 0.5)
        
        # Filter by consensus threshold
        consensus_items = {
            k: v for k, v in rec_scores.items() 
            if v / len(results) >= threshold
        }
        
        return {
            "consensus_recommendations": consensus_items,
            "consensus_score": len(consensus_items) / max(len(rec_scores), 1),
            "total_recommendations": len(all_recommendations),
            "unique_recommendations": len(rec_scores)
        }


# Agent implementations for swarm
class CreativeAgent:
    async def process(self, task: Dict, state: Dict) -> Dict[str, Any]:
        # Implement creative reasoning
        return {
            "themes": ["innovation", "disruption", "paradigm_shift"],
            "key_insights": ["Consider unconventional approaches", "Break traditional boundaries"],
            "recommendations": [
                {"key": "explore_novel_methods", "confidence": 0.8},
                {"key": "challenge_assumptions", "confidence": 0.9}
            ]
        }

class AnalyticalAgent:
    async def process(self, task: Dict, state: Dict) -> Dict[str, Any]:
        # Implement analytical reasoning
        return {
            "themes": ["data_driven", "evidence_based", "systematic"],
            "key_insights": ["Statistical significance required", "Empirical validation needed"],
            "recommendations": [
                {"key": "quantitative_analysis", "confidence": 0.95},
                {"key": "systematic_review", "confidence": 0.85}
            ]
        }

# ... (implement other agents similarly)
```

### 7.2 Emergent Intelligence Engine (backend/agent/nodes/emergent_intelligence_engine.py)

```python
import numpy as np
from typing import Dict, Any, List
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA

class EmergentIntelligenceEngine:
    """Synthesizes emergent intelligence from swarm collective."""
    
    def __init__(self):
        self.pattern_synthesizer = PatternSynthesizer()
        self.insight_crystallizer = InsightCrystallizer()
        self.meta_learner = MetaLearner()
    
    async def __call__(self, state: Dict[str, Any], config: Any) -> Dict[str, Any]:
        """Process swarm intelligence to extract emergent insights."""
        
        swarm_results = state.get("swarm_results", [])
        collective_insights = state.get("collective_insights", {})
        
        # Synthesize cross-agent patterns
        synthesized_patterns = await self.pattern_synthesizer.synthesize(
            swarm_results,
            collective_insights
        )
        
        # Crystallize key insights
        crystallized_insights = await self.insight_crystallizer.crystallize(
            synthesized_patterns,
            state.get("user_request", "")
        )
        
        # Meta-learning from collective intelligence
        meta_insights = await self.meta_learner.learn(
            swarm_results,
            crystallized_insights
        )
        
        # Generate emergent recommendations
        emergent_recommendations = self._generate_emergent_recommendations(
            crystallized_insights,
            meta_insights
        )
        
        return {
            "emergent_intelligence": {
                "synthesized_patterns": synthesized_patterns,
                "crystallized_insights": crystallized_insights,
                "meta_insights": meta_insights,
                "recommendations": emergent_recommendations
            },
            "intelligence_confidence": self._calculate_confidence(crystallized_insights),
            "emergence_score": self._calculate_emergence_score(meta_insights)
        }
    
    def _generate_emergent_recommendations(
        self,
        insights: Dict,
        meta_insights: Dict
    ) -> List[Dict[str, Any]]:
        """Generate recommendations from emergent intelligence."""
        
        recommendations = []
        
        # Primary recommendations from crystallized insights
        for key, insight in insights.get("primary_insights", {}).items():
            recommendations.append({
                "type": "primary",
                "recommendation": insight["recommendation"],
                "confidence": insight["confidence"],
                "supporting_agents": insight.get("supporting_agents", [])
            })
        
        # Meta-level recommendations
        for pattern in meta_insights.get("meta_patterns", []):
            recommendations.append({
                "type": "meta",
                "recommendation": pattern["insight"],
                "confidence": pattern["strength"],
                "emergence_factor": pattern.get("emergence_factor", 0)
            })
        
        # Sort by confidence and emergence
        recommendations.sort(
            key=lambda x: x["confidence"] * x.get("emergence_factor", 1),
            reverse=True
        )
        
        return recommendations[:10]  # Top 10 recommendations
    
    def _calculate_confidence(self, insights: Dict) -> float:
        """Calculate overall confidence in emergent intelligence."""
        if not insights.get("primary_insights"):
            return 0.0
        
        confidences = [
            i["confidence"] 
            for i in insights["primary_insights"].values()
        ]
        
        return np.mean(confidences) if confidences else 0.0
    
    def _calculate_emergence_score(self, meta_insights: Dict) -> float:
        """Calculate emergence score indicating novel insights."""
        meta_patterns = meta_insights.get("meta_patterns", [])
        if not meta_patterns:
            return 0.0
        
        emergence_factors = [p.get("emergence_factor", 0) for p in meta_patterns]
        return np.mean(emergence_factors)


class PatternSynthesizer:
    """Synthesizes patterns across swarm agent outputs."""
    
    async def synthesize(
        self,
        swarm_results: List[Dict],
        collective_insights: Dict
    ) -> Dict[str, Any]:
        """Synthesize cross-agent patterns."""
        
        # Extract feature vectors from each agent
        feature_vectors = self._extract_features(swarm_results)
        
        if len(feature_vectors) < 2:
            return {"patterns": [], "clusters": []}
        
        # Perform dimensionality reduction
        pca = PCA(n_components=min(3, len(feature_vectors)))
        reduced_features = pca.fit_transform(feature_vectors)
        
        # Cluster similar patterns
        clustering = DBSCAN(eps=0.5, min_samples=2)
        clusters = clustering.fit_predict(reduced_features)
        
        # Extract pattern descriptions
        patterns = []
        for cluster_id in set(clusters):
            if cluster_id == -1:  # Noise
                continue
            
            cluster_indices = [i for i, c in enumerate(clusters) if c == cluster_id]
            cluster_results = [swarm_results[i] for i in cluster_indices]
            
            pattern = {
                "cluster_id": int(cluster_id),
                "size": len(cluster_indices),
                "common_themes": self._extract_common_themes(cluster_results),
                "variance": float(np.var(reduced_features[cluster_indices])),
                "representative": cluster_results[0]  # Representative result
            }
            patterns.append(pattern)
        
        return {
            "patterns": patterns,
            "clusters": clusters.tolist(),
            "explained_variance": pca.explained_variance_ratio_.tolist()
        }
    
    def _extract_features(self, results: List[Dict]) -> np.ndarray:
        """Extract numerical features from agent results."""
        features = []
        
        for result in results:
            # Simple feature extraction (extend based on your needs)
            feature = [
                len(result.get("themes", [])),
                len(result.get("key_insights", [])),
                len(result.get("recommendations", [])),
                result.get("confidence", 0.5),
                result.get("complexity_score", 0.5)
            ]
            features.append(feature)
        
        return np.array(features)
    
    def _extract_common_themes(self, results: List[Dict]) -> List[str]:
        """Extract common themes from cluster results."""
        theme_counts = {}
        
        for result in results:
            for theme in result.get("themes", []):
                theme_counts[theme] = theme_counts.get(theme, 0) + 1
        
        # Return themes appearing in >50% of results
        threshold = len(results) * 0.5
        return [
            theme for theme, count in theme_counts.items()
            if count >= threshold
        ]


class InsightCrystallizer:
    """Crystallizes key insights from synthesized patterns."""
    
    async def crystallize(
        self,
        patterns: Dict,
        user_request: str
    ) -> Dict[str, Any]:
        """Crystallize insights from patterns."""
        
        primary_insights = {}
        secondary_insights = []
        
        for pattern in patterns.get("patterns", []):
            # Extract primary insight from pattern
            insight = {
                "summary": f"Pattern identified across {pattern['size']} agents",
                "themes": pattern["common_themes"],
                "confidence": 1.0 - pattern["variance"],
                "supporting_agents": pattern["size"],
                "recommendation": self._generate_recommendation(pattern, user_request)
            }
            
            # Classify as primary or secondary based on confidence and size
            if pattern["size"] >= 3 and insight["confidence"] > 0.7:
                key = "_".join(pattern["common_themes"][:2])
                primary_insights[key] = insight
            else:
                secondary_insights.append(insight)
        
        return {
            "primary_insights": primary_insights,
            "secondary_insights": secondary_insights,
            "total_patterns": len(patterns.get("patterns", [])),
            "crystallization_quality": self._assess_quality(primary_insights)
        }
    
    def _generate_recommendation(self, pattern: Dict, request: str) -> str:
        """Generate actionable recommendation from pattern."""
        themes = pattern["common_themes"]
        
        if "innovation" in themes:
            return "Explore innovative approaches leveraging emerging technologies"
        elif "data_driven" in themes:
            return "Implement rigorous data-driven methodology with empirical validation"
        elif "systematic" in themes:
            return "Adopt systematic framework with clear evaluation metrics"
        else:
            return f"Focus on {', '.join(themes[:2])} for optimal results"
    
    def _assess_quality(self, insights: Dict) -> float:
        """Assess crystallization quality."""
        if not insights:
            return 0.0
        
        # Quality based on confidence and diversity
        avg_confidence = np.mean([i["confidence"] for i in insights.values()])
        theme_diversity = len(set(
            theme 
            for i in insights.values() 
            for theme in i["themes"]
        ))
        
        return min((avg_confidence + theme_diversity / 10) / 2, 1.0)


class MetaLearner:
    """Learns meta-patterns from collective intelligence."""
    
    async def learn(
        self,
        swarm_results: List[Dict],
        crystallized_insights: Dict
    ) -> Dict[str, Any]:
        """Extract meta-level insights."""
        
        meta_patterns = []
        
        # Analyze convergence patterns
        convergence_analysis = self._analyze_convergence(swarm_results)
        if convergence_analysis["convergence_detected"]:
            meta_patterns.append({
                "type": "convergence",
                "insight": f"Strong convergence on {convergence_analysis['convergent_theme']}",
                "strength": convergence_analysis["strength"],
                "emergence_factor": 0.8
            })
        
        # Analyze divergence patterns
        divergence_analysis = self._analyze_divergence(swarm_results)
        if divergence_analysis["divergence_detected"]:
            meta_patterns.append({
                "type": "divergence",
                "insight": f"Creative divergence suggests multiple viable approaches",
                "strength": divergence_analysis["strength"],
                "emergence_factor": 0.9
            })
        
        # Analyze emergent complexity
        complexity_emergence = self._analyze_complexity_emergence(
            swarm_results,
            crystallized_insights
        )
        if complexity_emergence["detected"]:
            meta_patterns.append({
                "type": "complexity",
                "insight": complexity_emergence["insight"],
                "strength": complexity_emergence["strength"],
                "emergence_factor": 1.0
            })
        
        return {
            "meta_patterns": meta_patterns,
            "learning_confidence": self._calculate_learning_confidence(meta_patterns),
            "emergence_indicators": {
                "convergence": convergence_analysis,
                "divergence": divergence_analysis,
                "complexity": complexity_emergence
            }
        }
    
    def _analyze_convergence(self, results: List[Dict]) -> Dict[str, Any]:
        """Analyze convergence patterns in swarm results."""
        all_themes = []
        for result in results:
            all_themes.extend(result.get("themes", []))
        
        if not all_themes:
            return {"convergence_detected": False}
        
        # Find most common theme
        theme_counts = {}
        for theme in all_themes:
            theme_counts[theme] = theme_counts.get(theme, 0) + 1
        
        max_theme = max(theme_counts, key=theme_counts.get)
        max_count = theme_counts[max_theme]
        
        convergence_ratio = max_count / len(results)
        
        return {
            "convergence_detected": convergence_ratio > 0.6,
            "convergent_theme": max_theme,
            "strength": convergence_ratio,
            "participating_agents": max_count
        }# Multi-Agent Powered App - Complete Development Guide

## Project Overview

Building a production-ready multi-agent system with:
- **Backend**: FastAPI + LangGraph + Python agents
- **Frontend**: React 19 + Vite + Shadcn UI
- **Database**: Cloudflare D1/R2 or Supabase
- **Auth**: Dynamic.xyz with MPC wallets (Solana/Base)
- **AI Models**: Claude, GPT-4, Gemini 2.5, Perplexity, DeepSeek R1, Qwen 2.5, Grok 3
- **Mobile**: React Native Expo

## Step 1: Clean and Setup Base Repository

```bash
# Clone the Google Gemini quickstart
git clone https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart multiagent-app
cd multiagent-app

# Clean the repository
rm -rf .git
rm -rf node_modules
rm -rf __pycache__
rm -rf .pytest_cache
rm -rf dist
rm -rf build
find . -name "*.pyc" -delete
find . -name ".DS_Store" -delete

# Initialize new git repository
git init
git add .
git commit -m "Initial commit from Gemini quickstart"
```

## Step 2: Project Structure

```
multiagent-app/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”‚   â”œâ”€â”€ master_orchestrator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ enhanced_user_intent.py
â”‚   â”‚   â”‚   â”œâ”€â”€ intelligent_intent_analyzer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_gemini.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_perplexity.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_o3.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_deepseek.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_qwen.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search_grok.py
â”‚   â”‚   â”‚   â”œâ”€â”€ writer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ evaluator_advanced.py
â”‚   â”‚   â”‚   â”œâ”€â”€ formatter_advanced.py
â”‚   â”‚   â”‚   â”œâ”€â”€ swarm_intelligence_coordinator.py
â”‚   â”‚   â”‚   â””â”€â”€ emergent_intelligence_engine.py
â”‚   â”‚   â”œâ”€â”€ handywriterz_state.py
â”‚   â”‚   â””â”€â”€ handywriterz_graph.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚   â”œâ”€â”€ files.py
â”‚   â”‚   â”‚   â””â”€â”€ wallet.py
â”‚   â”‚   â””â”€â”€ middleware/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â””â”€â”€ storage.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ llm_service.py
â”‚   â”‚   â”œâ”€â”€ wallet_service.py
â”‚   â”‚   â””â”€â”€ file_service.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”‚   â”œâ”€â”€ landing/
â”‚   â”‚   â”‚   â””â”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ mobile/
â”‚   â””â”€â”€ expo-app/
â”œâ”€â”€ shared/
â”‚   â””â”€â”€ types/
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

## Step 3: Backend Implementation

### 3.1 Core Dependencies (requirements.txt)

```txt
fastapi==0.109.0
uvicorn[standard]==0.27.0
python-dotenv==1.0.0
langchain==0.1.0
langgraph==0.0.25
langchain-anthropic==0.1.1
langchain-openai==0.0.5
langchain-google-genai==0.0.5
langchain-community==0.0.12
httpx==0.26.0
redis==5.0.1
supabase==2.3.0
cloudflare==3.0.0
pydantic==2.5.3
python-multipart==0.0.6
aiofiles==23.2.1
Pillow==10.2.0
numpy==1.26.3
pandas==2.1.4
scikit-learn==1.4.0
```

### 3.2 Environment Configuration (.env)

```env
# API Keys
ANTHROPIC_API_KEY=your_key
OPENAI_API_KEY=your_key
GEMINI_API_KEY=your_key
PERPLEXITY_API_KEY=your_key
DEEPSEEK_API_KEY=your_key
QWEN_API_KEY=your_key
GROK_API_KEY=your_key

# Database
DATABASE_URL=your_supabase_url
DATABASE_KEY=your_supabase_key
CLOUDFLARE_ACCOUNT_ID=your_account_id
CLOUDFLARE_API_TOKEN=your_token
CLOUDFLARE_D1_DATABASE_ID=your_database_id

# Redis
REDIS_URL=redis://localhost:6379

# Dynamic.xyz
DYNAMIC_ENVIRONMENT_ID=your_env_id
DYNAMIC_API_KEY=your_api_key

# Storage
CLOUDFLARE_R2_ACCESS_KEY=your_key
CLOUDFLARE_R2_SECRET_KEY=your_secret
CLOUDFLARE_R2_BUCKET=your_bucket
```

### 3.3 FastAPI Main Application (backend/api/main.py)

```python
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import uvicorn

from api.routes import chat, files, wallet
from api.auth import auth_router
from core.config import settings
from core.database import init_db

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await init_db()
    yield
    # Shutdown
    pass

app = FastAPI(
    title="MultiAgent AI Platform",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "https://yourdomain.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(auth_router, prefix="/api/auth", tags=["auth"])
app.include_router(chat.router, prefix="/api/chat", tags=["chat"])
app.include_router(files.router, prefix="/api/files", tags=["files"])
app.include_router(wallet.router, prefix="/api/wallet", tags=["wallet"])

# WebSocket endpoint for real-time chat
@app.websocket("/ws/{client_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str):
    await websocket.accept()
    try:
        while True:
            data = await websocket.receive_json()
            # Process with LangGraph
            response = await process_with_agents(data)
            await websocket.send_json(response)
    except WebSocketDisconnect:
        pass

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
```

### 3.4 Enhanced Master Orchestrator (backend/agent/nodes/master_orchestrator.py)

```python
from typing import Dict, Any
from langchain_core.runnables import RunnableConfig
from agent.handywriterz_state import HandyWriterzState
import asyncio

class MasterOrchestratorAgent:
    """Revolutionary master orchestrator for intelligent workflow routing."""
    
    def __init__(self):
        self.complexity_analyzer = ComplexityAnalyzer()
        self.workflow_optimizer = WorkflowOptimizer()
    
    async def __call__(
        self, 
        state: HandyWriterzState, 
        config: RunnableConfig
    ) -> Dict[str, Any]:
        """Execute master orchestration logic."""
        
        # Analyze request complexity
        complexity_analysis = await self.complexity_analyzer.analyze(
            state.get("user_request", ""),
            state.get("context_files", []),
            state.get("requirements", [])
        )
        
        # Determine optimal workflow
        workflow_plan = await self.workflow_optimizer.optimize(
            complexity_analysis,
            state.get("available_resources", {}),
            state.get("time_constraints", None)
        )
        
        # Set orchestration metadata
        orchestration_result = {
            "workflow_intelligence": {
                "academic_complexity": complexity_analysis["score"],
                "recommended_agents": workflow_plan["agents"],
                "estimated_time": workflow_plan["estimated_time"],
                "success_probability": workflow_plan["success_probability"]
            },
            "routing_decision": workflow_plan["primary_route"],
            "use_swarm_intelligence": complexity_analysis["score"] >= 8.0,
            "parallel_processing": workflow_plan["parallel_capable"]
        }
        
        return {
            "orchestration_result": orchestration_result,
            "workflow_status": "orchestrated",
            "next_node": workflow_plan["primary_route"]
        }

class ComplexityAnalyzer:
    """Analyzes request complexity using multi-dimensional scoring."""
    
    async def analyze(
        self, 
        request: str, 
        files: list, 
        requirements: list
    ) -> Dict[str, Any]:
        # Implement sophisticated complexity analysis
        score = 5.0  # Base score
        
        # Length and depth analysis
        if len(request) > 1000:
            score += 2.0
        if len(files) > 5:
            score += 1.5
        if len(requirements) > 10:
            score += 1.5
            
        # Keyword complexity
        complex_keywords = [
            "analyze", "synthesize", "evaluate", "critique",
            "multi-dimensional", "cross-reference", "systematic"
        ]
        for keyword in complex_keywords:
            if keyword in request.lower():
                score += 0.5
                
        return {
            "score": min(score, 10.0),
            "dimensions": {
                "linguistic": score * 0.3,
                "technical": score * 0.4,
                "research": score * 0.3
            }
        }

class WorkflowOptimizer:
    """Optimizes workflow based on complexity and resources."""
    
    async def optimize(
        self,
        complexity: Dict[str, Any],
        resources: Dict[str, Any],
        constraints: Any
    ) -> Dict[str, Any]:
        score = complexity["score"]
        
        if score >= 8.0:
            return {
                "primary_route": "enhanced_user_intent",
                "agents": ["swarm_coordinator", "emergent_intelligence"],
                "estimated_time": 300,
                "success_probability": 0.95,
                "parallel_capable": True
            }
        elif score >= 5.0:
            return {
                "primary_route": "enhanced_user_intent",
                "agents": ["standard_pipeline"],
                "estimated_time": 180,
                "success_probability": 0.85,
                "parallel_capable": True
            }
        else:
            return {
                "primary_route": "user_intent",
                "agents": ["basic_pipeline"],
                "estimated_time": 60,
                "success_probability": 0.90,
                "parallel_capable": False
            }
```

### 3.5 Gemini Search Agent (backend/agent/nodes/search_gemini.py)

```python
import google.generativeai as genai
from typing import Dict, Any
import asyncio

class GeminiSearchAgent:
    """Advanced Gemini-powered search with multimodal capabilities."""
    
    def __init__(self):
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        self.model = genai.GenerativeModel('gemini-2.5-pro-latest')
        self.search_config = genai.types.GenerationConfig(
            temperature=0.7,
            top_p=0.95,
            max_output_tokens=8192,
        )
    
    async def execute(
        self, 
        state: Dict[str, Any], 
        config: Any
    ) -> Dict[str, Any]:
        """Execute Gemini search with Google Search grounding."""
        
        query = state.get("search_query", state.get("user_request", ""))
        files = state.get("context_files", [])
        
        # Build multimodal prompt
        prompt_parts = [
            f"Research Query: {query}",
            "Please provide comprehensive academic insights with sources."
        ]
        
        # Add file contents if available
        for file in files:
            if file["type"] in ["image", "video", "audio"]:
                # Handle multimodal inputs
                prompt_parts.append(file["content"])
        
        # Enable Google Search grounding
        generation_config = genai.types.GenerationConfig(
            temperature=0.7,
            tools='google_search',
            tool_config={'google_search': {'dynamic_retrieval_config': {
                'mode': 'MODE_DYNAMIC',
                'dynamic_threshold': 0.3
            }}}
        )
        
        try:
            response = await asyncio.to_thread(
                self.model.generate_content,
                prompt_parts,
                generation_config=generation_config
            )
            
            # Extract search results and synthesized knowledge
            search_results = self._parse_search_results(response)
            
            return {
                "gemini_results": search_results,
                "gemini_synthesis": response.text,
                "sources_found": len(search_results),
                "multimodal_processed": len(files) > 0
            }
            
        except Exception as e:
            return {
                "error": f"Gemini search failed: {str(e)}",
                "gemini_results": [],
                "sources_found": 0
            }
    
    def _parse_search_results(self, response):
        """Parse grounded search results from Gemini response."""
        results = []
        
        # Extract grounding metadata
        if hasattr(response, 'grounding_metadata'):
            for source in response.grounding_metadata.get('search_queries', []):
                results.append({
                    "title": source.get("title", ""),
                    "url": source.get("url", ""),
                    "snippet": source.get("snippet", ""),
                    "relevance_score": source.get("score", 0.0)
                })
        
        return results
```

### 3.6 Chat Route Handler (backend/api/routes/chat.py)

```python
from fastapi import APIRouter, HTTPException, Depends, UploadFile, File
from typing import List, Optional
import asyncio

from agent.handywriterz_graph import handywriterz_graph
from api.auth import get_current_user
from services.file_service import FileService

router = APIRouter()

@router.post("/message")
async def send_message(
    message: str,
    files: Optional[List[UploadFile]] = File(None),
    current_user=Depends(get_current_user)
):
    """Process user message through multi-agent system."""
    
    # Process uploaded files
    context_files = []
    if files:
        file_service = FileService()
        for file in files:
            processed_file = await file_service.process_file(file)
            context_files.append(processed_file)
    
    # Prepare state for LangGraph
    initial_state = {
        "user_request": message,
        "context_files": context_files,
        "user_id": current_user["id"],
        "timestamp": datetime.utcnow().isoformat()
    }
    
    # Execute the graph
    try:
        config = {"configurable": {"thread_id": current_user["id"]}}
        result = await handywriterz_graph.ainvoke(initial_state, config)
        
        return {
            "success": True,
            "response": result.get("final_response", ""),
            "sources": result.get("sources", []),
            "workflow_status": result.get("workflow_status", ""),
            "processing_time": result.get("processing_time", 0)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

## Step 4: Frontend Implementation

### 4.1 Package.json

```json
{
  "name": "multiagent-frontend",
  "version": "1.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc && vite build",
    "preview": "vite preview"
  },
  "dependencies": {
    "react": "^19.0.0",
    "react-dom": "^19.0.0",
    "@dynamic-labs/sdk-react-core": "^2.0.0",
    "@dynamic-labs/ethereum": "^2.0.0",
    "@dynamic-labs/solana": "^2.0.0",
    "@radix-ui/react-avatar": "^1.0.4",
    "@radix-ui/react-dialog": "^1.0.5",
    "@radix-ui/react-dropdown-menu": "^2.0.6",
    "@radix-ui/react-scroll-area": "^1.0.5",
    "class-variance-authority": "^0.7.0",
    "clsx": "^2.1.0",
    "framer-motion": "^10.16.16",
    "lucide-react": "^0.309.0",
    "react-dropzone": "^14.2.3",
    "react-markdown": "^9.0.1",
    "react-syntax-highlighter": "^15.5.0",
    "recharts": "^2.10.3",
    "tailwind-merge": "^2.2.0",
    "zustand": "^4.4.7"
  },
  "devDependencies": {
    "@types/react": "^18.2.46",
    "@types/react-dom": "^18.2.18",
    "@vitejs/plugin-react": "^4.2.1",
    "autoprefixer": "^10.4.16",
    "postcss": "^8.4.33",
    "tailwindcss": "^3.4.0",
    "typescript": "^5.3.3",
    "vite": "^5.0.10"
  }
}
```

### 4.2 Main App Component (frontend/src/App.tsx)

```tsx
import React from 'react';
import { BrowserRouter as Router, Routes, Route } from 'react-router-dom';
import { DynamicContextProvider } from '@dynamic-labs/sdk-react-core';
import { EthereumWalletConnectors } from '@dynamic-labs/ethereum';
import { SolanaWalletConnectors } from '@dynamic-labs/solana';

import LandingPage from './pages/LandingPage';
import ChatInterface from './pages/ChatInterface';
import { ThemeProvider } from './components/theme-provider';

const App: React.FC = () => {
  return (
    <DynamicContextProvider
      settings={{
        environmentId: import.meta.env.VITE_DYNAMIC_ENVIRONMENT_ID,
        walletConnectors: [EthereumWalletConnectors, SolanaWalletConnectors],
      }}
    >
      <ThemeProvider defaultTheme="dark" storageKey="multiagent-theme">
        <Router>
          <Routes>
            <Route path="/" element={<LandingPage />} />
            <Route path="/chat" element={<ChatInterface />} />
          </Routes>
        </Router>
      </ThemeProvider>
    </DynamicContextProvider>
  );
};

export default App;
```

### 4.3 Modern Chat Interface (frontend/src/pages/ChatInterface.tsx)

```tsx
import React, { useState, useRef, useEffect } from 'react';
import { Send, Paperclip, Sparkles, User, Bot } from 'lucide-react';
import { motion, AnimatePresence } from 'framer-motion';
import { useDynamicContext } from '@dynamic-labs/sdk-react-core';

import { Button } from '@/components/ui/button';
import { Textarea } from '@/components/ui/textarea';
import { ScrollArea } from '@/components/ui/scroll-area';
import { Avatar, AvatarFallback, AvatarImage } from '@/components/ui/avatar';
import FileUploadZone from '@/components/chat/FileUploadZone';
import MessageBubble from '@/components/chat/MessageBubble';
import TypingIndicator from '@/components/chat/TypingIndicator';
import SourceCard from '@/components/chat/SourceCard';

interface Message {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: Date;
  sources?: any[];
  files?: any[];
}

const ChatInterface: React.FC = () => {
  const { user, isAuthenticated } = useDynamicContext();
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState('');
  const [files, setFiles] = useState<File[]>([]);
  const [isLoading, setIsLoading] = useState(false);
  const [isTyping, setIsTyping] = useState(false);
  const scrollRef = useRef<HTMLDivElement>(null);
  const fileInputRef = useRef<HTMLInputElement>(null);

  useEffect(() => {
    scrollRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages]);

  const handleSendMessage = async () => {
    if (!input.trim() && files.length === 0) return;

    const userMessage: Message = {
      id: Date.now().toString(),
      role: 'user',
      content: input,
      timestamp: new Date(),
      files: files.map(f => ({ name: f.name, type: f.type }))
    };

    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsLoading(true);
    setIsTyping(true);

    try {
      const formData = new FormData();
      formData.append('message', input);
      files.forEach(file => formData.append('files', file));

      const response = await fetch('/api/chat/message', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${user?.sessionToken}`
        },
        body: formData
      });

      const data = await response.json();
      
      setIsTyping(false);
      
      const assistantMessage: Message = {
        id: (Date.now() + 1).toString(),
        role: 'assistant',
        content: data.response,
        timestamp: new Date(),
        sources: data.sources
      };

      setMessages(prev => [...prev, assistantMessage]);
      setFiles([]);
    } catch (error) {
      console.error('Error sending message:', error);
      setIsTyping(false);
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <div className="flex flex-col h-screen bg-background">
      {/* Header */}
      <header className="border-b px-4 py-3 flex items-center justify-between">
        <div className="flex items-center gap-3">
          <Sparkles className="w-6 h-6 text-primary" />
          <h1 className="text-xl font-semibold">MultiAgent AI</h1>
        </div>
        <div className="flex items-center gap-4">
          <span className="text-sm text-muted-foreground">
            {isAuthenticated ? `Connected: ${user?.email}` : 'Not connected'}
          </span>
          <Avatar>
            <AvatarImage src={user?.profilePicture} />
            <AvatarFallback>{user?.email?.[0]?.toUpperCase()}</AvatarFallback>
          </Avatar>
        </div>
      </header>

      {/* Messages Area */}
      <ScrollArea className="flex-1 p-4">
        <div className="max-w-4xl mx-auto space-y-4">
          <AnimatePresence>
            {messages.map((message) => (
              <motion.div
                key={message.id}
                initial={{ opacity: 0, y: 20 }}
                animate={{ opacity: 1, y: 0 }}
                exit={{ opacity: 0, y: -20 }}
                transition={{ duration: 0.3 }}
              >
                <MessageBubble message={message} />
                {message.sources && message.sources.length > 0 && (
                  <div className="mt-3 grid grid-cols-1 md:grid-cols-2 gap-3 ml-12">
                    {message.sources.map((source, idx) => (
                      <SourceCard key={idx} source={source} />
                    ))}
                  </div>
                )}
              </motion.div>
            ))}
            {isTyping && <TypingIndicator />}
          </AnimatePresence>
          <div ref={scrollRef} />
        </div>
      </ScrollArea>

      {/* File Upload Zone */}
      {files.length > 0 && (
        <div className="px-4 py-2 border-t">
          <FileUploadZone files={files} onRemove={(idx) => {
            setFiles(prev => prev.filter((_, i) => i !== idx));
          }} />
        </div>
      )}

      {/* Input Area */}
      <div className="border-t p-4">
        <div className="max-w-4xl mx-auto">
          <div className="flex gap-2 items-end">
            <Button
              variant="ghost"
              size="icon"
              onClick={() => fileInputRef.current?.click()}
              className="mb-1"
            >
              <Paperclip className="w-5 h-5" />
            </Button>
            <input
              ref={fileInputRef}
              type="file"
              multiple
              className="hidden"
              onChange={(e) => {
                if (e.target.files) {
                  setFiles(Array.from(e.target.files));
                }
              }}
              accept="image/*,video/*,audio/*,.pdf,.doc,.docx,.txt"
            />
            <Textarea
              value={input}
              onChange={(e) => setInput(e.target.value)}
              onKeyDown={(e) => {
                if (e.key === 'Enter' && !e.shiftKey) {
                  e.preventDefault();
                  handleSendMessage();
                }
              }}
              placeholder="Ask anything... (Shift+Enter for new line)"
              className="min-h-[60px] max-h-[200px] resize-none"
              disabled={isLoading}
            />
            <Button
              onClick={handleSendMessage}
              disabled={isLoading || (!input.trim() && files.length === 0)}
              className="mb-1"
            >
              <Send className="w-5 h-5" />
            </Button>
          </div>
          <p className="text-xs text-muted-foreground mt-2 text-center">
            Powered by Claude, GPT-4, Gemini, and advanced multi-agent orchestration
          </p>
        </div>
      </div>
    </div>
  );
};

export default ChatInterface;
```

## Step 5: Docker Configuration

### 5.1 Backend Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    git \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port
EXPOSE 8000

# Run with hot reload in development
CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```

### 5.2 Docker Compose (docker-compose.yml)

```yaml
version: '3.8'

services:
  backend:
    build: 
      context: ./backend
      dockerfile: Dockerfile
    volumes:
      - ./backend:/app
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
    env_file:
      - ./backend/.env
    depends_on:
      - redis
    networks:
      - app-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    volumes:
      - ./frontend:/app
      - /app/node_modules
    ports:
      - "5173:5173"
    environment:
      - VITE_API_URL=http://localhost:8000
    networks:
      - app-network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - app-network

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - backend
      - frontend
    networks:
      - app-network

volumes:
  redis-data:

networks:
  app-network:
    driver: bridge
```

### 5.3 Makefile

```makefile
.PHONY: help dev build test deploy clean

help: ## Show this help message
	@echo 'Usage: make [target]'
	@echo ''
	@echo 'Targets:'
	@awk 'BEGIN {FS = ":.*?## "} /^[a-zA-Z_-]+:.*?## / {printf "  %-15s %s\n", $$1, $$2}' $(MAKEFILE_LIST)

dev: ## Start development environment with hot reload
	docker-compose up --build

dev-backend: ## Start only backend with hot reload
	cd backend && uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

dev-frontend: ## Start only frontend with hot reload
	cd frontend && npm run dev

build: ## Build production images
	docker-compose build --no-cache

test: ## Run tests
	cd backend && pytest tests/ -v
	cd frontend && npm test

deploy-cf: ## Deploy to Cloudflare
	cd frontend && npm run build
	npx wrangler pages publish dist --project-name multiagent-app

deploy-vercel: ## Deploy to Vercel
	vercel --prod

clean: ## Clean all containers, volumes and build artifacts
	docker-compose down -v
	rm -rf backend/__pycache__
	rm -rf frontend/node_modules
	rm -rf frontend/dist
	rm -rf mobile/node_modules

install: ## Install all dependencies
	cd backend && pip install -r requirements.txt
	cd frontend && npm install
	cd mobile && npm install

migrate: ## Run database migrations
	cd backend && python -m alembic upgrade head

seed: ## Seed database with sample data
	cd backend && python scripts/seed_db.py

logs: ## Show logs
	docker-compose logs -f

shell-backend: ## Open shell in backend container
	docker-compose exec backend bash

shell-frontend: ## Open shell in frontend container
	docker-compose exec frontend sh
